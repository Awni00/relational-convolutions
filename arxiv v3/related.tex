\subsection{Related Work}\label{ssec:related_work}

To place our framework in the context of previous work, we briefly discuss related forms of relational learning below, pointing first to the review of relational learning inductive biases by~\citet{battagliaRelationalInductiveBiases2018}.

{Graph neural networks} (GNNs) are a class of neural network architectures which operate on graphs and process ``relational'' data~\citep[e.g.,][]{niepertLearningConvolutionalNeural2016,kipfSemiSupervisedClassificationGraph2017,schlichtkrullModelingRelationalData2017,velickovicGraphAttentionNetworks2017,kipfNeuralRelationalInference2018,xuHowPowerfulAre2018}. A defining feature of GNN models is their use of a form of neural message-passing, wherein the hidden representation of a node is updated as a function of the hidden representations of its neighbors on a graph~\citep{gilmerNeuralMessagePassing2017}. Typical examples of tasks that GNNs are applied to include node classification, graph classification, and link prediction~\citep{hamiltonGraphRepresentationLearning2020}. %This is a very general model which includes as a special case convolutional neural networks, where the graph is a grid, and Transformers, where the graph is a complete graph and the message-passing function is a convex sum of the neighbors' representations.

In GNNs, the `relations' are given to the model as input via edges in a graph. In contrast, our architecture, as well as the relational architectures described below, operate on collections of objects without any relations given as input. Instead, such relational architectures must infer the relevant relations from the objects themselves. Still, graph neural networks can be applied to these relational tasks by passing in the collection of objects along with a complete graph. % A Transformer Encoder can be thought of as a special case of this architecture.%, and hence is the representative baseline we compare against in our experiments.

Several works have proposed architectures with the ability to model relations by incorporating an {attention mechanism}~\citep[e.g.,][]{vaswani2017attention,velickovicGraphAttentionNetworks2017,santoroRelationalRecurrent2018,zambaldiDeepReinforcementLearning2018,locatelloObjectCentricLearningSlot2020}. Attention mechanisms, such as self-attention in Transformers~\citep{vaswani2017attention}, model relations between objects implicitly as an intermediate step in an information-retrieval operation
% a form of neural message-passing in order 
to update the representation of each object as a function of its context.

There also exists a growing literature on neural architectures that aim to explicitly model relational information between objects. An early example is the relation network proposed by~\citet{santoroSimpleNeural2017}, which produces an embedding representation for a set of objects based on aggregated pairwise relations.~\citet{shanahanExplicitlyRelationalNeural} proposes the PrediNet architecture, which aims to learn relational representations that are compatible with predicate logic.
% \aanote{removed citation of ESBN for space (since not in baselines). can put back in camera-ready.}
% ~\citet{webbEmergentSymbols2021} proposes ESBN, a recurrent neural network augmented with external memory whose memory-write operation aims to factors representations into `sensory' and `relational'.
~\citet{kergNeuralArchitecture2022} proposes CoRelNet, a simple architecture based on `similarity scores' that aims to distill the relational inductive biases discovered in previous work into a minimal architecture.~\citet{altabaaAbstractorsRelationalCrossattention2024,altabaa2024disentangling} explore relational inductive biases in the context of Transformers, and propose a view of relational inductive biases as a type of selective ``information bottleneck'' which disentangles relational information from object-level features.~\citet{webbRelationalBottleneckInductive2023} provides a cognitive science perspective on this idea, arguing that a relational information bottleneck may be a mechanism for abstraction in the human mind.% and brain.
