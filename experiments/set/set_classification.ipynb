{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SET Classification: RelConvNet vs CoRelNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-14 21:28:35.804540: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-09-14 21:28:35.855828: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-09-14 21:28:37.481111: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from setGame import *\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Model\n",
    "from sklearn.model_selection import train_test_split\n",
    "sys.path.append('../..'); sys.path.append('../');\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "setgame = SetGame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x100 with 6 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 5]\n"
     ]
    }
   ],
   "source": [
    "num_cards=6\n",
    "_ = setgame.init_state(num_cards=num_cards, shuffle=False)\n",
    "hand = setgame.state.dealt_cards\n",
    "fig, axarr = plt.subplots(1, len(hand), figsize=(len(hand)*1,1))\n",
    "pi = np.random.choice(range(len(hand)), size=len(hand), replace=False)\n",
    "for i in range(len(hand)):\n",
    "    card = hand[i]\n",
    "    axarr[i].imshow(setgame.image_of_card(card[0], card[1]))\n",
    "    axarr[i].axis('off')\n",
    "plt.show()\n",
    "print(np.sort(np.argsort(pi)[0:3]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Model, Sequential\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "class SetEmbedder():\n",
    "    def __init__(self, ff_dim1=128, ff_dim2=64):\n",
    "        img_input = layers.Input(shape=(70, 50, 4))\n",
    "        x = layers.Conv2D(32, (5, 5), activation='relu')(img_input)\n",
    "        x = layers.MaxPooling2D((4,4))(x)\n",
    "        x = layers.Conv2D(32, (5, 5), activation='relu')(x)\n",
    "        x = layers.MaxPooling2D((4,4))(x)\n",
    "        x = layers.Flatten()(x)\n",
    "        x = layers.Dense(ff_dim1, activation='relu')(x)\n",
    "        x = layers.Dense(ff_dim2, activation='tanh')(x)\n",
    "        outputs = layers.Dense(12, activation='sigmoid')(x)\n",
    "        self.model = Model(inputs=img_input, outputs=outputs)\n",
    "        self.embed = Model(self.model.input, self.model.layers[7].output)\n",
    "        self.model.summary()\n",
    "        opt = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "        self.model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['binary_accuracy'])\n",
    "        self.model_initial_weights = self.model.get_weights()\n",
    "\n",
    "    def train(self, X_train, y_train, epochs=2):\n",
    "        self.model.set_weights(self.model_initial_weights)\n",
    "        self.model.fit(X_train, y_train, epochs=epochs, batch_size=32, verbose=1)\n",
    "\n",
    "    def predict(self, X_test):\n",
    "        out = self.model.predict(X_test, verbose=0)\n",
    "        return out\n",
    "\n",
    "    def embed(self, X_test):\n",
    "        out = self.embed(X_test, verbose=0)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_binary(attrs):\n",
    "    color = {'red':[1,0,0], 'green':[0,1,0], 'purple':[0,0,1]}\n",
    "    pattern = {'empty':[1,0,0], 'striped':[0,1,0], 'solid':[0,0,1]}\n",
    "    shape = {'diamond':[1,0,0], 'oval':[0,1,0], 'squiggle':[0,0,1]}\n",
    "    number = {'one':[1,0,0], 'two':[0,1,0], 'three':[0,0,1]}\n",
    "    binary_attrs = number[attrs[0]] + color[attrs[1]] + pattern[attrs[2]] + shape[attrs[3]]\n",
    "    return binary_attrs\n",
    "\n",
    "n = 1000\n",
    "X = np.empty((n, 70, 50, 4), dtype=np.float32)\n",
    "y = np.empty((n, 12), dtype=int)\n",
    "\n",
    "card_coord = [(i,j) for i in np.arange(9) for j in np.arange(9)]\n",
    "for i in np.arange(n):\n",
    "    c = np.random.choice(np.arange(81), size=1)[0]\n",
    "    (row, col) = card_coord[c]\n",
    "    attrs = setgame.attributes_of_card(row, col)\n",
    "    binary_attrs = convert_to_binary(attrs)\n",
    "    X[i] = setgame.image_of_card(row, col)\n",
    "    y[i] = binary_attrs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, 70, 50, 4)]       0         \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 66, 46, 32)        3232      \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPoolin  (None, 16, 11, 32)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 12, 7, 32)         25632     \n",
      "                                                                 \n",
      " max_pooling2d_3 (MaxPoolin  (None, 3, 1, 32)          0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 96)                0         \n",
      "                                                                 \n",
      " dense_37 (Dense)            (None, 64)                6208      \n",
      "                                                                 \n",
      " dense_38 (Dense)            (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_39 (Dense)            (None, 12)                780       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 40012 (156.30 KB)\n",
      "Trainable params: 40012 (156.30 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "24/24 [==============================] - 2s 4ms/step - loss: 0.6443 - binary_accuracy: 0.6576\n",
      "Epoch 2/50\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 0.6152 - binary_accuracy: 0.6676\n",
      "Epoch 3/50\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 0.5568 - binary_accuracy: 0.7189\n",
      "Epoch 4/50\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 0.4707 - binary_accuracy: 0.7764\n",
      "Epoch 5/50\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 0.3797 - binary_accuracy: 0.8399\n",
      "Epoch 6/50\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 0.3068 - binary_accuracy: 0.8834\n",
      "Epoch 7/50\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 0.2559 - binary_accuracy: 0.9162\n",
      "Epoch 8/50\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 0.2152 - binary_accuracy: 0.9386\n",
      "Epoch 9/50\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 0.1819 - binary_accuracy: 0.9600\n",
      "Epoch 10/50\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 0.1494 - binary_accuracy: 0.9747\n",
      "Epoch 11/50\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 0.1237 - binary_accuracy: 0.9852\n",
      "Epoch 12/50\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 0.1014 - binary_accuracy: 0.9936\n",
      "Epoch 13/50\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 0.0841 - binary_accuracy: 0.9963\n",
      "Epoch 14/50\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 0.0689 - binary_accuracy: 0.9984\n",
      "Epoch 15/50\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 0.0574 - binary_accuracy: 0.9994\n",
      "Epoch 16/50\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 0.0499 - binary_accuracy: 0.9994\n",
      "Epoch 17/50\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 0.0426 - binary_accuracy: 1.0000\n",
      "Epoch 18/50\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 0.0372 - binary_accuracy: 0.9998\n",
      "Epoch 19/50\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 0.0327 - binary_accuracy: 1.0000\n",
      "Epoch 20/50\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 0.0289 - binary_accuracy: 1.0000\n",
      "Epoch 21/50\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 0.0259 - binary_accuracy: 1.0000\n",
      "Epoch 22/50\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 0.0237 - binary_accuracy: 1.0000\n",
      "Epoch 23/50\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 0.0215 - binary_accuracy: 1.0000\n",
      "Epoch 24/50\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 0.0198 - binary_accuracy: 1.0000\n",
      "Epoch 25/50\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 0.0182 - binary_accuracy: 1.0000\n",
      "Epoch 26/50\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 0.0168 - binary_accuracy: 1.0000\n",
      "Epoch 27/50\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 0.0157 - binary_accuracy: 1.0000\n",
      "Epoch 28/50\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 0.0146 - binary_accuracy: 1.0000\n",
      "Epoch 29/50\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 0.0137 - binary_accuracy: 1.0000\n",
      "Epoch 30/50\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 0.0129 - binary_accuracy: 1.0000\n",
      "Epoch 31/50\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 0.0121 - binary_accuracy: 1.0000\n",
      "Epoch 32/50\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 0.0114 - binary_accuracy: 1.0000\n",
      "Epoch 33/50\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 0.0107 - binary_accuracy: 1.0000\n",
      "Epoch 34/50\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 0.0102 - binary_accuracy: 1.0000\n",
      "Epoch 35/50\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 0.0097 - binary_accuracy: 1.0000\n",
      "Epoch 36/50\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 0.0091 - binary_accuracy: 1.0000\n",
      "Epoch 37/50\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 0.0087 - binary_accuracy: 1.0000\n",
      "Epoch 38/50\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 0.0083 - binary_accuracy: 1.0000\n",
      "Epoch 39/50\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 0.0079 - binary_accuracy: 1.0000\n",
      "Epoch 40/50\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 0.0075 - binary_accuracy: 1.0000\n",
      "Epoch 41/50\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 0.0072 - binary_accuracy: 1.0000\n",
      "Epoch 42/50\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 0.0068 - binary_accuracy: 1.0000\n",
      "Epoch 43/50\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 0.0066 - binary_accuracy: 1.0000\n",
      "Epoch 44/50\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 0.0063 - binary_accuracy: 1.0000\n",
      "Epoch 45/50\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 0.0060 - binary_accuracy: 1.0000\n",
      "Epoch 46/50\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 0.0058 - binary_accuracy: 1.0000\n",
      "Epoch 47/50\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 0.0056 - binary_accuracy: 1.0000\n",
      "Epoch 48/50\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 0.0054 - binary_accuracy: 1.0000\n",
      "Epoch 49/50\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 0.0052 - binary_accuracy: 1.0000\n",
      "Epoch 50/50\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 0.0050 - binary_accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25)\n",
    "cnn = SetEmbedder(ff_dim1=64, ff_dim2=64)\n",
    "cnn.train(X_train, y_train, epochs=50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0]\n",
      "[0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0]\n",
      "\n",
      "\n",
      "[0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0]\n",
      "[0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0]\n",
      "\n",
      "\n",
      "[1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0]\n",
      "[1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0]\n",
      "\n",
      "\n",
      "[0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0]\n",
      "[0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0]\n",
      "\n",
      "\n",
      "[0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0]\n",
      "[0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    j = np.random.choice(range(X_test.shape[0]))\n",
    "    pred = np.round(cnn.predict(X_test[j:(j+1)]))[0]\n",
    "    pred = [int(pred[b]) for b in range(len(pred))]\n",
    "    print(list(pred))\n",
    "    print(list(y_test[j:(j+1)][0]))\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = cnn.predict(X_test)\n",
    "pred = np.array(np.round(out), dtype=int)\n",
    "1-np.sum(pred != y_test) / (np.prod(pred.shape))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SET Classification Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_set_classification_dataset(num_seqs):\n",
    "\n",
    "    vocab_size = 81\n",
    "    setgame = SetGame()\n",
    "    dim = len(cnn.embed(np.expand_dims(setgame.image_of_card(0, 0), axis=0)).numpy().squeeze())\n",
    "\n",
    "    # generate random features for each object\n",
    "    card_images = np.zeros((9, 9, dim))\n",
    "    for i in range(9):\n",
    "        for j in range(9):\n",
    "            card_images[i,j] = cnn.embed(np.expand_dims(setgame.image_of_card(i, j), axis=0)).numpy().squeeze()\n",
    "\n",
    "    object_seqs = np.zeros((num_seqs, 3, dim))\n",
    "    card_seqs = np.zeros((num_seqs, 3, 2), dtype=int)\n",
    "    labels = np.zeros(num_seqs, dtype=int)\n",
    "\n",
    "    for s in np.arange(0, num_seqs, 2):\n",
    "        _ = setgame.init_state(num_cards=6, shuffle=False)\n",
    "        hand = setgame.state.dealt_cards\n",
    "        for i in np.arange(3):\n",
    "            card = hand[i]\n",
    "            object_seqs[s, i] = card_images[card[0], card[1]]\n",
    "            card_seqs[s, i] = [card[0], card[1]]\n",
    "        labels[s] = 1\n",
    "        for i in np.arange(3):\n",
    "            card = hand[i+3]\n",
    "            object_seqs[s+1, i] = card_images[card[0], card[1]]\n",
    "            card_seqs[s+1, i] = [card[0], card[1]]\n",
    "        labels[s+1] = 0\n",
    "\n",
    "    return card_images, card_seqs, labels, object_seqs,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of triples: 85320\n",
      "Probability of SET! (in 85320 samples): 0.012658 (1/79=0.012658)\n"
     ]
    }
   ],
   "source": [
    "setgame = SetGame()\n",
    "X, y, triples = setgame.generate_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_triples = [triple for triple, y_ in zip(triples, y) if y_]\n",
    "X_set = X[y==1]\n",
    "nonset_triples = [triple for triple, y_ in zip(triples, y) if not y_]\n",
    "X_nonset = X[y==0]\n",
    "\n",
    "def train_val_test_split(X, val_size=0.1, test_size=0.2):\n",
    "    X_train, X_test = train_test_split(X, test_size=test_size)\n",
    "    X_train, X_val = train_test_split(X_train, test_size=val_size/(1-test_size))\n",
    "    return X_train, X_val, X_test\n",
    "\n",
    "set_triples_train, set_triples_val, set_triples_test = train_val_test_split(set_triples, val_size=0.1, test_size=0.2)\n",
    "nonset_triples_train, nonset_triples_val, nonset_triples_test = train_val_test_split(nonset_triples, val_size=0.1, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "all_cards = [(i,j) for i in range(9) for j in range(9)]\n",
    "\n",
    "def contains_set(cards):\n",
    "    return any(setgame.triple_is_set(triple) for triple in itertools.combinations(cards, 3))\n",
    "\n",
    "def sample_nonset(k):\n",
    "    idxs = np.random.choice(len(all_cards), k)\n",
    "    candidate_tuple = [all_cards[i] for i in idxs]\n",
    "    while contains_set(candidate_tuple):\n",
    "        idxs = np.random.choice(len(all_cards), k)\n",
    "        candidate_tuple = [all_cards[i] for i in idxs]\n",
    "    return candidate_tuple\n",
    "\n",
    "def sample_set(k, set_triples):\n",
    "    # sample set triple\n",
    "    set_cards = set_triples[np.random.choice(len(set_triples))]\n",
    "    remaining_cards = [card for card in all_cards if card not in set_cards]\n",
    "    idxs = np.random.choice(len(remaining_cards), k-3, replace=False)\n",
    "    card_tuple = set_cards + [remaining_cards[i] for i in idxs]\n",
    "    np.random.shuffle(card_tuple)\n",
    "    return card_tuple\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "167 s  3.97 s per loop (mean  std. dev. of 7 runs, 10,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit sample_nonset(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37.8 s  276 ns per loop (mean  std. dev. of 7 runs, 10,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit sample_set(5, set_triples=set_triples_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "contains_set(sample_nonset(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "contains_set(sample_set(5, set_triples_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_set_classification_dataset(num_seqs, k, set_triples):\n",
    "\n",
    "    vocab_size = 81\n",
    "    setgame = SetGame()\n",
    "    dim = len(cnn.embed(np.expand_dims(setgame.image_of_card(0, 0), axis=0)).numpy().squeeze())\n",
    "\n",
    "    # generate random features for each object\n",
    "    card_images = np.zeros((9, 9, dim))\n",
    "    for i in range(9):\n",
    "        for j in range(9):\n",
    "            card_images[i,j] = cnn.embed(np.expand_dims(setgame.image_of_card(i, j), axis=0)).numpy().squeeze()\n",
    "\n",
    "    object_seqs = np.zeros((num_seqs, k, dim))\n",
    "    card_seqs = np.zeros((num_seqs, k, 2), dtype=int)\n",
    "    labels = np.zeros(num_seqs, dtype=int)\n",
    "\n",
    "    # sample tuples containing sets\n",
    "    set_tuples = [sample_set(k, set_triples) for _ in range(num_seqs//2)]\n",
    "    nonset_tuples = [sample_nonset(k) for _ in range(num_seqs//2)]\n",
    "\n",
    "    # sample tuples not containing set\n",
    "\n",
    "    # get card image embedding for each and create object_seqs, card_seqs, etc\n",
    "\n",
    "    for s in np.arange(0, num_seqs, 2):\n",
    "        for i in np.arange(k):\n",
    "            card = set_tuples[s//2][i]\n",
    "            object_seqs[s, i] = card_images[card[0], card[1]]\n",
    "            card_seqs[s, i] = [card[0], card[1]]\n",
    "        labels[s] = 1\n",
    "        for i in np.arange(k):\n",
    "            card = nonset_tuples[s//2][i]\n",
    "            object_seqs[s+1, i] = card_images[card[0], card[1]]\n",
    "            card_seqs[s+1, i] = [card[0], card[1]]\n",
    "        labels[s+1] = 0\n",
    "\n",
    "    return card_images, card_seqs, labels, object_seqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 5\n",
    "_, _, labels_train, object_seqs_train = create_set_classification_dataset(num_seqs=len(set_triples_train)*24, k=k, set_triples=set_triples_train)\n",
    "_, _, labels_val, object_seqs_val = create_set_classification_dataset(num_seqs=len(set_triples_val)*24, k=k, set_triples=set_triples_val)\n",
    "_, _, labels_test, object_seqs_test = create_set_classification_dataset(num_seqs=len(set_triples_test)*24, k=k, set_triples=set_triples_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, X_test = object_seqs_train, object_seqs_val, object_seqs_test\n",
    "y_train, y_val, y_test = labels_train, labels_val, labels_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((18144, 5, 64), (18144,))"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, name='binary_crossentropy')\n",
    "create_opt = lambda : tf.keras.optimizers.Adam()\n",
    "\n",
    "def create_callbacks():\n",
    "    callbacks = [tf.keras.callbacks.EarlyStopping(monitor='val_acc', restore_best_weights=True, patience=50, start_from_epoch=30)]\n",
    "    return callbacks\n",
    "\n",
    "n_epochs = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_models as tfm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_34\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " transformer_encoder_34 (Tr  (32, 5, 64)               20960     \n",
      " ansformerEncoder)                                               \n",
      "                                                                 \n",
      " global_average_pooling1d_3  (32, 64)                  0         \n",
      " 4 (GlobalAveragePooling1D)                                      \n",
      "                                                                 \n",
      " dense_40 (Dense)            (32, 2)                   130       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 21090 (82.38 KB)\n",
      "Trainable params: 21090 (82.38 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "encoder_kwargs = dict(num_layers=1, num_attention_heads=8, intermediate_size=32,\n",
    "    activation='relu', dropout_rate=0.0, attention_dropout_rate=0.0,\n",
    "    use_bias=False, norm_first=True, norm_epsilon=1e-06, intermediate_dropout=0.0)\n",
    "def create_transformer():\n",
    "    encoder = tfm.nlp.models.TransformerEncoder(\n",
    "        **encoder_kwargs)\n",
    "\n",
    "    model = tf.keras.Sequential([\n",
    "        encoder,\n",
    "        tf.keras.layers.GlobalAveragePooling1D(),\n",
    "        tf.keras.layers.Dense(2)])\n",
    "    return model\n",
    "\n",
    "transformer_model = create_transformer()\n",
    "\n",
    "transformer_model.compile(loss=loss, optimizer=create_opt(), metrics=['acc'])\n",
    "transformer_model(X_train[:32]); # build\n",
    "transformer_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "567/567 [==============================] - 6s 5ms/step - loss: 0.6979 - acc: 0.5364 - val_loss: 0.6976 - val_acc: 0.5459\n",
      "Epoch 2/100\n",
      "567/567 [==============================] - 3s 5ms/step - loss: 0.6484 - acc: 0.6243 - val_loss: 0.6709 - val_acc: 0.5826\n",
      "Epoch 3/100\n",
      "567/567 [==============================] - 3s 5ms/step - loss: 0.6185 - acc: 0.6591 - val_loss: 0.6578 - val_acc: 0.6250\n",
      "Epoch 4/100\n",
      "567/567 [==============================] - 3s 5ms/step - loss: 0.5972 - acc: 0.6770 - val_loss: 0.6567 - val_acc: 0.6281\n",
      "Epoch 5/100\n",
      "567/567 [==============================] - 3s 5ms/step - loss: 0.5809 - acc: 0.6966 - val_loss: 0.6800 - val_acc: 0.6150\n",
      "Epoch 6/100\n",
      "567/567 [==============================] - 3s 5ms/step - loss: 0.5649 - acc: 0.7098 - val_loss: 0.6739 - val_acc: 0.6134\n",
      "Epoch 7/100\n",
      "567/567 [==============================] - 3s 5ms/step - loss: 0.5485 - acc: 0.7186 - val_loss: 0.6971 - val_acc: 0.6065\n",
      "Epoch 8/100\n",
      "567/567 [==============================] - 3s 5ms/step - loss: 0.5356 - acc: 0.7277 - val_loss: 0.7023 - val_acc: 0.5988\n",
      "Epoch 9/100\n",
      "567/567 [==============================] - 3s 5ms/step - loss: 0.5231 - acc: 0.7358 - val_loss: 0.7000 - val_acc: 0.6173\n",
      "Epoch 10/100\n",
      "567/567 [==============================] - 3s 5ms/step - loss: 0.5116 - acc: 0.7458 - val_loss: 0.6996 - val_acc: 0.6188\n",
      "Epoch 11/100\n",
      "567/567 [==============================] - 3s 5ms/step - loss: 0.5021 - acc: 0.7547 - val_loss: 0.7155 - val_acc: 0.6211\n",
      "Epoch 12/100\n",
      "567/567 [==============================] - 3s 5ms/step - loss: 0.4849 - acc: 0.7646 - val_loss: 0.7504 - val_acc: 0.5918\n",
      "Epoch 13/100\n",
      "567/567 [==============================] - 3s 5ms/step - loss: 0.4780 - acc: 0.7703 - val_loss: 0.7369 - val_acc: 0.6084\n",
      "Epoch 14/100\n",
      "567/567 [==============================] - 3s 5ms/step - loss: 0.4645 - acc: 0.7758 - val_loss: 0.7274 - val_acc: 0.6061\n",
      "Epoch 15/100\n",
      "567/567 [==============================] - 3s 5ms/step - loss: 0.4570 - acc: 0.7836 - val_loss: 0.7555 - val_acc: 0.6034\n",
      "Epoch 16/100\n",
      "567/567 [==============================] - 3s 5ms/step - loss: 0.4446 - acc: 0.7906 - val_loss: 0.7594 - val_acc: 0.6208\n",
      "Epoch 17/100\n",
      "567/567 [==============================] - 3s 5ms/step - loss: 0.4334 - acc: 0.7953 - val_loss: 0.8277 - val_acc: 0.5953\n",
      "Epoch 18/100\n",
      "567/567 [==============================] - 3s 5ms/step - loss: 0.4271 - acc: 0.7994 - val_loss: 0.8106 - val_acc: 0.6084\n",
      "Epoch 19/100\n",
      "567/567 [==============================] - 3s 5ms/step - loss: 0.4182 - acc: 0.8057 - val_loss: 0.8214 - val_acc: 0.5961\n",
      "Epoch 20/100\n",
      "567/567 [==============================] - 3s 5ms/step - loss: 0.4087 - acc: 0.8122 - val_loss: 0.8368 - val_acc: 0.6030\n",
      "Epoch 21/100\n",
      "567/567 [==============================] - 3s 5ms/step - loss: 0.4015 - acc: 0.8155 - val_loss: 0.8959 - val_acc: 0.5872\n",
      "Epoch 22/100\n",
      "567/567 [==============================] - 3s 5ms/step - loss: 0.3942 - acc: 0.8212 - val_loss: 0.9003 - val_acc: 0.5918\n",
      "Epoch 23/100\n",
      "567/567 [==============================] - 3s 5ms/step - loss: 0.3828 - acc: 0.8273 - val_loss: 0.8538 - val_acc: 0.5938\n",
      "Epoch 24/100\n",
      "567/567 [==============================] - 3s 5ms/step - loss: 0.3787 - acc: 0.8295 - val_loss: 0.8552 - val_acc: 0.6107\n",
      "Epoch 25/100\n",
      "388/567 [===================>..........] - ETA: 0s - loss: 0.3573 - acc: 0.8426"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[247], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m X_train_ \u001b[38;5;241m=\u001b[39m X_train[:train_size]\n\u001b[1;32m      3\u001b[0m y_train_ \u001b[38;5;241m=\u001b[39m y_train[:train_size]\n\u001b[0;32m----> 4\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mtransformer_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcreate_callbacks\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/relconvnet/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/.conda/envs/relconvnet/lib/python3.10/site-packages/keras/src/engine/training.py:1748\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1746\u001b[0m logs \u001b[39m=\u001b[39m tmp_logs\n\u001b[1;32m   1747\u001b[0m end_step \u001b[39m=\u001b[39m step \u001b[39m+\u001b[39m data_handler\u001b[39m.\u001b[39mstep_increment\n\u001b[0;32m-> 1748\u001b[0m callbacks\u001b[39m.\u001b[39;49mon_train_batch_end(end_step, logs)\n\u001b[1;32m   1749\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstop_training:\n\u001b[1;32m   1750\u001b[0m     \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/relconvnet/lib/python3.10/site-packages/keras/src/callbacks.py:475\u001b[0m, in \u001b[0;36mCallbackList.on_train_batch_end\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m    468\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Calls the `on_train_batch_end` methods of its callbacks.\u001b[39;00m\n\u001b[1;32m    469\u001b[0m \n\u001b[1;32m    470\u001b[0m \u001b[39mArgs:\u001b[39;00m\n\u001b[1;32m    471\u001b[0m \u001b[39m    batch: Integer, index of batch within the current epoch.\u001b[39;00m\n\u001b[1;32m    472\u001b[0m \u001b[39m    logs: Dict. Aggregated metric results up until this batch.\u001b[39;00m\n\u001b[1;32m    473\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    474\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_should_call_train_batch_hooks:\n\u001b[0;32m--> 475\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_batch_hook(ModeKeys\u001b[39m.\u001b[39;49mTRAIN, \u001b[39m\"\u001b[39;49m\u001b[39mend\u001b[39;49m\u001b[39m\"\u001b[39;49m, batch, logs\u001b[39m=\u001b[39;49mlogs)\n",
      "File \u001b[0;32m~/.conda/envs/relconvnet/lib/python3.10/site-packages/keras/src/callbacks.py:322\u001b[0m, in \u001b[0;36mCallbackList._call_batch_hook\u001b[0;34m(self, mode, hook, batch, logs)\u001b[0m\n\u001b[1;32m    320\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_batch_begin_hook(mode, batch, logs)\n\u001b[1;32m    321\u001b[0m \u001b[39melif\u001b[39;00m hook \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mend\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m--> 322\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_batch_end_hook(mode, batch, logs)\n\u001b[1;32m    323\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    324\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    325\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mUnrecognized hook: \u001b[39m\u001b[39m{\u001b[39;00mhook\u001b[39m}\u001b[39;00m\u001b[39m. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    326\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mExpected values are [\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mbegin\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m, \u001b[39m\u001b[39m\"\u001b[39m\u001b[39mend\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m]\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    327\u001b[0m     )\n",
      "File \u001b[0;32m~/.conda/envs/relconvnet/lib/python3.10/site-packages/keras/src/callbacks.py:345\u001b[0m, in \u001b[0;36mCallbackList._call_batch_end_hook\u001b[0;34m(self, mode, batch, logs)\u001b[0m\n\u001b[1;32m    342\u001b[0m     batch_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_batch_start_time\n\u001b[1;32m    343\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_batch_times\u001b[39m.\u001b[39mappend(batch_time)\n\u001b[0;32m--> 345\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_batch_hook_helper(hook_name, batch, logs)\n\u001b[1;32m    347\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_batch_times) \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_batches_for_timing_check:\n\u001b[1;32m    348\u001b[0m     end_hook_name \u001b[39m=\u001b[39m hook_name\n",
      "File \u001b[0;32m~/.conda/envs/relconvnet/lib/python3.10/site-packages/keras/src/callbacks.py:393\u001b[0m, in \u001b[0;36mCallbackList._call_batch_hook_helper\u001b[0;34m(self, hook_name, batch, logs)\u001b[0m\n\u001b[1;32m    391\u001b[0m \u001b[39mfor\u001b[39;00m callback \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallbacks:\n\u001b[1;32m    392\u001b[0m     hook \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(callback, hook_name)\n\u001b[0;32m--> 393\u001b[0m     hook(batch, logs)\n\u001b[1;32m    395\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_timing:\n\u001b[1;32m    396\u001b[0m     \u001b[39mif\u001b[39;00m hook_name \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_hook_times:\n",
      "File \u001b[0;32m~/.conda/envs/relconvnet/lib/python3.10/site-packages/keras/src/callbacks.py:1093\u001b[0m, in \u001b[0;36mProgbarLogger.on_train_batch_end\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m   1092\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mon_train_batch_end\u001b[39m(\u001b[39mself\u001b[39m, batch, logs\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m-> 1093\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_batch_update_progbar(batch, logs)\n",
      "File \u001b[0;32m~/.conda/envs/relconvnet/lib/python3.10/site-packages/keras/src/callbacks.py:1169\u001b[0m, in \u001b[0;36mProgbarLogger._batch_update_progbar\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m   1165\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mseen \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m add_seen\n\u001b[1;32m   1167\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m   1168\u001b[0m     \u001b[39m# Only block async when verbose = 1.\u001b[39;00m\n\u001b[0;32m-> 1169\u001b[0m     logs \u001b[39m=\u001b[39m tf_utils\u001b[39m.\u001b[39;49msync_to_numpy_or_python_type(logs)\n\u001b[1;32m   1170\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprogbar\u001b[39m.\u001b[39mupdate(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mseen, \u001b[39mlist\u001b[39m(logs\u001b[39m.\u001b[39mitems()), finalize\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m~/.conda/envs/relconvnet/lib/python3.10/site-packages/keras/src/utils/tf_utils.py:694\u001b[0m, in \u001b[0;36msync_to_numpy_or_python_type\u001b[0;34m(tensors)\u001b[0m\n\u001b[1;32m    691\u001b[0m         \u001b[39mreturn\u001b[39;00m t\n\u001b[1;32m    692\u001b[0m     \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39mitem() \u001b[39mif\u001b[39;00m np\u001b[39m.\u001b[39mndim(t) \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m \u001b[39melse\u001b[39;00m t\n\u001b[0;32m--> 694\u001b[0m \u001b[39mreturn\u001b[39;00m tf\u001b[39m.\u001b[39;49mnest\u001b[39m.\u001b[39;49mmap_structure(_to_single_numpy_or_python_type, tensors)\n",
      "File \u001b[0;32m~/.conda/envs/relconvnet/lib/python3.10/site-packages/tensorflow/python/util/nest.py:624\u001b[0m, in \u001b[0;36mmap_structure\u001b[0;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[1;32m    538\u001b[0m \u001b[39m@tf_export\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mnest.map_structure\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    539\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mmap_structure\u001b[39m(func, \u001b[39m*\u001b[39mstructure, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    540\u001b[0m \u001b[39m  \u001b[39m\u001b[39m\"\"\"Creates a new structure by applying `func` to each atom in `structure`.\u001b[39;00m\n\u001b[1;32m    541\u001b[0m \n\u001b[1;32m    542\u001b[0m \u001b[39m  Refer to [tf.nest](https://www.tensorflow.org/api_docs/python/tf/nest)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    622\u001b[0m \u001b[39m    ValueError: If wrong keyword arguments are provided.\u001b[39;00m\n\u001b[1;32m    623\u001b[0m \u001b[39m  \"\"\"\u001b[39;00m\n\u001b[0;32m--> 624\u001b[0m   \u001b[39mreturn\u001b[39;00m nest_util\u001b[39m.\u001b[39;49mmap_structure(\n\u001b[1;32m    625\u001b[0m       nest_util\u001b[39m.\u001b[39;49mModality\u001b[39m.\u001b[39;49mCORE, func, \u001b[39m*\u001b[39;49mstructure, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs\n\u001b[1;32m    626\u001b[0m   )\n",
      "File \u001b[0;32m~/.conda/envs/relconvnet/lib/python3.10/site-packages/tensorflow/python/util/nest_util.py:1054\u001b[0m, in \u001b[0;36mmap_structure\u001b[0;34m(modality, func, *structure, **kwargs)\u001b[0m\n\u001b[1;32m    957\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Creates a new structure by applying `func` to each atom in `structure`.\u001b[39;00m\n\u001b[1;32m    958\u001b[0m \n\u001b[1;32m    959\u001b[0m \u001b[39m- For Modality.CORE: Refer to\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1051\u001b[0m \u001b[39m  ValueError: If wrong keyword arguments are provided.\u001b[39;00m\n\u001b[1;32m   1052\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1053\u001b[0m \u001b[39mif\u001b[39;00m modality \u001b[39m==\u001b[39m Modality\u001b[39m.\u001b[39mCORE:\n\u001b[0;32m-> 1054\u001b[0m   \u001b[39mreturn\u001b[39;00m _tf_core_map_structure(func, \u001b[39m*\u001b[39;49mstructure, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1055\u001b[0m \u001b[39melif\u001b[39;00m modality \u001b[39m==\u001b[39m Modality\u001b[39m.\u001b[39mDATA:\n\u001b[1;32m   1056\u001b[0m   \u001b[39mreturn\u001b[39;00m _tf_data_map_structure(func, \u001b[39m*\u001b[39mstructure, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/.conda/envs/relconvnet/lib/python3.10/site-packages/tensorflow/python/util/nest_util.py:1094\u001b[0m, in \u001b[0;36m_tf_core_map_structure\u001b[0;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[1;32m   1089\u001b[0m flat_structure \u001b[39m=\u001b[39m (_tf_core_flatten(s, expand_composites) \u001b[39mfor\u001b[39;00m s \u001b[39min\u001b[39;00m structure)\n\u001b[1;32m   1090\u001b[0m entries \u001b[39m=\u001b[39m \u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39mflat_structure)\n\u001b[1;32m   1092\u001b[0m \u001b[39mreturn\u001b[39;00m _tf_core_pack_sequence_as(\n\u001b[1;32m   1093\u001b[0m     structure[\u001b[39m0\u001b[39m],\n\u001b[0;32m-> 1094\u001b[0m     [func(\u001b[39m*\u001b[39mx) \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m entries],\n\u001b[1;32m   1095\u001b[0m     expand_composites\u001b[39m=\u001b[39mexpand_composites,\n\u001b[1;32m   1096\u001b[0m )\n",
      "File \u001b[0;32m~/.conda/envs/relconvnet/lib/python3.10/site-packages/tensorflow/python/util/nest_util.py:1094\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1089\u001b[0m flat_structure \u001b[39m=\u001b[39m (_tf_core_flatten(s, expand_composites) \u001b[39mfor\u001b[39;00m s \u001b[39min\u001b[39;00m structure)\n\u001b[1;32m   1090\u001b[0m entries \u001b[39m=\u001b[39m \u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39mflat_structure)\n\u001b[1;32m   1092\u001b[0m \u001b[39mreturn\u001b[39;00m _tf_core_pack_sequence_as(\n\u001b[1;32m   1093\u001b[0m     structure[\u001b[39m0\u001b[39m],\n\u001b[0;32m-> 1094\u001b[0m     [func(\u001b[39m*\u001b[39;49mx) \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m entries],\n\u001b[1;32m   1095\u001b[0m     expand_composites\u001b[39m=\u001b[39mexpand_composites,\n\u001b[1;32m   1096\u001b[0m )\n",
      "File \u001b[0;32m~/.conda/envs/relconvnet/lib/python3.10/site-packages/keras/src/utils/tf_utils.py:687\u001b[0m, in \u001b[0;36msync_to_numpy_or_python_type.<locals>._to_single_numpy_or_python_type\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    684\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_to_single_numpy_or_python_type\u001b[39m(t):\n\u001b[1;32m    685\u001b[0m     \u001b[39m# Don't turn ragged or sparse tensors to NumPy.\u001b[39;00m\n\u001b[1;32m    686\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(t, tf\u001b[39m.\u001b[39mTensor):\n\u001b[0;32m--> 687\u001b[0m         t \u001b[39m=\u001b[39m t\u001b[39m.\u001b[39;49mnumpy()\n\u001b[1;32m    688\u001b[0m     \u001b[39m# Strings, ragged and sparse tensors don't have .item(). Return them\u001b[39;00m\n\u001b[1;32m    689\u001b[0m     \u001b[39m# as-is.\u001b[39;00m\n\u001b[1;32m    690\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(t, (np\u001b[39m.\u001b[39mndarray, np\u001b[39m.\u001b[39mgeneric)):\n",
      "File \u001b[0;32m~/.conda/envs/relconvnet/lib/python3.10/site-packages/tensorflow/python/framework/ops.py:1141\u001b[0m, in \u001b[0;36m_EagerTensorBase.numpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1118\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Copy of the contents of this Tensor into a NumPy array or scalar.\u001b[39;00m\n\u001b[1;32m   1119\u001b[0m \n\u001b[1;32m   1120\u001b[0m \u001b[39mUnlike NumPy arrays, Tensors are immutable, so this method has to copy\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1138\u001b[0m \u001b[39m    NumPy dtype.\u001b[39;00m\n\u001b[1;32m   1139\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1140\u001b[0m \u001b[39m# TODO(slebedev): Consider avoiding a copy for non-CPU or remote tensors.\u001b[39;00m\n\u001b[0;32m-> 1141\u001b[0m maybe_arr \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_numpy()  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m   1142\u001b[0m \u001b[39mreturn\u001b[39;00m maybe_arr\u001b[39m.\u001b[39mcopy() \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(maybe_arr, np\u001b[39m.\u001b[39mndarray) \u001b[39melse\u001b[39;00m maybe_arr\n",
      "File \u001b[0;32m~/.conda/envs/relconvnet/lib/python3.10/site-packages/tensorflow/python/framework/ops.py:1107\u001b[0m, in \u001b[0;36m_EagerTensorBase._numpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1105\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_numpy\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m   1106\u001b[0m   \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1107\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_numpy_internal()\n\u001b[1;32m   1108\u001b[0m   \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m   1109\u001b[0m     \u001b[39mraise\u001b[39;00m core\u001b[39m.\u001b[39m_status_to_exception(e) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_size = -1\n",
    "X_train_ = X_train[:train_size]\n",
    "y_train_ = y_train[:train_size]\n",
    "history = transformer_model.fit(X_train_, y_train_, validation_data=(X_val, y_val), epochs=100, verbose=1, callbacks=create_callbacks())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.plot_history(history, ('loss', 'acc'));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "81/81 [==============================] - 0s 2ms/step - loss: 2.9267 - acc: 0.5586\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'loss': 2.92673397064209, 'acc': 0.5586419701576233}"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformer_model.evaluate(X_test, y_test, return_dict=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CorelNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_dense_size = 64\n",
    "def create_corelnet():\n",
    "    corelnet = tf.keras.layers.Lambda(lambda x: tf.matmul(x, x, transpose_b=True), name='similarity_matrix')\n",
    "\n",
    "    model = tf.keras.Sequential(\n",
    "        [\n",
    "            corelnet,\n",
    "            tf.keras.layers.Softmax(axis=-1, name='softmax'),\n",
    "            tf.keras.layers.Flatten(name='flatten'),\n",
    "            tf.keras.layers.Dense(hidden_dense_size, activation='relu', name='hidden_dense1'),\n",
    "            tf.keras.layers.Dense(2, name='output')],\n",
    "        name='corelnet')\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"corelnet\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " similarity_matrix (Lambda)  (32, 5, 5)                0         \n",
      "                                                                 \n",
      " softmax (Softmax)           (32, 5, 5)                0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (32, 25)                  0         \n",
      "                                                                 \n",
      " hidden_dense1 (Dense)       (32, 64)                  1664      \n",
      "                                                                 \n",
      " output (Dense)              (32, 2)                   130       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1794 (7.01 KB)\n",
      "Trainable params: 1794 (7.01 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "corelnet_model = create_corelnet()\n",
    "corelnet_model.compile(loss=loss, optimizer=create_opt(), metrics=['acc'])\n",
    "corelnet_model(X_train[:32]); # build\n",
    "corelnet_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "567/567 [==============================] - 2s 3ms/step - loss: 0.6667 - acc: 0.5436 - val_loss: 0.6554 - val_acc: 0.5563\n",
      "Epoch 2/100\n",
      "567/567 [==============================] - 1s 2ms/step - loss: 0.6502 - acc: 0.5537 - val_loss: 0.6533 - val_acc: 0.5563\n",
      "Epoch 3/100\n",
      "567/567 [==============================] - 1s 2ms/step - loss: 0.6498 - acc: 0.5507 - val_loss: 0.6545 - val_acc: 0.5000\n",
      "Epoch 4/100\n",
      "567/567 [==============================] - 1s 2ms/step - loss: 0.6500 - acc: 0.5554 - val_loss: 0.6556 - val_acc: 0.5000\n",
      "Epoch 5/100\n",
      "567/567 [==============================] - 1s 2ms/step - loss: 0.6497 - acc: 0.5512 - val_loss: 0.6526 - val_acc: 0.5563\n",
      "Epoch 6/100\n",
      "567/567 [==============================] - 1s 2ms/step - loss: 0.6484 - acc: 0.5581 - val_loss: 0.6525 - val_acc: 0.5563\n",
      "Epoch 7/100\n",
      "567/567 [==============================] - 1s 2ms/step - loss: 0.6485 - acc: 0.5517 - val_loss: 0.6557 - val_acc: 0.5563\n",
      "Epoch 8/100\n",
      "567/567 [==============================] - 1s 2ms/step - loss: 0.6486 - acc: 0.5527 - val_loss: 0.6527 - val_acc: 0.5563\n",
      "Epoch 9/100\n",
      "567/567 [==============================] - 1s 2ms/step - loss: 0.6479 - acc: 0.5570 - val_loss: 0.6525 - val_acc: 0.5563\n",
      "Epoch 10/100\n",
      "567/567 [==============================] - 1s 2ms/step - loss: 0.6484 - acc: 0.5536 - val_loss: 0.6526 - val_acc: 0.5563\n",
      "Epoch 11/100\n",
      "567/567 [==============================] - 1s 2ms/step - loss: 0.6483 - acc: 0.5602 - val_loss: 0.6554 - val_acc: 0.5000\n",
      "Epoch 12/100\n",
      "567/567 [==============================] - 1s 2ms/step - loss: 0.6483 - acc: 0.5560 - val_loss: 0.6525 - val_acc: 0.5563\n",
      "Epoch 13/100\n",
      "567/567 [==============================] - 1s 2ms/step - loss: 0.6481 - acc: 0.5561 - val_loss: 0.6527 - val_acc: 0.5563\n",
      "Epoch 14/100\n",
      "567/567 [==============================] - 1s 2ms/step - loss: 0.6487 - acc: 0.5560 - val_loss: 0.6530 - val_acc: 0.5563\n",
      "Epoch 15/100\n",
      "567/567 [==============================] - 1s 2ms/step - loss: 0.6480 - acc: 0.5566 - val_loss: 0.6538 - val_acc: 0.5563\n",
      "Epoch 16/100\n",
      "567/567 [==============================] - 1s 2ms/step - loss: 0.6483 - acc: 0.5577 - val_loss: 0.6589 - val_acc: 0.5563\n",
      "Epoch 17/100\n",
      "567/567 [==============================] - 1s 2ms/step - loss: 0.6482 - acc: 0.5589 - val_loss: 0.6525 - val_acc: 0.5563\n",
      "Epoch 18/100\n",
      "567/567 [==============================] - 1s 2ms/step - loss: 0.6479 - acc: 0.5619 - val_loss: 0.6547 - val_acc: 0.5563\n",
      "Epoch 19/100\n",
      "567/567 [==============================] - 1s 2ms/step - loss: 0.6480 - acc: 0.5578 - val_loss: 0.6527 - val_acc: 0.5563\n",
      "Epoch 20/100\n",
      "567/567 [==============================] - 1s 2ms/step - loss: 0.6483 - acc: 0.5554 - val_loss: 0.6525 - val_acc: 0.5563\n",
      "Epoch 21/100\n",
      "567/567 [==============================] - 1s 2ms/step - loss: 0.6476 - acc: 0.5612 - val_loss: 0.6524 - val_acc: 0.5563\n",
      "Epoch 22/100\n",
      "567/567 [==============================] - 1s 2ms/step - loss: 0.6476 - acc: 0.5608 - val_loss: 0.6529 - val_acc: 0.5563\n",
      "Epoch 23/100\n",
      "567/567 [==============================] - 1s 2ms/step - loss: 0.6477 - acc: 0.5625 - val_loss: 0.6530 - val_acc: 0.5563\n",
      "Epoch 24/100\n",
      "567/567 [==============================] - 1s 2ms/step - loss: 0.6476 - acc: 0.5608 - val_loss: 0.6551 - val_acc: 0.5563\n",
      "Epoch 25/100\n",
      "567/567 [==============================] - 1s 2ms/step - loss: 0.6476 - acc: 0.5585 - val_loss: 0.6527 - val_acc: 0.5563\n",
      "Epoch 26/100\n",
      "567/567 [==============================] - 1s 2ms/step - loss: 0.6476 - acc: 0.5630 - val_loss: 0.6524 - val_acc: 0.5563\n",
      "Epoch 27/100\n",
      "567/567 [==============================] - 1s 2ms/step - loss: 0.6475 - acc: 0.5636 - val_loss: 0.6534 - val_acc: 0.5563\n",
      "Epoch 28/100\n",
      "567/567 [==============================] - 1s 2ms/step - loss: 0.6475 - acc: 0.5636 - val_loss: 0.6525 - val_acc: 0.5563\n",
      "Epoch 29/100\n",
      "567/567 [==============================] - 1s 2ms/step - loss: 0.6475 - acc: 0.5623 - val_loss: 0.6536 - val_acc: 0.5563\n",
      "Epoch 30/100\n",
      "567/567 [==============================] - 1s 2ms/step - loss: 0.6478 - acc: 0.5603 - val_loss: 0.6526 - val_acc: 0.5563\n",
      "Epoch 31/100\n",
      "567/567 [==============================] - 1s 2ms/step - loss: 0.6472 - acc: 0.5625 - val_loss: 0.6535 - val_acc: 0.5563\n",
      "Epoch 32/100\n",
      "567/567 [==============================] - 1s 2ms/step - loss: 0.6474 - acc: 0.5634 - val_loss: 0.6526 - val_acc: 0.5563\n",
      "Epoch 33/100\n",
      "567/567 [==============================] - 1s 2ms/step - loss: 0.6475 - acc: 0.5623 - val_loss: 0.6524 - val_acc: 0.5563\n",
      "Epoch 34/100\n",
      "567/567 [==============================] - 1s 2ms/step - loss: 0.6475 - acc: 0.5636 - val_loss: 0.6524 - val_acc: 0.5563\n",
      "Epoch 35/100\n",
      "567/567 [==============================] - 1s 2ms/step - loss: 0.6473 - acc: 0.5609 - val_loss: 0.6524 - val_acc: 0.5563\n",
      "Epoch 36/100\n",
      "567/567 [==============================] - 1s 2ms/step - loss: 0.6472 - acc: 0.5636 - val_loss: 0.6524 - val_acc: 0.5563\n",
      "Epoch 37/100\n",
      "567/567 [==============================] - 1s 2ms/step - loss: 0.6473 - acc: 0.5636 - val_loss: 0.6525 - val_acc: 0.5563\n",
      "Epoch 38/100\n",
      "567/567 [==============================] - 1s 2ms/step - loss: 0.6473 - acc: 0.5636 - val_loss: 0.6527 - val_acc: 0.5563\n",
      "Epoch 39/100\n",
      "567/567 [==============================] - 1s 2ms/step - loss: 0.6472 - acc: 0.5636 - val_loss: 0.6524 - val_acc: 0.5563\n",
      "Epoch 40/100\n",
      "567/567 [==============================] - 1s 2ms/step - loss: 0.6470 - acc: 0.5636 - val_loss: 0.6525 - val_acc: 0.5563\n",
      "Epoch 41/100\n",
      "567/567 [==============================] - 1s 2ms/step - loss: 0.6471 - acc: 0.5636 - val_loss: 0.6526 - val_acc: 0.5563\n",
      "Epoch 42/100\n",
      "567/567 [==============================] - 1s 2ms/step - loss: 0.6472 - acc: 0.5636 - val_loss: 0.6525 - val_acc: 0.5563\n",
      "Epoch 43/100\n",
      "567/567 [==============================] - 1s 2ms/step - loss: 0.6471 - acc: 0.5636 - val_loss: 0.6526 - val_acc: 0.5563\n",
      "Epoch 44/100\n",
      "567/567 [==============================] - 1s 2ms/step - loss: 0.6472 - acc: 0.5636 - val_loss: 0.6525 - val_acc: 0.5563\n",
      "Epoch 45/100\n",
      "567/567 [==============================] - 1s 2ms/step - loss: 0.6471 - acc: 0.5636 - val_loss: 0.6524 - val_acc: 0.5563\n",
      "Epoch 46/100\n",
      "567/567 [==============================] - 1s 2ms/step - loss: 0.6471 - acc: 0.5636 - val_loss: 0.6524 - val_acc: 0.5563\n",
      "Epoch 47/100\n",
      "567/567 [==============================] - 1s 2ms/step - loss: 0.6471 - acc: 0.5636 - val_loss: 0.6526 - val_acc: 0.5563\n",
      "Epoch 48/100\n",
      "567/567 [==============================] - 1s 2ms/step - loss: 0.6471 - acc: 0.5636 - val_loss: 0.6526 - val_acc: 0.5563\n",
      "Epoch 49/100\n",
      "567/567 [==============================] - 1s 2ms/step - loss: 0.6471 - acc: 0.5636 - val_loss: 0.6533 - val_acc: 0.5563\n",
      "Epoch 50/100\n",
      "567/567 [==============================] - 1s 2ms/step - loss: 0.6473 - acc: 0.5614 - val_loss: 0.6525 - val_acc: 0.5563\n",
      "Epoch 51/100\n",
      "567/567 [==============================] - 1s 2ms/step - loss: 0.6471 - acc: 0.5636 - val_loss: 0.6527 - val_acc: 0.5563\n",
      "Epoch 52/100\n",
      "567/567 [==============================] - 1s 2ms/step - loss: 0.6472 - acc: 0.5621 - val_loss: 0.6525 - val_acc: 0.5563\n",
      "Epoch 53/100\n",
      "567/567 [==============================] - 1s 2ms/step - loss: 0.6471 - acc: 0.5636 - val_loss: 0.6526 - val_acc: 0.5563\n",
      "Epoch 54/100\n",
      "567/567 [==============================] - 1s 2ms/step - loss: 0.6470 - acc: 0.5636 - val_loss: 0.6528 - val_acc: 0.5563\n",
      "Epoch 55/100\n",
      "567/567 [==============================] - 1s 2ms/step - loss: 0.6471 - acc: 0.5636 - val_loss: 0.6527 - val_acc: 0.5563\n",
      "Epoch 56/100\n",
      "567/567 [==============================] - 1s 2ms/step - loss: 0.6472 - acc: 0.5636 - val_loss: 0.6526 - val_acc: 0.5563\n",
      "Epoch 57/100\n",
      "567/567 [==============================] - 1s 2ms/step - loss: 0.6471 - acc: 0.5631 - val_loss: 0.6524 - val_acc: 0.5563\n",
      "Epoch 58/100\n",
      "567/567 [==============================] - 1s 2ms/step - loss: 0.6471 - acc: 0.5636 - val_loss: 0.6526 - val_acc: 0.5563\n",
      "Epoch 59/100\n",
      "567/567 [==============================] - 1s 2ms/step - loss: 0.6471 - acc: 0.5636 - val_loss: 0.6525 - val_acc: 0.5563\n",
      "Epoch 60/100\n",
      "567/567 [==============================] - 1s 2ms/step - loss: 0.6471 - acc: 0.5636 - val_loss: 0.6525 - val_acc: 0.5563\n",
      "Epoch 61/100\n",
      "567/567 [==============================] - 1s 2ms/step - loss: 0.6471 - acc: 0.5636 - val_loss: 0.6525 - val_acc: 0.5563\n",
      "Epoch 62/100\n",
      "567/567 [==============================] - 1s 2ms/step - loss: 0.6471 - acc: 0.5636 - val_loss: 0.6524 - val_acc: 0.5563\n",
      "Epoch 63/100\n",
      "567/567 [==============================] - 1s 2ms/step - loss: 0.6471 - acc: 0.5636 - val_loss: 0.6525 - val_acc: 0.5563\n",
      "Epoch 64/100\n",
      "567/567 [==============================] - 1s 2ms/step - loss: 0.6470 - acc: 0.5636 - val_loss: 0.6524 - val_acc: 0.5563\n",
      "Epoch 65/100\n",
      "567/567 [==============================] - 1s 2ms/step - loss: 0.6470 - acc: 0.5636 - val_loss: 0.6525 - val_acc: 0.5563\n",
      "Epoch 66/100\n",
      "567/567 [==============================] - 1s 2ms/step - loss: 0.6471 - acc: 0.5636 - val_loss: 0.6524 - val_acc: 0.5563\n",
      "Epoch 67/100\n",
      "567/567 [==============================] - 1s 2ms/step - loss: 0.6470 - acc: 0.5636 - val_loss: 0.6525 - val_acc: 0.5563\n",
      "Epoch 68/100\n",
      "567/567 [==============================] - 1s 2ms/step - loss: 0.6470 - acc: 0.5636 - val_loss: 0.6524 - val_acc: 0.5563\n",
      "Epoch 69/100\n",
      "567/567 [==============================] - 1s 2ms/step - loss: 0.6470 - acc: 0.5636 - val_loss: 0.6526 - val_acc: 0.5563\n",
      "Epoch 70/100\n",
      "567/567 [==============================] - 1s 2ms/step - loss: 0.6471 - acc: 0.5625 - val_loss: 0.6524 - val_acc: 0.5563\n",
      "Epoch 71/100\n",
      "567/567 [==============================] - 1s 2ms/step - loss: 0.6471 - acc: 0.5636 - val_loss: 0.6525 - val_acc: 0.5563\n",
      "Epoch 72/100\n",
      "567/567 [==============================] - 1s 2ms/step - loss: 0.6471 - acc: 0.5636 - val_loss: 0.6525 - val_acc: 0.5563\n",
      "Epoch 73/100\n",
      "567/567 [==============================] - 1s 2ms/step - loss: 0.6470 - acc: 0.5636 - val_loss: 0.6525 - val_acc: 0.5563\n",
      "Epoch 74/100\n",
      "567/567 [==============================] - 1s 2ms/step - loss: 0.6470 - acc: 0.5636 - val_loss: 0.6524 - val_acc: 0.5563\n",
      "Epoch 75/100\n",
      "567/567 [==============================] - 1s 2ms/step - loss: 0.6470 - acc: 0.5636 - val_loss: 0.6524 - val_acc: 0.5563\n",
      "Epoch 76/100\n",
      "567/567 [==============================] - 1s 2ms/step - loss: 0.6470 - acc: 0.5636 - val_loss: 0.6525 - val_acc: 0.5563\n",
      "Epoch 77/100\n",
      "567/567 [==============================] - 1s 2ms/step - loss: 0.6470 - acc: 0.5636 - val_loss: 0.6525 - val_acc: 0.5563\n",
      "Epoch 78/100\n",
      "567/567 [==============================] - 1s 2ms/step - loss: 0.6471 - acc: 0.5623 - val_loss: 0.6524 - val_acc: 0.5563\n",
      "Epoch 79/100\n",
      "567/567 [==============================] - 1s 2ms/step - loss: 0.6470 - acc: 0.5636 - val_loss: 0.6524 - val_acc: 0.5563\n",
      "Epoch 80/100\n",
      "567/567 [==============================] - 1s 2ms/step - loss: 0.6470 - acc: 0.5636 - val_loss: 0.6524 - val_acc: 0.5563\n",
      "Epoch 81/100\n",
      "567/567 [==============================] - 1s 2ms/step - loss: 0.6470 - acc: 0.5636 - val_loss: 0.6525 - val_acc: 0.5563\n"
     ]
    }
   ],
   "source": [
    "train_size = -1\n",
    "X_train_ = X_train[:train_size]\n",
    "y_train_ = y_train[:train_size]\n",
    "history = corelnet_model.fit(X_train_, y_train_, validation_data=(X_val, y_val), epochs=n_epochs, verbose=1, callbacks=create_callbacks())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.plot_history(history, ('loss', 'acc'));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "81/81 [==============================] - 0s 2ms/step - loss: 0.6594 - acc: 0.5556\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.6594493985176086, 0.5555555820465088]"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corelnet_model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CorelNet (no-softmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_dense_size = 64\n",
    "def create_nosoftmax_corelnet():\n",
    "    corelnet = tf.keras.layers.Lambda(lambda x: tf.matmul(x, x, transpose_b=True), name='similarity_matrix')\n",
    "\n",
    "    model = tf.keras.Sequential(\n",
    "        [\n",
    "            corelnet,\n",
    "            # tf.keras.layers.Softmax(axis=-1, name='softmax'),\n",
    "            tf.keras.layers.Flatten(name='flatten'),\n",
    "            tf.keras.layers.Dense(hidden_dense_size, activation='relu', name='hidden_dense1'),\n",
    "            tf.keras.layers.Dense(2, name='output')],\n",
    "        name='corelnet')\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"corelnet\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " similarity_matrix (Lambda)  (32, 5, 5)                0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (32, 25)                  0         \n",
      "                                                                 \n",
      " hidden_dense1 (Dense)       (32, 64)                  1664      \n",
      "                                                                 \n",
      " output (Dense)              (32, 2)                   130       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1794 (7.01 KB)\n",
      "Trainable params: 1794 (7.01 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "corelnet_nosoftmax_model = create_nosoftmax_corelnet()\n",
    "corelnet_nosoftmax_model.compile(loss=loss, optimizer=create_opt(), metrics=['acc'])\n",
    "corelnet_nosoftmax_model(X_train[:32]); # build\n",
    "corelnet_nosoftmax_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "284/284 [==============================] - 1s 2ms/step - loss: 0.8701 - acc: 0.5749 - val_loss: 0.7698 - val_acc: 0.5949\n",
      "Epoch 2/100\n",
      "284/284 [==============================] - 1s 2ms/step - loss: 0.7391 - acc: 0.5997 - val_loss: 0.6884 - val_acc: 0.6034\n",
      "Epoch 3/100\n",
      "284/284 [==============================] - 1s 2ms/step - loss: 0.6726 - acc: 0.6188 - val_loss: 0.7774 - val_acc: 0.5818\n",
      "Epoch 4/100\n",
      "284/284 [==============================] - 1s 2ms/step - loss: 0.6617 - acc: 0.6203 - val_loss: 0.6653 - val_acc: 0.6265\n",
      "Epoch 5/100\n",
      "284/284 [==============================] - 1s 2ms/step - loss: 0.6472 - acc: 0.6331 - val_loss: 0.6539 - val_acc: 0.6096\n",
      "Epoch 6/100\n",
      "284/284 [==============================] - 1s 2ms/step - loss: 0.6441 - acc: 0.6330 - val_loss: 0.6565 - val_acc: 0.6235\n",
      "Epoch 7/100\n",
      "284/284 [==============================] - 1s 2ms/step - loss: 0.6379 - acc: 0.6427 - val_loss: 0.7028 - val_acc: 0.6103\n",
      "Epoch 8/100\n",
      "284/284 [==============================] - 1s 2ms/step - loss: 0.6271 - acc: 0.6450 - val_loss: 0.6466 - val_acc: 0.6312\n",
      "Epoch 9/100\n",
      "284/284 [==============================] - 1s 2ms/step - loss: 0.6319 - acc: 0.6417 - val_loss: 0.6504 - val_acc: 0.6289\n",
      "Epoch 10/100\n",
      "284/284 [==============================] - 1s 2ms/step - loss: 0.6118 - acc: 0.6501 - val_loss: 0.6382 - val_acc: 0.6358\n",
      "Epoch 11/100\n",
      "284/284 [==============================] - 1s 2ms/step - loss: 0.6190 - acc: 0.6425 - val_loss: 0.6346 - val_acc: 0.6458\n",
      "Epoch 12/100\n",
      "284/284 [==============================] - 1s 2ms/step - loss: 0.6304 - acc: 0.6401 - val_loss: 0.6442 - val_acc: 0.6304\n",
      "Epoch 13/100\n",
      "284/284 [==============================] - 1s 2ms/step - loss: 0.6176 - acc: 0.6509 - val_loss: 0.6312 - val_acc: 0.6312\n",
      "Epoch 14/100\n",
      "284/284 [==============================] - 1s 2ms/step - loss: 0.6144 - acc: 0.6467 - val_loss: 0.6152 - val_acc: 0.6497\n",
      "Epoch 15/100\n",
      "284/284 [==============================] - 1s 2ms/step - loss: 0.6006 - acc: 0.6548 - val_loss: 0.6334 - val_acc: 0.6350\n",
      "Epoch 16/100\n",
      "284/284 [==============================] - 1s 2ms/step - loss: 0.5998 - acc: 0.6594 - val_loss: 0.6337 - val_acc: 0.6404\n",
      "Epoch 17/100\n",
      "284/284 [==============================] - 1s 2ms/step - loss: 0.5915 - acc: 0.6697 - val_loss: 0.6207 - val_acc: 0.6466\n",
      "Epoch 18/100\n",
      "284/284 [==============================] - 1s 2ms/step - loss: 0.6074 - acc: 0.6509 - val_loss: 0.6242 - val_acc: 0.6528\n",
      "Epoch 19/100\n",
      "284/284 [==============================] - 1s 2ms/step - loss: 0.5948 - acc: 0.6549 - val_loss: 0.6481 - val_acc: 0.6235\n",
      "Epoch 20/100\n",
      "284/284 [==============================] - 1s 3ms/step - loss: 0.5925 - acc: 0.6653 - val_loss: 0.6911 - val_acc: 0.6265\n",
      "Epoch 21/100\n",
      "284/284 [==============================] - 1s 2ms/step - loss: 0.5936 - acc: 0.6630 - val_loss: 0.6496 - val_acc: 0.6250\n",
      "Epoch 22/100\n",
      "284/284 [==============================] - 1s 2ms/step - loss: 0.5881 - acc: 0.6653 - val_loss: 0.6150 - val_acc: 0.6389\n",
      "Epoch 23/100\n",
      "284/284 [==============================] - 1s 2ms/step - loss: 0.5877 - acc: 0.6648 - val_loss: 0.6427 - val_acc: 0.6296\n",
      "Epoch 24/100\n",
      "284/284 [==============================] - 1s 2ms/step - loss: 0.5890 - acc: 0.6649 - val_loss: 0.6378 - val_acc: 0.6381\n",
      "Epoch 25/100\n",
      "284/284 [==============================] - 1s 2ms/step - loss: 0.5804 - acc: 0.6669 - val_loss: 0.6131 - val_acc: 0.6404\n",
      "Epoch 26/100\n",
      "284/284 [==============================] - 1s 2ms/step - loss: 0.5802 - acc: 0.6741 - val_loss: 0.6153 - val_acc: 0.6404\n",
      "Epoch 27/100\n",
      "284/284 [==============================] - 1s 2ms/step - loss: 0.5856 - acc: 0.6692 - val_loss: 0.6656 - val_acc: 0.6196\n",
      "Epoch 28/100\n",
      "284/284 [==============================] - 1s 2ms/step - loss: 0.5862 - acc: 0.6686 - val_loss: 0.6413 - val_acc: 0.6304\n",
      "Epoch 29/100\n",
      "284/284 [==============================] - 1s 2ms/step - loss: 0.5741 - acc: 0.6763 - val_loss: 0.6354 - val_acc: 0.6343\n",
      "Epoch 30/100\n",
      "284/284 [==============================] - 1s 2ms/step - loss: 0.5698 - acc: 0.6785 - val_loss: 0.6318 - val_acc: 0.6451\n",
      "Epoch 31/100\n",
      "284/284 [==============================] - 1s 2ms/step - loss: 0.5745 - acc: 0.6732 - val_loss: 0.6454 - val_acc: 0.6373\n",
      "Epoch 32/100\n",
      "284/284 [==============================] - 1s 2ms/step - loss: 0.5720 - acc: 0.6772 - val_loss: 0.6692 - val_acc: 0.6196\n",
      "Epoch 33/100\n",
      "284/284 [==============================] - 1s 2ms/step - loss: 0.5729 - acc: 0.6745 - val_loss: 0.6447 - val_acc: 0.6304\n",
      "Epoch 34/100\n",
      "284/284 [==============================] - 1s 2ms/step - loss: 0.5680 - acc: 0.6817 - val_loss: 0.6228 - val_acc: 0.6389\n",
      "Epoch 35/100\n",
      "284/284 [==============================] - 1s 2ms/step - loss: 0.5604 - acc: 0.6810 - val_loss: 0.6191 - val_acc: 0.6543\n",
      "Epoch 36/100\n",
      "284/284 [==============================] - 1s 2ms/step - loss: 0.5695 - acc: 0.6785 - val_loss: 0.6246 - val_acc: 0.6497\n",
      "Epoch 37/100\n",
      "284/284 [==============================] - 1s 2ms/step - loss: 0.5613 - acc: 0.6857 - val_loss: 0.6094 - val_acc: 0.6443\n",
      "Epoch 38/100\n",
      "284/284 [==============================] - 1s 2ms/step - loss: 0.5631 - acc: 0.6821 - val_loss: 0.6097 - val_acc: 0.6497\n",
      "Epoch 39/100\n",
      "284/284 [==============================] - 1s 2ms/step - loss: 0.5600 - acc: 0.6890 - val_loss: 0.6119 - val_acc: 0.6458\n",
      "Epoch 40/100\n",
      "284/284 [==============================] - 1s 2ms/step - loss: 0.5572 - acc: 0.6852 - val_loss: 0.6901 - val_acc: 0.6057\n",
      "Epoch 41/100\n",
      "284/284 [==============================] - 1s 2ms/step - loss: 0.5604 - acc: 0.6855 - val_loss: 0.6093 - val_acc: 0.6613\n",
      "Epoch 42/100\n",
      "284/284 [==============================] - 1s 2ms/step - loss: 0.5550 - acc: 0.6881 - val_loss: 0.6159 - val_acc: 0.6427\n",
      "Epoch 43/100\n",
      "284/284 [==============================] - 1s 2ms/step - loss: 0.5506 - acc: 0.6977 - val_loss: 0.6075 - val_acc: 0.6466\n",
      "Epoch 44/100\n",
      "284/284 [==============================] - 1s 2ms/step - loss: 0.5507 - acc: 0.6886 - val_loss: 0.6205 - val_acc: 0.6458\n",
      "Epoch 45/100\n",
      "284/284 [==============================] - 1s 2ms/step - loss: 0.5532 - acc: 0.6909 - val_loss: 0.6111 - val_acc: 0.6543\n",
      "Epoch 46/100\n",
      "284/284 [==============================] - 1s 2ms/step - loss: 0.5545 - acc: 0.6891 - val_loss: 0.6074 - val_acc: 0.6435\n",
      "Epoch 47/100\n",
      "284/284 [==============================] - 1s 2ms/step - loss: 0.5549 - acc: 0.6898 - val_loss: 0.6029 - val_acc: 0.6597\n",
      "Epoch 48/100\n",
      "284/284 [==============================] - 1s 2ms/step - loss: 0.5495 - acc: 0.6918 - val_loss: 0.6057 - val_acc: 0.6543\n",
      "Epoch 49/100\n",
      "284/284 [==============================] - 1s 2ms/step - loss: 0.5473 - acc: 0.6928 - val_loss: 0.6073 - val_acc: 0.6636\n",
      "Epoch 50/100\n",
      "284/284 [==============================] - 1s 2ms/step - loss: 0.5488 - acc: 0.6962 - val_loss: 0.6073 - val_acc: 0.6505\n",
      "Epoch 51/100\n",
      "284/284 [==============================] - 1s 2ms/step - loss: 0.5450 - acc: 0.6976 - val_loss: 0.6216 - val_acc: 0.6443\n",
      "Epoch 52/100\n",
      "284/284 [==============================] - 1s 2ms/step - loss: 0.5464 - acc: 0.6956 - val_loss: 0.5960 - val_acc: 0.6559\n",
      "Epoch 53/100\n",
      "284/284 [==============================] - 1s 2ms/step - loss: 0.5482 - acc: 0.6939 - val_loss: 0.6658 - val_acc: 0.6242\n",
      "Epoch 54/100\n",
      "284/284 [==============================] - 1s 2ms/step - loss: 0.5441 - acc: 0.6997 - val_loss: 0.5967 - val_acc: 0.6574\n",
      "Epoch 55/100\n",
      "284/284 [==============================] - 1s 2ms/step - loss: 0.5431 - acc: 0.7037 - val_loss: 0.6015 - val_acc: 0.6620\n",
      "Epoch 56/100\n",
      "284/284 [==============================] - 1s 2ms/step - loss: 0.5453 - acc: 0.6950 - val_loss: 0.6041 - val_acc: 0.6543\n",
      "Epoch 57/100\n",
      "284/284 [==============================] - 1s 2ms/step - loss: 0.5412 - acc: 0.6983 - val_loss: 0.6206 - val_acc: 0.6505\n",
      "Epoch 58/100\n",
      "284/284 [==============================] - 1s 2ms/step - loss: 0.5404 - acc: 0.7007 - val_loss: 0.6131 - val_acc: 0.6505\n",
      "Epoch 59/100\n",
      "284/284 [==============================] - 1s 2ms/step - loss: 0.5406 - acc: 0.7026 - val_loss: 0.6106 - val_acc: 0.6512\n",
      "Epoch 60/100\n",
      "284/284 [==============================] - 1s 2ms/step - loss: 0.5429 - acc: 0.7039 - val_loss: 0.6429 - val_acc: 0.6373\n",
      "Epoch 61/100\n",
      "284/284 [==============================] - 1s 2ms/step - loss: 0.5369 - acc: 0.7075 - val_loss: 0.6001 - val_acc: 0.6435\n",
      "Epoch 62/100\n",
      "284/284 [==============================] - 1s 2ms/step - loss: 0.5403 - acc: 0.7011 - val_loss: 0.6115 - val_acc: 0.6427\n",
      "Epoch 63/100\n",
      "284/284 [==============================] - 1s 2ms/step - loss: 0.5382 - acc: 0.7051 - val_loss: 0.5999 - val_acc: 0.6466\n",
      "Epoch 64/100\n",
      "284/284 [==============================] - 1s 2ms/step - loss: 0.5356 - acc: 0.7016 - val_loss: 0.6123 - val_acc: 0.6551\n",
      "Epoch 65/100\n",
      "284/284 [==============================] - 1s 2ms/step - loss: 0.5351 - acc: 0.7017 - val_loss: 0.6198 - val_acc: 0.6458\n",
      "Epoch 66/100\n",
      "284/284 [==============================] - 1s 2ms/step - loss: 0.5367 - acc: 0.6989 - val_loss: 0.6086 - val_acc: 0.6481\n",
      "Epoch 67/100\n",
      "284/284 [==============================] - 1s 2ms/step - loss: 0.5365 - acc: 0.7046 - val_loss: 0.6038 - val_acc: 0.6404\n",
      "Epoch 68/100\n",
      "284/284 [==============================] - 1s 2ms/step - loss: 0.5368 - acc: 0.7017 - val_loss: 0.6163 - val_acc: 0.6458\n",
      "Epoch 69/100\n",
      "284/284 [==============================] - 1s 2ms/step - loss: 0.5403 - acc: 0.7003 - val_loss: 0.6043 - val_acc: 0.6350\n",
      "Epoch 70/100\n",
      "284/284 [==============================] - 1s 2ms/step - loss: 0.5327 - acc: 0.7016 - val_loss: 0.6175 - val_acc: 0.6304\n",
      "Epoch 71/100\n",
      "284/284 [==============================] - 1s 2ms/step - loss: 0.5330 - acc: 0.7047 - val_loss: 0.6100 - val_acc: 0.6466\n",
      "Epoch 72/100\n",
      "284/284 [==============================] - 1s 2ms/step - loss: 0.5352 - acc: 0.7064 - val_loss: 0.6040 - val_acc: 0.6443\n",
      "Epoch 73/100\n",
      "284/284 [==============================] - 1s 2ms/step - loss: 0.5282 - acc: 0.7105 - val_loss: 0.6044 - val_acc: 0.6497\n",
      "Epoch 74/100\n",
      "284/284 [==============================] - 1s 2ms/step - loss: 0.5271 - acc: 0.7080 - val_loss: 0.6243 - val_acc: 0.6289\n",
      "Epoch 75/100\n",
      "284/284 [==============================] - 1s 2ms/step - loss: 0.5298 - acc: 0.7089 - val_loss: 0.6097 - val_acc: 0.6458\n",
      "Epoch 76/100\n",
      "284/284 [==============================] - 1s 2ms/step - loss: 0.5253 - acc: 0.7177 - val_loss: 0.6330 - val_acc: 0.6427\n",
      "Epoch 77/100\n",
      "284/284 [==============================] - 1s 2ms/step - loss: 0.5256 - acc: 0.7176 - val_loss: 0.6103 - val_acc: 0.6512\n",
      "Epoch 78/100\n",
      "284/284 [==============================] - 1s 2ms/step - loss: 0.5215 - acc: 0.7149 - val_loss: 0.6424 - val_acc: 0.6327\n",
      "Epoch 79/100\n",
      "284/284 [==============================] - 1s 2ms/step - loss: 0.5272 - acc: 0.7147 - val_loss: 0.6265 - val_acc: 0.6474\n",
      "Epoch 80/100\n",
      "284/284 [==============================] - 1s 2ms/step - loss: 0.5257 - acc: 0.7087 - val_loss: 0.6072 - val_acc: 0.6435\n",
      "Epoch 81/100\n",
      "284/284 [==============================] - 1s 2ms/step - loss: 0.5272 - acc: 0.7093 - val_loss: 0.6166 - val_acc: 0.6435\n",
      "Epoch 82/100\n",
      "284/284 [==============================] - 1s 2ms/step - loss: 0.5258 - acc: 0.7154 - val_loss: 0.6286 - val_acc: 0.6427\n",
      "Epoch 83/100\n",
      "284/284 [==============================] - 1s 2ms/step - loss: 0.5229 - acc: 0.7113 - val_loss: 0.6172 - val_acc: 0.6381\n",
      "Epoch 84/100\n",
      "284/284 [==============================] - 1s 2ms/step - loss: 0.5200 - acc: 0.7124 - val_loss: 0.6313 - val_acc: 0.6373\n",
      "Epoch 85/100\n",
      "284/284 [==============================] - 1s 2ms/step - loss: 0.5213 - acc: 0.7197 - val_loss: 0.6227 - val_acc: 0.6574\n",
      "Epoch 86/100\n",
      "284/284 [==============================] - 1s 2ms/step - loss: 0.5230 - acc: 0.7101 - val_loss: 0.6303 - val_acc: 0.6543\n",
      "Epoch 87/100\n",
      "284/284 [==============================] - 1s 2ms/step - loss: 0.5175 - acc: 0.7183 - val_loss: 0.6173 - val_acc: 0.6381\n",
      "Epoch 88/100\n",
      "284/284 [==============================] - 1s 2ms/step - loss: 0.5223 - acc: 0.7146 - val_loss: 0.6185 - val_acc: 0.6497\n",
      "Epoch 89/100\n",
      "284/284 [==============================] - 1s 2ms/step - loss: 0.5193 - acc: 0.7148 - val_loss: 0.6113 - val_acc: 0.6512\n",
      "Epoch 90/100\n",
      "284/284 [==============================] - 1s 2ms/step - loss: 0.5197 - acc: 0.7190 - val_loss: 0.6210 - val_acc: 0.6420\n",
      "Epoch 91/100\n",
      "284/284 [==============================] - 1s 2ms/step - loss: 0.5192 - acc: 0.7167 - val_loss: 0.6653 - val_acc: 0.6258\n",
      "Epoch 92/100\n",
      "284/284 [==============================] - 1s 2ms/step - loss: 0.5194 - acc: 0.7172 - val_loss: 0.6400 - val_acc: 0.6273\n",
      "Epoch 93/100\n",
      "284/284 [==============================] - 1s 2ms/step - loss: 0.5157 - acc: 0.7208 - val_loss: 0.6192 - val_acc: 0.6296\n",
      "Epoch 94/100\n",
      "284/284 [==============================] - 1s 2ms/step - loss: 0.5184 - acc: 0.7159 - val_loss: 0.6315 - val_acc: 0.6559\n",
      "Epoch 95/100\n",
      "284/284 [==============================] - 1s 2ms/step - loss: 0.5187 - acc: 0.7188 - val_loss: 0.6358 - val_acc: 0.6373\n",
      "Epoch 96/100\n",
      "284/284 [==============================] - 1s 2ms/step - loss: 0.5149 - acc: 0.7192 - val_loss: 0.6147 - val_acc: 0.6435\n",
      "Epoch 97/100\n",
      "284/284 [==============================] - 1s 2ms/step - loss: 0.5158 - acc: 0.7172 - val_loss: 0.6188 - val_acc: 0.6497\n",
      "Epoch 98/100\n",
      "284/284 [==============================] - 1s 2ms/step - loss: 0.5164 - acc: 0.7191 - val_loss: 0.6554 - val_acc: 0.6335\n",
      "Epoch 99/100\n",
      "284/284 [==============================] - 1s 2ms/step - loss: 0.5160 - acc: 0.7140 - val_loss: 0.6361 - val_acc: 0.6335\n"
     ]
    }
   ],
   "source": [
    "train_size = -1\n",
    "X_train_ = X_train[:train_size]\n",
    "y_train_ = y_train[:train_size]\n",
    "history = corelnet_nosoftmax_model.fit(X_train_, y_train_, validation_data=(X_val, y_val), epochs=n_epochs, verbose=1, callbacks=create_callbacks())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.plot_history(history, ('loss', 'acc'));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "81/81 [==============================] - 0s 2ms/step - loss: 0.6048 - acc: 0.6516\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.6048232316970825, 0.6516203880310059]"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corelnet_nosoftmax_model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PrediNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "from relational_neural_networks.predinet import PrediNet\n",
    "predinet_kwargs = dict(key_dim=4, n_heads=4, n_relations=16, add_temp_tag=False)\n",
    "def create_predinet():\n",
    "    model = tf.keras.Sequential(\n",
    "        [\n",
    "            PrediNet(**predinet_kwargs),\n",
    "            tf.keras.layers.Dense(hidden_dense_size, activation='relu', name='hidden_dense1'),\n",
    "            tf.keras.layers.Dense(2, name='output')],\n",
    "        name='predinet')\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"predinet\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " predi_net_2 (PrediNet)      (32, 64)                  11520     \n",
      "                                                                 \n",
      " hidden_dense1 (Dense)       (32, 64)                  4160      \n",
      "                                                                 \n",
      " output (Dense)              (32, 2)                   130       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 15810 (61.76 KB)\n",
      "Trainable params: 15810 (61.76 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "predinet_model = create_predinet()\n",
    "predinet_model.compile(loss=loss, optimizer=create_opt(), metrics=['acc'])\n",
    "predinet_model(X_train[:32]); # build\n",
    "predinet_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "284/284 [==============================] - 3s 5ms/step - loss: 0.7212 - acc: 0.5164 - val_loss: 0.7105 - val_acc: 0.5231\n",
      "Epoch 2/100\n",
      "284/284 [==============================] - 1s 4ms/step - loss: 0.6725 - acc: 0.5830 - val_loss: 0.7187 - val_acc: 0.5046\n",
      "Epoch 3/100\n",
      "284/284 [==============================] - 1s 4ms/step - loss: 0.6480 - acc: 0.6223 - val_loss: 0.7395 - val_acc: 0.4954\n",
      "Epoch 4/100\n",
      "284/284 [==============================] - 1s 4ms/step - loss: 0.6264 - acc: 0.6422 - val_loss: 0.7605 - val_acc: 0.5069\n",
      "Epoch 5/100\n",
      "284/284 [==============================] - 1s 4ms/step - loss: 0.6037 - acc: 0.6706 - val_loss: 0.7729 - val_acc: 0.4892\n",
      "Epoch 6/100\n",
      "284/284 [==============================] - 1s 4ms/step - loss: 0.5799 - acc: 0.6941 - val_loss: 0.8031 - val_acc: 0.5239\n",
      "Epoch 7/100\n",
      "284/284 [==============================] - 1s 4ms/step - loss: 0.5487 - acc: 0.7151 - val_loss: 0.8124 - val_acc: 0.5247\n",
      "Epoch 8/100\n",
      "284/284 [==============================] - 1s 4ms/step - loss: 0.5292 - acc: 0.7323 - val_loss: 0.8349 - val_acc: 0.5247\n",
      "Epoch 9/100\n",
      "284/284 [==============================] - 1s 4ms/step - loss: 0.4997 - acc: 0.7548 - val_loss: 0.9013 - val_acc: 0.5216\n",
      "Epoch 10/100\n",
      "284/284 [==============================] - 1s 4ms/step - loss: 0.4795 - acc: 0.7669 - val_loss: 0.9171 - val_acc: 0.5316\n",
      "Epoch 11/100\n",
      "284/284 [==============================] - 1s 4ms/step - loss: 0.4494 - acc: 0.7908 - val_loss: 0.9692 - val_acc: 0.5224\n",
      "Epoch 12/100\n",
      "284/284 [==============================] - 1s 4ms/step - loss: 0.4290 - acc: 0.7995 - val_loss: 1.0100 - val_acc: 0.5262\n",
      "Epoch 13/100\n",
      "284/284 [==============================] - 1s 4ms/step - loss: 0.4077 - acc: 0.8129 - val_loss: 1.0532 - val_acc: 0.5247\n",
      "Epoch 14/100\n",
      "284/284 [==============================] - 1s 4ms/step - loss: 0.3905 - acc: 0.8227 - val_loss: 1.1054 - val_acc: 0.5015\n",
      "Epoch 15/100\n",
      "284/284 [==============================] - 1s 4ms/step - loss: 0.3626 - acc: 0.8390 - val_loss: 1.1344 - val_acc: 0.5208\n",
      "Epoch 16/100\n",
      "284/284 [==============================] - 1s 4ms/step - loss: 0.3473 - acc: 0.8460 - val_loss: 1.1535 - val_acc: 0.5386\n",
      "Epoch 17/100\n",
      "284/284 [==============================] - 1s 4ms/step - loss: 0.3291 - acc: 0.8555 - val_loss: 1.2623 - val_acc: 0.5324\n",
      "Epoch 18/100\n",
      "284/284 [==============================] - 1s 4ms/step - loss: 0.3108 - acc: 0.8624 - val_loss: 1.3211 - val_acc: 0.5108\n",
      "Epoch 19/100\n",
      "284/284 [==============================] - 1s 4ms/step - loss: 0.2839 - acc: 0.8784 - val_loss: 1.3674 - val_acc: 0.5347\n",
      "Epoch 20/100\n",
      "284/284 [==============================] - 1s 4ms/step - loss: 0.2674 - acc: 0.8869 - val_loss: 1.4673 - val_acc: 0.5108\n",
      "Epoch 21/100\n",
      "284/284 [==============================] - 1s 4ms/step - loss: 0.2603 - acc: 0.8934 - val_loss: 1.4699 - val_acc: 0.5278\n",
      "Epoch 22/100\n",
      "284/284 [==============================] - 1s 4ms/step - loss: 0.2451 - acc: 0.8967 - val_loss: 1.5038 - val_acc: 0.5293\n",
      "Epoch 23/100\n",
      "284/284 [==============================] - 1s 4ms/step - loss: 0.2304 - acc: 0.9070 - val_loss: 1.6111 - val_acc: 0.5324\n",
      "Epoch 24/100\n",
      "284/284 [==============================] - 1s 4ms/step - loss: 0.2219 - acc: 0.9044 - val_loss: 1.6702 - val_acc: 0.5386\n",
      "Epoch 25/100\n",
      "284/284 [==============================] - 1s 4ms/step - loss: 0.2133 - acc: 0.9126 - val_loss: 1.7499 - val_acc: 0.5231\n",
      "Epoch 26/100\n",
      "284/284 [==============================] - 1s 4ms/step - loss: 0.2018 - acc: 0.9146 - val_loss: 1.7674 - val_acc: 0.5247\n",
      "Epoch 27/100\n",
      "284/284 [==============================] - 1s 4ms/step - loss: 0.1880 - acc: 0.9245 - val_loss: 1.9236 - val_acc: 0.5309\n",
      "Epoch 28/100\n",
      "284/284 [==============================] - 1s 4ms/step - loss: 0.1882 - acc: 0.9254 - val_loss: 1.8896 - val_acc: 0.5332\n",
      "Epoch 29/100\n",
      "284/284 [==============================] - 1s 4ms/step - loss: 0.1805 - acc: 0.9261 - val_loss: 1.9945 - val_acc: 0.5301\n",
      "Epoch 30/100\n",
      "284/284 [==============================] - 1s 4ms/step - loss: 0.1809 - acc: 0.9233 - val_loss: 1.9801 - val_acc: 0.5147\n",
      "Epoch 31/100\n",
      "284/284 [==============================] - 1s 4ms/step - loss: 0.1643 - acc: 0.9366 - val_loss: 2.0232 - val_acc: 0.5301\n",
      "Epoch 32/100\n",
      "284/284 [==============================] - 1s 4ms/step - loss: 0.1413 - acc: 0.9426 - val_loss: 2.1375 - val_acc: 0.5131\n",
      "Epoch 33/100\n",
      "284/284 [==============================] - 1s 4ms/step - loss: 0.1374 - acc: 0.9481 - val_loss: 2.2030 - val_acc: 0.5386\n",
      "Epoch 34/100\n",
      "284/284 [==============================] - 1s 4ms/step - loss: 0.1604 - acc: 0.9384 - val_loss: 2.2280 - val_acc: 0.5162\n",
      "Epoch 35/100\n",
      "284/284 [==============================] - 1s 4ms/step - loss: 0.1533 - acc: 0.9372 - val_loss: 2.2445 - val_acc: 0.5262\n",
      "Epoch 36/100\n",
      "284/284 [==============================] - 1s 4ms/step - loss: 0.1335 - acc: 0.9484 - val_loss: 2.2822 - val_acc: 0.5347\n",
      "Epoch 37/100\n",
      "284/284 [==============================] - 1s 4ms/step - loss: 0.1273 - acc: 0.9491 - val_loss: 2.3632 - val_acc: 0.5262\n",
      "Epoch 38/100\n",
      "284/284 [==============================] - 1s 4ms/step - loss: 0.1242 - acc: 0.9548 - val_loss: 2.3877 - val_acc: 0.5324\n",
      "Epoch 39/100\n",
      "284/284 [==============================] - 1s 4ms/step - loss: 0.1216 - acc: 0.9563 - val_loss: 2.5042 - val_acc: 0.5247\n",
      "Epoch 40/100\n",
      "284/284 [==============================] - 1s 4ms/step - loss: 0.1219 - acc: 0.9520 - val_loss: 2.5227 - val_acc: 0.5285\n",
      "Epoch 41/100\n",
      "284/284 [==============================] - 1s 4ms/step - loss: 0.1136 - acc: 0.9569 - val_loss: 2.5205 - val_acc: 0.5471\n",
      "Epoch 42/100\n",
      "284/284 [==============================] - 1s 4ms/step - loss: 0.1566 - acc: 0.9373 - val_loss: 2.5126 - val_acc: 0.5432\n",
      "Epoch 43/100\n",
      "284/284 [==============================] - 1s 4ms/step - loss: 0.1513 - acc: 0.9396 - val_loss: 2.5534 - val_acc: 0.5285\n",
      "Epoch 44/100\n",
      "284/284 [==============================] - 1s 4ms/step - loss: 0.0991 - acc: 0.9648 - val_loss: 2.5580 - val_acc: 0.5440\n",
      "Epoch 45/100\n",
      "284/284 [==============================] - 1s 4ms/step - loss: 0.0762 - acc: 0.9745 - val_loss: 2.7341 - val_acc: 0.5247\n",
      "Epoch 46/100\n",
      "284/284 [==============================] - 1s 4ms/step - loss: 0.1042 - acc: 0.9627 - val_loss: 2.6967 - val_acc: 0.5355\n",
      "Epoch 47/100\n",
      "284/284 [==============================] - 1s 4ms/step - loss: 0.1397 - acc: 0.9459 - val_loss: 2.6224 - val_acc: 0.5278\n",
      "Epoch 48/100\n",
      "284/284 [==============================] - 1s 4ms/step - loss: 0.1309 - acc: 0.9500 - val_loss: 2.6532 - val_acc: 0.5409\n",
      "Epoch 49/100\n",
      "284/284 [==============================] - 1s 4ms/step - loss: 0.0651 - acc: 0.9794 - val_loss: 2.7488 - val_acc: 0.5316\n",
      "Epoch 50/100\n",
      "284/284 [==============================] - 1s 4ms/step - loss: 0.0808 - acc: 0.9724 - val_loss: 2.8886 - val_acc: 0.5131\n",
      "Epoch 51/100\n",
      "284/284 [==============================] - 1s 4ms/step - loss: 0.1394 - acc: 0.9485 - val_loss: 2.8385 - val_acc: 0.5131\n",
      "Epoch 52/100\n",
      "284/284 [==============================] - 1s 4ms/step - loss: 0.1477 - acc: 0.9454 - val_loss: 2.7380 - val_acc: 0.5332\n",
      "Epoch 53/100\n",
      "284/284 [==============================] - 1s 4ms/step - loss: 0.0800 - acc: 0.9713 - val_loss: 2.8465 - val_acc: 0.5270\n",
      "Epoch 54/100\n",
      "284/284 [==============================] - 1s 4ms/step - loss: 0.0540 - acc: 0.9851 - val_loss: 3.0660 - val_acc: 0.5340\n",
      "Epoch 55/100\n",
      "284/284 [==============================] - 1s 4ms/step - loss: 0.0692 - acc: 0.9781 - val_loss: 3.0379 - val_acc: 0.5185\n",
      "Epoch 56/100\n",
      "284/284 [==============================] - 1s 4ms/step - loss: 0.1456 - acc: 0.9460 - val_loss: 2.7228 - val_acc: 0.5378\n",
      "Epoch 57/100\n",
      "284/284 [==============================] - 1s 4ms/step - loss: 0.1664 - acc: 0.9384 - val_loss: 2.7996 - val_acc: 0.5378\n",
      "Epoch 58/100\n",
      "284/284 [==============================] - 1s 4ms/step - loss: 0.0910 - acc: 0.9669 - val_loss: 2.9793 - val_acc: 0.5278\n",
      "Epoch 59/100\n",
      "284/284 [==============================] - 1s 4ms/step - loss: 0.0700 - acc: 0.9774 - val_loss: 2.9922 - val_acc: 0.5224\n",
      "Epoch 60/100\n",
      "284/284 [==============================] - 1s 4ms/step - loss: 0.0512 - acc: 0.9848 - val_loss: 3.0419 - val_acc: 0.5201\n",
      "Epoch 61/100\n",
      "284/284 [==============================] - 1s 4ms/step - loss: 0.0351 - acc: 0.9922 - val_loss: 3.0606 - val_acc: 0.5401\n",
      "Epoch 62/100\n",
      "284/284 [==============================] - 1s 4ms/step - loss: 0.0459 - acc: 0.9856 - val_loss: 3.2016 - val_acc: 0.5247\n",
      "Epoch 63/100\n",
      "284/284 [==============================] - 1s 4ms/step - loss: 0.2568 - acc: 0.9146 - val_loss: 2.8435 - val_acc: 0.5316\n",
      "Epoch 64/100\n",
      "284/284 [==============================] - 1s 4ms/step - loss: 0.2041 - acc: 0.9255 - val_loss: 2.8191 - val_acc: 0.5231\n",
      "Epoch 65/100\n",
      "284/284 [==============================] - 1s 4ms/step - loss: 0.0794 - acc: 0.9737 - val_loss: 2.8969 - val_acc: 0.5262\n",
      "Epoch 66/100\n",
      "284/284 [==============================] - 1s 4ms/step - loss: 0.0397 - acc: 0.9893 - val_loss: 3.0251 - val_acc: 0.5347\n",
      "Epoch 67/100\n",
      "284/284 [==============================] - 1s 4ms/step - loss: 0.0370 - acc: 0.9911 - val_loss: 3.0881 - val_acc: 0.5293\n",
      "Epoch 68/100\n",
      "284/284 [==============================] - 1s 4ms/step - loss: 0.0503 - acc: 0.9862 - val_loss: 3.1912 - val_acc: 0.5455\n",
      "Epoch 69/100\n",
      "284/284 [==============================] - 1s 4ms/step - loss: 0.1222 - acc: 0.9573 - val_loss: 3.2074 - val_acc: 0.5201\n",
      "Epoch 70/100\n",
      "284/284 [==============================] - 1s 4ms/step - loss: 0.1534 - acc: 0.9448 - val_loss: 3.1193 - val_acc: 0.5324\n",
      "Epoch 71/100\n",
      "284/284 [==============================] - 1s 4ms/step - loss: 0.0964 - acc: 0.9647 - val_loss: 3.0772 - val_acc: 0.5502\n",
      "Epoch 72/100\n",
      "284/284 [==============================] - 1s 4ms/step - loss: 0.0594 - acc: 0.9806 - val_loss: 3.1283 - val_acc: 0.5363\n",
      "Epoch 73/100\n",
      "284/284 [==============================] - 1s 4ms/step - loss: 0.0405 - acc: 0.9891 - val_loss: 3.2145 - val_acc: 0.5309\n",
      "Epoch 74/100\n",
      "284/284 [==============================] - 1s 4ms/step - loss: 0.0633 - acc: 0.9793 - val_loss: 3.3801 - val_acc: 0.5193\n",
      "Epoch 75/100\n",
      "284/284 [==============================] - 1s 4ms/step - loss: 0.1390 - acc: 0.9509 - val_loss: 3.2159 - val_acc: 0.5378\n",
      "Epoch 76/100\n",
      "284/284 [==============================] - 1s 4ms/step - loss: 0.1633 - acc: 0.9423 - val_loss: 3.1107 - val_acc: 0.5293\n",
      "Epoch 77/100\n",
      "284/284 [==============================] - 1s 4ms/step - loss: 0.1014 - acc: 0.9637 - val_loss: 3.1038 - val_acc: 0.5394\n",
      "Epoch 78/100\n",
      "284/284 [==============================] - 1s 4ms/step - loss: 0.0557 - acc: 0.9824 - val_loss: 3.2175 - val_acc: 0.5301\n",
      "Epoch 79/100\n",
      "284/284 [==============================] - 1s 4ms/step - loss: 0.0362 - acc: 0.9906 - val_loss: 3.3554 - val_acc: 0.5378\n",
      "Epoch 80/100\n",
      "284/284 [==============================] - 1s 4ms/step - loss: 0.0180 - acc: 0.9972 - val_loss: 3.4653 - val_acc: 0.5231\n",
      "Epoch 81/100\n",
      "284/284 [==============================] - 1s 4ms/step - loss: 0.0121 - acc: 0.9991 - val_loss: 3.5370 - val_acc: 0.5309\n",
      "Epoch 82/100\n",
      "284/284 [==============================] - 1s 4ms/step - loss: 0.0084 - acc: 0.9998 - val_loss: 3.6197 - val_acc: 0.5262\n",
      "Epoch 83/100\n",
      "284/284 [==============================] - 1s 4ms/step - loss: 0.0059 - acc: 0.9998 - val_loss: 3.6942 - val_acc: 0.5278\n",
      "Epoch 84/100\n",
      "284/284 [==============================] - 1s 4ms/step - loss: 0.0049 - acc: 0.9998 - val_loss: 3.7795 - val_acc: 0.5255\n",
      "Epoch 85/100\n",
      "284/284 [==============================] - 1s 4ms/step - loss: 0.0042 - acc: 0.9998 - val_loss: 3.8374 - val_acc: 0.5247\n",
      "Epoch 86/100\n",
      "284/284 [==============================] - 1s 4ms/step - loss: 0.0036 - acc: 0.9998 - val_loss: 3.9164 - val_acc: 0.5301\n",
      "Epoch 87/100\n",
      "284/284 [==============================] - 1s 4ms/step - loss: 0.0032 - acc: 0.9998 - val_loss: 4.0017 - val_acc: 0.5270\n",
      "Epoch 88/100\n",
      "284/284 [==============================] - 1s 4ms/step - loss: 0.0029 - acc: 0.9998 - val_loss: 4.0758 - val_acc: 0.5301\n",
      "Epoch 89/100\n",
      "284/284 [==============================] - 1s 4ms/step - loss: 0.0026 - acc: 0.9998 - val_loss: 4.1707 - val_acc: 0.5216\n",
      "Epoch 90/100\n",
      "284/284 [==============================] - 1s 4ms/step - loss: 0.0023 - acc: 0.9998 - val_loss: 4.2359 - val_acc: 0.5309\n",
      "Epoch 91/100\n",
      "284/284 [==============================] - 1s 4ms/step - loss: 0.0021 - acc: 0.9998 - val_loss: 4.3297 - val_acc: 0.5239\n",
      "Epoch 92/100\n",
      "284/284 [==============================] - 1s 4ms/step - loss: 0.0019 - acc: 0.9998 - val_loss: 4.4278 - val_acc: 0.5208\n",
      "Epoch 93/100\n",
      "284/284 [==============================] - 1s 4ms/step - loss: 0.0017 - acc: 0.9998 - val_loss: 4.5447 - val_acc: 0.5185\n",
      "Epoch 94/100\n",
      "284/284 [==============================] - 1s 4ms/step - loss: 0.0015 - acc: 0.9998 - val_loss: 4.6381 - val_acc: 0.5239\n",
      "Epoch 95/100\n",
      "284/284 [==============================] - 1s 4ms/step - loss: 0.7131 - acc: 0.8776 - val_loss: 2.6300 - val_acc: 0.5162\n",
      "Epoch 96/100\n",
      "284/284 [==============================] - 1s 4ms/step - loss: 0.9139 - acc: 0.7252 - val_loss: 1.8691 - val_acc: 0.5432\n",
      "Epoch 97/100\n",
      "284/284 [==============================] - 1s 4ms/step - loss: 0.4305 - acc: 0.8342 - val_loss: 1.8707 - val_acc: 0.5231\n",
      "Epoch 98/100\n",
      "284/284 [==============================] - 1s 4ms/step - loss: 0.2585 - acc: 0.8968 - val_loss: 1.9345 - val_acc: 0.5139\n",
      "Epoch 99/100\n",
      "284/284 [==============================] - 1s 4ms/step - loss: 0.1668 - acc: 0.9372 - val_loss: 2.0630 - val_acc: 0.5324\n",
      "Epoch 100/100\n",
      "284/284 [==============================] - 1s 4ms/step - loss: 0.1135 - acc: 0.9617 - val_loss: 2.1699 - val_acc: 0.5355\n"
     ]
    }
   ],
   "source": [
    "train_size = -1\n",
    "X_train_ = X_train[:train_size]\n",
    "y_train_ = y_train[:train_size]\n",
    "history = predinet_model.fit(X_train_, y_train_, validation_data=(X_val, y_val), epochs=n_epochs, verbose=1, callbacks=create_callbacks())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.plot_history(history, ('loss', 'acc'));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "81/81 [==============================] - 0s 2ms/step - loss: 0.6048 - acc: 0.6516\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.6048232316970825, 0.6516203880310059]"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corelnet_nosoftmax_model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RelConvNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "from relational_neural_networks.multi_head_relation import MultiHeadRelation\n",
    "from relational_neural_networks.relational_graphlet_convolution import RelationalGraphletConvolution\n",
    "\n",
    "\n",
    "def create_relconvnet():\n",
    "    relconv_mhr_kwargs = dict(rel_dim=16, proj_dim=4, symmetric=True)\n",
    "    relconv_kwargs = dict(n_filters=16, graphlet_size=3,\n",
    "            symmetric_inner_prod=False)\n",
    "    mhr1 = MultiHeadRelation(**relconv_mhr_kwargs, name='mhr1')\n",
    "    rel_conv1 = RelationalGraphletConvolution(\n",
    "        **relconv_kwargs, groups='combinations', name='rgc1')\n",
    "\n",
    "    model = tf.keras.Sequential([\n",
    "        mhr1,\n",
    "        rel_conv1,\n",
    "            tf.keras.layers.GlobalMaxPooling1D(),\n",
    "        # tf.keras.layers.Flatten(name='flatten'),\n",
    "        tf.keras.layers.Dense(hidden_dense_size, activation='relu', name='hidden_dense1'),\n",
    "        tf.keras.layers.Dense(2, name='output')\n",
    "        ], name='relconv'\n",
    "    )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"relconv\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " mhr1 (MultiHeadRelation)    (32, 5, 5, 16)            4160      \n",
      "                                                                 \n",
      " rgc1 (RelationalGraphletCo  (32, 10, 16)              2304      \n",
      " nvolution)                                                      \n",
      "                                                                 \n",
      " global_max_pooling1d_1 (Gl  (32, 16)                  0         \n",
      " obalMaxPooling1D)                                               \n",
      "                                                                 \n",
      " hidden_dense1 (Dense)       (32, 64)                  1088      \n",
      "                                                                 \n",
      " output (Dense)              (32, 2)                   130       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 7682 (30.01 KB)\n",
      "Trainable params: 7682 (30.01 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "relconvnet_model = create_relconvnet()\n",
    "relconvnet_model.compile(loss=loss, optimizer=create_opt(), metrics=['acc'])\n",
    "relconvnet_model(X_train[:32]); # build\n",
    "relconvnet_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "284/284 [==============================] - 18s 27ms/step - loss: 0.7389 - acc: 0.5210 - val_loss: 0.7027 - val_acc: 0.5046\n",
      "Epoch 2/100\n",
      "284/284 [==============================] - 7s 24ms/step - loss: 0.6823 - acc: 0.5530 - val_loss: 0.6854 - val_acc: 0.5417\n",
      "Epoch 3/100\n",
      "284/284 [==============================] - 7s 24ms/step - loss: 0.6641 - acc: 0.5866 - val_loss: 0.6899 - val_acc: 0.5610\n",
      "Epoch 4/100\n",
      "284/284 [==============================] - 7s 25ms/step - loss: 0.6387 - acc: 0.6243 - val_loss: 0.6674 - val_acc: 0.5802\n",
      "Epoch 5/100\n",
      "284/284 [==============================] - 7s 24ms/step - loss: 0.6135 - acc: 0.6516 - val_loss: 0.6491 - val_acc: 0.6096\n",
      "Epoch 6/100\n",
      "284/284 [==============================] - 7s 24ms/step - loss: 0.5803 - acc: 0.6807 - val_loss: 0.6592 - val_acc: 0.6073\n",
      "Epoch 7/100\n",
      "284/284 [==============================] - 7s 24ms/step - loss: 0.5612 - acc: 0.6939 - val_loss: 0.6479 - val_acc: 0.6049\n",
      "Epoch 8/100\n",
      "284/284 [==============================] - 7s 24ms/step - loss: 0.5406 - acc: 0.7102 - val_loss: 0.6472 - val_acc: 0.6227\n",
      "Epoch 9/100\n",
      "284/284 [==============================] - 7s 24ms/step - loss: 0.5165 - acc: 0.7266 - val_loss: 0.6701 - val_acc: 0.6111\n",
      "Epoch 10/100\n",
      "284/284 [==============================] - 7s 24ms/step - loss: 0.5001 - acc: 0.7420 - val_loss: 0.6779 - val_acc: 0.6165\n",
      "Epoch 11/100\n",
      "284/284 [==============================] - 7s 24ms/step - loss: 0.4805 - acc: 0.7526 - val_loss: 0.6653 - val_acc: 0.6312\n",
      "Epoch 12/100\n",
      "284/284 [==============================] - 7s 24ms/step - loss: 0.4672 - acc: 0.7625 - val_loss: 0.7000 - val_acc: 0.6451\n",
      "Epoch 13/100\n",
      "284/284 [==============================] - 7s 24ms/step - loss: 0.4574 - acc: 0.7704 - val_loss: 0.7143 - val_acc: 0.6235\n",
      "Epoch 14/100\n",
      "284/284 [==============================] - 7s 24ms/step - loss: 0.4396 - acc: 0.7855 - val_loss: 0.6965 - val_acc: 0.6235\n",
      "Epoch 15/100\n",
      "284/284 [==============================] - 7s 24ms/step - loss: 0.4225 - acc: 0.7954 - val_loss: 0.7260 - val_acc: 0.6435\n",
      "Epoch 16/100\n",
      "284/284 [==============================] - 7s 24ms/step - loss: 0.4147 - acc: 0.8019 - val_loss: 0.7447 - val_acc: 0.6273\n",
      "Epoch 17/100\n",
      "284/284 [==============================] - 7s 24ms/step - loss: 0.3998 - acc: 0.8066 - val_loss: 0.7430 - val_acc: 0.6304\n",
      "Epoch 18/100\n",
      "284/284 [==============================] - 7s 24ms/step - loss: 0.3855 - acc: 0.8163 - val_loss: 0.8071 - val_acc: 0.6304\n",
      "Epoch 19/100\n",
      "284/284 [==============================] - 7s 24ms/step - loss: 0.3771 - acc: 0.8248 - val_loss: 0.8197 - val_acc: 0.6242\n",
      "Epoch 20/100\n",
      "284/284 [==============================] - 7s 24ms/step - loss: 0.3699 - acc: 0.8298 - val_loss: 0.8336 - val_acc: 0.6296\n",
      "Epoch 21/100\n",
      "284/284 [==============================] - 7s 24ms/step - loss: 0.3555 - acc: 0.8330 - val_loss: 0.8706 - val_acc: 0.6173\n",
      "Epoch 22/100\n",
      "284/284 [==============================] - 7s 24ms/step - loss: 0.3421 - acc: 0.8395 - val_loss: 0.9088 - val_acc: 0.6235\n",
      "Epoch 23/100\n",
      "284/284 [==============================] - 7s 24ms/step - loss: 0.3406 - acc: 0.8444 - val_loss: 0.8986 - val_acc: 0.6312\n",
      "Epoch 24/100\n",
      "284/284 [==============================] - 7s 24ms/step - loss: 0.3415 - acc: 0.8405 - val_loss: 0.8897 - val_acc: 0.6358\n",
      "Epoch 25/100\n",
      "284/284 [==============================] - 7s 24ms/step - loss: 0.3191 - acc: 0.8566 - val_loss: 0.9654 - val_acc: 0.6227\n",
      "Epoch 26/100\n",
      "284/284 [==============================] - 7s 24ms/step - loss: 0.3135 - acc: 0.8614 - val_loss: 1.0357 - val_acc: 0.6211\n",
      "Epoch 27/100\n",
      "284/284 [==============================] - 7s 24ms/step - loss: 0.3110 - acc: 0.8577 - val_loss: 1.0205 - val_acc: 0.6196\n",
      "Epoch 28/100\n",
      "284/284 [==============================] - 7s 24ms/step - loss: 0.2907 - acc: 0.8699 - val_loss: 1.0516 - val_acc: 0.6150\n",
      "Epoch 29/100\n",
      "284/284 [==============================] - 7s 24ms/step - loss: 0.2835 - acc: 0.8768 - val_loss: 1.0926 - val_acc: 0.6219\n",
      "Epoch 30/100\n",
      "284/284 [==============================] - 7s 24ms/step - loss: 0.2771 - acc: 0.8813 - val_loss: 1.0918 - val_acc: 0.6227\n",
      "Epoch 31/100\n",
      "284/284 [==============================] - 7s 24ms/step - loss: 0.2777 - acc: 0.8755 - val_loss: 1.0784 - val_acc: 0.6204\n",
      "Epoch 32/100\n",
      "284/284 [==============================] - 7s 24ms/step - loss: 0.2686 - acc: 0.8830 - val_loss: 1.1752 - val_acc: 0.6119\n",
      "Epoch 33/100\n",
      "284/284 [==============================] - 7s 24ms/step - loss: 0.2708 - acc: 0.8801 - val_loss: 1.1281 - val_acc: 0.6211\n",
      "Epoch 34/100\n",
      "284/284 [==============================] - 7s 24ms/step - loss: 0.2621 - acc: 0.8839 - val_loss: 1.1182 - val_acc: 0.6312\n",
      "Epoch 35/100\n",
      "284/284 [==============================] - 7s 24ms/step - loss: 0.2635 - acc: 0.8866 - val_loss: 1.1786 - val_acc: 0.6204\n",
      "Epoch 36/100\n",
      " 40/284 [===>..........................] - ETA: 5s - loss: 0.2157 - acc: 0.9148"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[226], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m X_train_ \u001b[38;5;241m=\u001b[39m X_train[:train_size]\n\u001b[1;32m      3\u001b[0m y_train_ \u001b[38;5;241m=\u001b[39m y_train[:train_size]\n\u001b[0;32m----> 4\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mrelconvnet_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcreate_callbacks\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/relconvnet/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/.conda/envs/relconvnet/lib/python3.10/site-packages/keras/src/engine/training.py:1742\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1734\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mexperimental\u001b[39m.\u001b[39mTrace(\n\u001b[1;32m   1735\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   1736\u001b[0m     epoch_num\u001b[39m=\u001b[39mepoch,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1739\u001b[0m     _r\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[1;32m   1740\u001b[0m ):\n\u001b[1;32m   1741\u001b[0m     callbacks\u001b[39m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m-> 1742\u001b[0m     tmp_logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_function(iterator)\n\u001b[1;32m   1743\u001b[0m     \u001b[39mif\u001b[39;00m data_handler\u001b[39m.\u001b[39mshould_sync:\n\u001b[1;32m   1744\u001b[0m         context\u001b[39m.\u001b[39masync_wait()\n",
      "File \u001b[0;32m~/.conda/envs/relconvnet/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/.conda/envs/relconvnet/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:825\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    822\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    824\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 825\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m    827\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    828\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m~/.conda/envs/relconvnet/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:857\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    854\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n\u001b[1;32m    855\u001b[0m   \u001b[39m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m    856\u001b[0m   \u001b[39m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[0;32m--> 857\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_no_variable_creation_fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)  \u001b[39m# pylint: disable=not-callable\u001b[39;00m\n\u001b[1;32m    858\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_variable_creation_fn \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    859\u001b[0m   \u001b[39m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[1;32m    860\u001b[0m   \u001b[39m# in parallel.\u001b[39;00m\n\u001b[1;32m    861\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n",
      "File \u001b[0;32m~/.conda/envs/relconvnet/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py:148\u001b[0m, in \u001b[0;36mTracingCompiler.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[1;32m    146\u001b[0m   (concrete_function,\n\u001b[1;32m    147\u001b[0m    filtered_flat_args) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[0;32m--> 148\u001b[0m \u001b[39mreturn\u001b[39;00m concrete_function\u001b[39m.\u001b[39;49m_call_flat(\n\u001b[1;32m    149\u001b[0m     filtered_flat_args, captured_inputs\u001b[39m=\u001b[39;49mconcrete_function\u001b[39m.\u001b[39;49mcaptured_inputs)\n",
      "File \u001b[0;32m~/.conda/envs/relconvnet/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py:1349\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, args, captured_inputs)\u001b[0m\n\u001b[1;32m   1345\u001b[0m possible_gradient_type \u001b[39m=\u001b[39m gradients_util\u001b[39m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1346\u001b[0m \u001b[39mif\u001b[39;00m (possible_gradient_type \u001b[39m==\u001b[39m gradients_util\u001b[39m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1347\u001b[0m     \u001b[39mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1348\u001b[0m   \u001b[39m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1349\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_call_outputs(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inference_function(\u001b[39m*\u001b[39;49margs))\n\u001b[1;32m   1350\u001b[0m forward_backward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1351\u001b[0m     args,\n\u001b[1;32m   1352\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1353\u001b[0m     executing_eagerly)\n\u001b[1;32m   1354\u001b[0m forward_function, args_with_tangents \u001b[39m=\u001b[39m forward_backward\u001b[39m.\u001b[39mforward()\n",
      "File \u001b[0;32m~/.conda/envs/relconvnet/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:196\u001b[0m, in \u001b[0;36mAtomicFunction.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[39mwith\u001b[39;00m record\u001b[39m.\u001b[39mstop_recording():\n\u001b[1;32m    195\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_bound_context\u001b[39m.\u001b[39mexecuting_eagerly():\n\u001b[0;32m--> 196\u001b[0m     outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_bound_context\u001b[39m.\u001b[39;49mcall_function(\n\u001b[1;32m    197\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname,\n\u001b[1;32m    198\u001b[0m         \u001b[39mlist\u001b[39;49m(args),\n\u001b[1;32m    199\u001b[0m         \u001b[39mlen\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfunction_type\u001b[39m.\u001b[39;49mflat_outputs),\n\u001b[1;32m    200\u001b[0m     )\n\u001b[1;32m    201\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    202\u001b[0m     outputs \u001b[39m=\u001b[39m make_call_op_in_graph(\u001b[39mself\u001b[39m, \u001b[39mlist\u001b[39m(args))\n",
      "File \u001b[0;32m~/.conda/envs/relconvnet/lib/python3.10/site-packages/tensorflow/python/eager/context.py:1457\u001b[0m, in \u001b[0;36mContext.call_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1455\u001b[0m cancellation_context \u001b[39m=\u001b[39m cancellation\u001b[39m.\u001b[39mcontext()\n\u001b[1;32m   1456\u001b[0m \u001b[39mif\u001b[39;00m cancellation_context \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m-> 1457\u001b[0m   outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39;49mexecute(\n\u001b[1;32m   1458\u001b[0m       name\u001b[39m.\u001b[39;49mdecode(\u001b[39m\"\u001b[39;49m\u001b[39mutf-8\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[1;32m   1459\u001b[0m       num_outputs\u001b[39m=\u001b[39;49mnum_outputs,\n\u001b[1;32m   1460\u001b[0m       inputs\u001b[39m=\u001b[39;49mtensor_inputs,\n\u001b[1;32m   1461\u001b[0m       attrs\u001b[39m=\u001b[39;49mattrs,\n\u001b[1;32m   1462\u001b[0m       ctx\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m,\n\u001b[1;32m   1463\u001b[0m   )\n\u001b[1;32m   1464\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1465\u001b[0m   outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m   1466\u001b[0m       name\u001b[39m.\u001b[39mdecode(\u001b[39m\"\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m\"\u001b[39m),\n\u001b[1;32m   1467\u001b[0m       num_outputs\u001b[39m=\u001b[39mnum_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1471\u001b[0m       cancellation_manager\u001b[39m=\u001b[39mcancellation_context,\n\u001b[1;32m   1472\u001b[0m   )\n",
      "File \u001b[0;32m~/.conda/envs/relconvnet/lib/python3.10/site-packages/tensorflow/python/eager/execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 53\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_Execute(ctx\u001b[39m.\u001b[39;49m_handle, device_name, op_name,\n\u001b[1;32m     54\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     55\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     56\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_size = -1\n",
    "X_train_ = X_train[:train_size]\n",
    "y_train_ = y_train[:train_size]\n",
    "history = relconvnet_model.fit(X_train_, y_train_, validation_data=(X_val, y_val), epochs=n_epochs, verbose=1, callbacks=create_callbacks())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.plot_history(history, ('loss', 'acc'));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63/63 [==============================] - 0s 3ms/step - loss: 0.2331 - acc: 0.9590\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.2330666184425354, 0.9589999914169312]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relconvnet_model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RelConvNet (agg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "from relational_neural_networks.multi_head_relation import MultiHeadRelation\n",
    "from relational_neural_networks.relational_graphlet_convolution import RelationalGraphletConvolution\n",
    "\n",
    "def create_relconvnet_agg():\n",
    "    relconv_mhr_kwargs = dict(rel_dim=16, proj_dim=4, symmetric=True)\n",
    "    relconv_kwargs = dict(n_filters=16, graphlet_size=3,\n",
    "            symmetric_inner_prod=True, permutation_aggregator='max')\n",
    "    mhr1 = MultiHeadRelation(**relconv_mhr_kwargs, name='mhr1')\n",
    "    rel_conv1 = RelationalGraphletConvolution(\n",
    "        **relconv_kwargs, groups='combinations', name='rgc1')\n",
    "\n",
    "    model = tf.keras.Sequential([\n",
    "        mhr1,\n",
    "        rel_conv1,\n",
    "        tf.keras.layers.GlobalMaxPooling1D(),\n",
    "        # tf.keras.layers.Flatten(name='flatten'),\n",
    "        tf.keras.layers.Dense(hidden_dense_size, activation='relu', name='hidden_dense1'),\n",
    "        tf.keras.layers.Dense(2, name='output')\n",
    "        ], name='relconv'\n",
    "    )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"relconv\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " mhr1 (MultiHeadRelation)    (32, 5, 5, 16)            4160      \n",
      "                                                                 \n",
      " rgc1 (RelationalGraphletCo  (32, 10, 16)              2304      \n",
      " nvolution)                                                      \n",
      "                                                                 \n",
      " global_max_pooling1d (Glob  (32, 16)                  0         \n",
      " alMaxPooling1D)                                                 \n",
      "                                                                 \n",
      " flatten (Flatten)           (32, 16)                  0         \n",
      "                                                                 \n",
      " hidden_dense1 (Dense)       (32, 64)                  1088      \n",
      "                                                                 \n",
      " output (Dense)              (32, 2)                   130       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 7682 (30.01 KB)\n",
      "Trainable params: 7682 (30.01 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "relconvnet_model = create_relconvnet_agg()\n",
    "relconvnet_model.compile(loss=loss, optimizer=create_opt(), metrics=['acc'])\n",
    "relconvnet_model(X_train[:32]); # build\n",
    "relconvnet_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "284/284 [==============================] - 73s 142ms/step - loss: 0.7025 - acc: 0.5574 - val_loss: 0.6710 - val_acc: 0.5787\n",
      "Epoch 2/100\n",
      "284/284 [==============================] - 36s 128ms/step - loss: 0.6452 - acc: 0.6175 - val_loss: 0.6687 - val_acc: 0.6003\n",
      "Epoch 3/100\n",
      "284/284 [==============================] - 36s 128ms/step - loss: 0.6069 - acc: 0.6537 - val_loss: 0.6774 - val_acc: 0.6019\n",
      "Epoch 4/100\n",
      "284/284 [==============================] - 36s 127ms/step - loss: 0.5796 - acc: 0.6727 - val_loss: 0.6291 - val_acc: 0.6312\n",
      "Epoch 5/100\n",
      "284/284 [==============================] - 37s 129ms/step - loss: 0.5517 - acc: 0.7026 - val_loss: 0.6454 - val_acc: 0.6127\n",
      "Epoch 6/100\n",
      "284/284 [==============================] - 36s 128ms/step - loss: 0.5358 - acc: 0.7112 - val_loss: 0.6594 - val_acc: 0.6319\n",
      "Epoch 7/100\n",
      "284/284 [==============================] - 36s 127ms/step - loss: 0.5126 - acc: 0.7288 - val_loss: 0.6601 - val_acc: 0.6281\n",
      "Epoch 8/100\n",
      "284/284 [==============================] - 36s 125ms/step - loss: 0.4931 - acc: 0.7402 - val_loss: 0.6509 - val_acc: 0.6250\n",
      "Epoch 9/100\n",
      "246/284 [========================>.....] - ETA: 4s - loss: 0.4750 - acc: 0.7564"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[223], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m X_train_ \u001b[38;5;241m=\u001b[39m X_train[:train_size]\n\u001b[1;32m      3\u001b[0m y_train_ \u001b[38;5;241m=\u001b[39m y_train[:train_size]\n\u001b[0;32m----> 4\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mrelconvnet_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcreate_callbacks\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/relconvnet/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/.conda/envs/relconvnet/lib/python3.10/site-packages/keras/src/engine/training.py:1742\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1734\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mexperimental\u001b[39m.\u001b[39mTrace(\n\u001b[1;32m   1735\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   1736\u001b[0m     epoch_num\u001b[39m=\u001b[39mepoch,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1739\u001b[0m     _r\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[1;32m   1740\u001b[0m ):\n\u001b[1;32m   1741\u001b[0m     callbacks\u001b[39m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m-> 1742\u001b[0m     tmp_logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_function(iterator)\n\u001b[1;32m   1743\u001b[0m     \u001b[39mif\u001b[39;00m data_handler\u001b[39m.\u001b[39mshould_sync:\n\u001b[1;32m   1744\u001b[0m         context\u001b[39m.\u001b[39masync_wait()\n",
      "File \u001b[0;32m~/.conda/envs/relconvnet/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/.conda/envs/relconvnet/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:825\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    822\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    824\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 825\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m    827\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    828\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m~/.conda/envs/relconvnet/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:857\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    854\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n\u001b[1;32m    855\u001b[0m   \u001b[39m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m    856\u001b[0m   \u001b[39m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[0;32m--> 857\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_no_variable_creation_fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)  \u001b[39m# pylint: disable=not-callable\u001b[39;00m\n\u001b[1;32m    858\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_variable_creation_fn \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    859\u001b[0m   \u001b[39m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[1;32m    860\u001b[0m   \u001b[39m# in parallel.\u001b[39;00m\n\u001b[1;32m    861\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n",
      "File \u001b[0;32m~/.conda/envs/relconvnet/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py:148\u001b[0m, in \u001b[0;36mTracingCompiler.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[1;32m    146\u001b[0m   (concrete_function,\n\u001b[1;32m    147\u001b[0m    filtered_flat_args) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[0;32m--> 148\u001b[0m \u001b[39mreturn\u001b[39;00m concrete_function\u001b[39m.\u001b[39;49m_call_flat(\n\u001b[1;32m    149\u001b[0m     filtered_flat_args, captured_inputs\u001b[39m=\u001b[39;49mconcrete_function\u001b[39m.\u001b[39;49mcaptured_inputs)\n",
      "File \u001b[0;32m~/.conda/envs/relconvnet/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py:1349\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, args, captured_inputs)\u001b[0m\n\u001b[1;32m   1345\u001b[0m possible_gradient_type \u001b[39m=\u001b[39m gradients_util\u001b[39m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1346\u001b[0m \u001b[39mif\u001b[39;00m (possible_gradient_type \u001b[39m==\u001b[39m gradients_util\u001b[39m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1347\u001b[0m     \u001b[39mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1348\u001b[0m   \u001b[39m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1349\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_call_outputs(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inference_function(\u001b[39m*\u001b[39;49margs))\n\u001b[1;32m   1350\u001b[0m forward_backward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1351\u001b[0m     args,\n\u001b[1;32m   1352\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1353\u001b[0m     executing_eagerly)\n\u001b[1;32m   1354\u001b[0m forward_function, args_with_tangents \u001b[39m=\u001b[39m forward_backward\u001b[39m.\u001b[39mforward()\n",
      "File \u001b[0;32m~/.conda/envs/relconvnet/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:196\u001b[0m, in \u001b[0;36mAtomicFunction.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[39mwith\u001b[39;00m record\u001b[39m.\u001b[39mstop_recording():\n\u001b[1;32m    195\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_bound_context\u001b[39m.\u001b[39mexecuting_eagerly():\n\u001b[0;32m--> 196\u001b[0m     outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_bound_context\u001b[39m.\u001b[39;49mcall_function(\n\u001b[1;32m    197\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname,\n\u001b[1;32m    198\u001b[0m         \u001b[39mlist\u001b[39;49m(args),\n\u001b[1;32m    199\u001b[0m         \u001b[39mlen\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfunction_type\u001b[39m.\u001b[39;49mflat_outputs),\n\u001b[1;32m    200\u001b[0m     )\n\u001b[1;32m    201\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    202\u001b[0m     outputs \u001b[39m=\u001b[39m make_call_op_in_graph(\u001b[39mself\u001b[39m, \u001b[39mlist\u001b[39m(args))\n",
      "File \u001b[0;32m~/.conda/envs/relconvnet/lib/python3.10/site-packages/tensorflow/python/eager/context.py:1457\u001b[0m, in \u001b[0;36mContext.call_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1455\u001b[0m cancellation_context \u001b[39m=\u001b[39m cancellation\u001b[39m.\u001b[39mcontext()\n\u001b[1;32m   1456\u001b[0m \u001b[39mif\u001b[39;00m cancellation_context \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m-> 1457\u001b[0m   outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39;49mexecute(\n\u001b[1;32m   1458\u001b[0m       name\u001b[39m.\u001b[39;49mdecode(\u001b[39m\"\u001b[39;49m\u001b[39mutf-8\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[1;32m   1459\u001b[0m       num_outputs\u001b[39m=\u001b[39;49mnum_outputs,\n\u001b[1;32m   1460\u001b[0m       inputs\u001b[39m=\u001b[39;49mtensor_inputs,\n\u001b[1;32m   1461\u001b[0m       attrs\u001b[39m=\u001b[39;49mattrs,\n\u001b[1;32m   1462\u001b[0m       ctx\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m,\n\u001b[1;32m   1463\u001b[0m   )\n\u001b[1;32m   1464\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1465\u001b[0m   outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m   1466\u001b[0m       name\u001b[39m.\u001b[39mdecode(\u001b[39m\"\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m\"\u001b[39m),\n\u001b[1;32m   1467\u001b[0m       num_outputs\u001b[39m=\u001b[39mnum_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1471\u001b[0m       cancellation_manager\u001b[39m=\u001b[39mcancellation_context,\n\u001b[1;32m   1472\u001b[0m   )\n",
      "File \u001b[0;32m~/.conda/envs/relconvnet/lib/python3.10/site-packages/tensorflow/python/eager/execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 53\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_Execute(ctx\u001b[39m.\u001b[39;49m_handle, device_name, op_name,\n\u001b[1;32m     54\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     55\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     56\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_size = -1\n",
    "X_train_ = X_train[:train_size]\n",
    "y_train_ = y_train[:train_size]\n",
    "history = relconvnet_model.fit(X_train_, y_train_, validation_data=(X_val, y_val), epochs=n_epochs, verbose=1, callbacks=create_callbacks())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.plot_history(history, ('loss', 'acc'));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63/63 [==============================] - 0s 3ms/step - loss: 0.1179 - acc: 0.9805\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.11788409948348999, 0.9804999828338623]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relconvnet_model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm, trange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_callbacks(use_wandb=False):\n",
    "    callbacks = []\n",
    "    callbacks.append(\n",
    "        tf.keras.callbacks.EarlyStopping(\n",
    "            monitor='val_acc', restore_best_weights=True, patience=50, start_from_epoch=30)\n",
    "            )\n",
    "    if use_wandb: callbacks.append(wandb.keras.WandbMetricsLogger(log_freq='epoch'))\n",
    "    return callbacks\n",
    "\n",
    "def evaluate_model(model, X_test, y_test):\n",
    "    eval_dict = model.evaluate(X_test, y_test, return_dict=True, verbose=False)\n",
    "    return eval_dict\n",
    "\n",
    "def log_to_wandb(evaluation_dict):\n",
    "    wandb.log(evaluation_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_trial = 0\n",
    "wandb_project_name = 'set_classification'\n",
    "\n",
    "fit_kwargs = {'epochs': 100, 'batch_size': 256}\n",
    "\n",
    "def evaluate_learning_curves(\n",
    "    create_model, group_name,\n",
    "    X_train, y_train, X_val, y_val, X_test, y_test,\n",
    "    train_sizes, num_trials, use_wandb=False):\n",
    "\n",
    "    eval_dicts =[]\n",
    "\n",
    "    for train_size in tqdm(train_sizes, desc='train size'):\n",
    "\n",
    "        for trial in trange(start_trial, start_trial + num_trials, desc='trial', leave=False):\n",
    "\n",
    "            if use_wandb:\n",
    "                run = wandb.init(project=wandb_project_name, group=group_name, name=f'train size = {train_size}; trial = {trial}',\n",
    "                    config={'train size': train_size, 'trial': trial, 'group': group_name})\n",
    "            model = create_model()\n",
    "\n",
    "            sample_idx = np.random.choice(len(X_train), train_size, replace=False)\n",
    "            X_train_ = X_train[sample_idx]\n",
    "            y_train_ = y_train[sample_idx]\n",
    "\n",
    "            history = model.fit(X_train_, y_train_, validation_data=(X_val, y_val), verbose=0,\n",
    "                callbacks=create_callbacks(use_wandb=use_wandb), **fit_kwargs)\n",
    "\n",
    "            eval_dict = evaluate_model(model, X_test, y_test)\n",
    "\n",
    "            if use_wandb:\n",
    "                log_to_wandb(eval_dict)\n",
    "                wandb.finish(quiet=True)\n",
    "\n",
    "            eval_dict.update({'group': group_name, 'train_size': train_size, 'trial': trial})\n",
    "            eval_dicts.append(eval_dict)\n",
    "\n",
    "            del model\n",
    "\n",
    "    return eval_dicts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_transformer_model():\n",
    "    transformer_model = create_transformer()\n",
    "    transformer_model.compile(loss=loss, optimizer=create_opt(), metrics=['acc'])\n",
    "    transformer_model(X_train[:32]); # build\n",
    "    return transformer_model\n",
    "\n",
    "def create_corelnet_model():\n",
    "    corelnet_model = create_corelnet()\n",
    "    corelnet_model.compile(loss=loss, optimizer=create_opt(), metrics=['acc'])\n",
    "    corelnet_model(X_train[:32]); # build\n",
    "    return corelnet_model\n",
    "\n",
    "def create_nosoftmax_corelnet_model():\n",
    "    corelnet_model = create_nosoftmax_corelnet()\n",
    "    corelnet_model.compile(loss=loss, optimizer=create_opt(), metrics=['acc'])\n",
    "    corelnet_model(X_train[:32]); # build\n",
    "    return corelnet_model\n",
    "\n",
    "def create_predinet_model():\n",
    "    predinet_model = create_predinet()\n",
    "    predinet_model.compile(loss=loss, optimizer=create_opt(), metrics=['acc'])\n",
    "    predinet_model(X_train[:32]); # build\n",
    "    return predinet_model\n",
    "\n",
    "def create_relconvnet_model():\n",
    "    relconvnet_model = create_relconvnet()\n",
    "    relconvnet_model.compile(loss=loss, optimizer=create_opt(), metrics=['acc'])\n",
    "    relconvnet_model(X_train[:32]); # build\n",
    "    return relconvnet_model\n",
    "\n",
    "def create_relconvnet_agg_model():\n",
    "    relconvnet_model = create_relconvet_agg()\n",
    "    relconvnet_model.compile(loss=loss, optimizer=create_opt(), metrics=['acc'])\n",
    "    relconvnet_model(X_train[:32]); # build\n",
    "    return relconvnet_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating learning curves for transformer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train size: 100%|| 6/6 [05:30<00:00, 55.15s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating learning curves for corelnet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train size:  83%| | 5/6 [02:39<00:31, 31.86s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[48], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m model_name, model_creator \u001b[38;5;129;01min\u001b[39;00m model_creators\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mevaluating learning curves for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 11\u001b[0m     model_eval_dict \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate_learning_curves\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_creator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain_sizes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_trials\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_wandb\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m     model_eval_dicts\u001b[38;5;241m.\u001b[39mappend(model_eval_dict)\n",
      "Cell \u001b[0;32mIn[46], line 26\u001b[0m, in \u001b[0;36mevaluate_learning_curves\u001b[0;34m(create_model, group_name, X_train, y_train, X_val, y_val, X_test, y_test, train_sizes, num_trials, use_wandb)\u001b[0m\n\u001b[1;32m     23\u001b[0m X_train_ \u001b[38;5;241m=\u001b[39m X_train[sample_idx]\n\u001b[1;32m     24\u001b[0m y_train_ \u001b[38;5;241m=\u001b[39m y_train[sample_idx]\n\u001b[0;32m---> 26\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcreate_callbacks\u001b[49m\u001b[43m(\u001b[49m\u001b[43muse_wandb\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_wandb\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m eval_dict \u001b[38;5;241m=\u001b[39m evaluate_model(model, X_test, y_test)\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_wandb:\n",
      "File \u001b[0;32m~/.conda/envs/relconvnet/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/.conda/envs/relconvnet/lib/python3.10/site-packages/keras/src/engine/training.py:1791\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1775\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mgetattr\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m_eval_data_handler\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m) \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   1776\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_eval_data_handler \u001b[39m=\u001b[39m data_adapter\u001b[39m.\u001b[39mget_data_handler(\n\u001b[1;32m   1777\u001b[0m         x\u001b[39m=\u001b[39mval_x,\n\u001b[1;32m   1778\u001b[0m         y\u001b[39m=\u001b[39mval_y,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1789\u001b[0m         pss_evaluation_shards\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pss_evaluation_shards,\n\u001b[1;32m   1790\u001b[0m     )\n\u001b[0;32m-> 1791\u001b[0m val_logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mevaluate(\n\u001b[1;32m   1792\u001b[0m     x\u001b[39m=\u001b[39;49mval_x,\n\u001b[1;32m   1793\u001b[0m     y\u001b[39m=\u001b[39;49mval_y,\n\u001b[1;32m   1794\u001b[0m     sample_weight\u001b[39m=\u001b[39;49mval_sample_weight,\n\u001b[1;32m   1795\u001b[0m     batch_size\u001b[39m=\u001b[39;49mvalidation_batch_size \u001b[39mor\u001b[39;49;00m batch_size,\n\u001b[1;32m   1796\u001b[0m     steps\u001b[39m=\u001b[39;49mvalidation_steps,\n\u001b[1;32m   1797\u001b[0m     callbacks\u001b[39m=\u001b[39;49mcallbacks,\n\u001b[1;32m   1798\u001b[0m     max_queue_size\u001b[39m=\u001b[39;49mmax_queue_size,\n\u001b[1;32m   1799\u001b[0m     workers\u001b[39m=\u001b[39;49mworkers,\n\u001b[1;32m   1800\u001b[0m     use_multiprocessing\u001b[39m=\u001b[39;49muse_multiprocessing,\n\u001b[1;32m   1801\u001b[0m     return_dict\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m   1802\u001b[0m     _use_cached_eval_dataset\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m   1803\u001b[0m )\n\u001b[1;32m   1804\u001b[0m val_logs \u001b[39m=\u001b[39m {\n\u001b[1;32m   1805\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mval_\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m name: val \u001b[39mfor\u001b[39;00m name, val \u001b[39min\u001b[39;00m val_logs\u001b[39m.\u001b[39mitems()\n\u001b[1;32m   1806\u001b[0m }\n\u001b[1;32m   1807\u001b[0m epoch_logs\u001b[39m.\u001b[39mupdate(val_logs)\n",
      "File \u001b[0;32m~/.conda/envs/relconvnet/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/.conda/envs/relconvnet/lib/python3.10/site-packages/keras/src/engine/training.py:2189\u001b[0m, in \u001b[0;36mModel.evaluate\u001b[0;34m(self, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing, return_dict, **kwargs)\u001b[0m\n\u001b[1;32m   2187\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_test_counter\u001b[39m.\u001b[39massign(\u001b[39m0\u001b[39m)\n\u001b[1;32m   2188\u001b[0m callbacks\u001b[39m.\u001b[39mon_test_begin()\n\u001b[0;32m-> 2189\u001b[0m \u001b[39mfor\u001b[39;00m (\n\u001b[1;32m   2190\u001b[0m     _,\n\u001b[1;32m   2191\u001b[0m     dataset_or_iterator,\n\u001b[1;32m   2192\u001b[0m ) \u001b[39min\u001b[39;00m data_handler\u001b[39m.\u001b[39menumerate_epochs():  \u001b[39m# Single epoch.\u001b[39;00m\n\u001b[1;32m   2193\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreset_metrics()\n\u001b[1;32m   2194\u001b[0m     \u001b[39mwith\u001b[39;00m data_handler\u001b[39m.\u001b[39mcatch_stop_iteration():\n",
      "File \u001b[0;32m~/.conda/envs/relconvnet/lib/python3.10/site-packages/keras/src/engine/data_adapter.py:1331\u001b[0m, in \u001b[0;36mDataHandler.enumerate_epochs\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1329\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Yields `(epoch, tf.data.Iterator)`.\"\"\"\u001b[39;00m\n\u001b[1;32m   1330\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_truncate_execution_to_epoch():\n\u001b[0;32m-> 1331\u001b[0m     data_iterator \u001b[39m=\u001b[39m \u001b[39miter\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset)\n\u001b[1;32m   1332\u001b[0m     \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_initial_epoch, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_epochs):\n\u001b[1;32m   1333\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_insufficient_data:  \u001b[39m# Set by `catch_stop_iteration`.\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/relconvnet/lib/python3.10/site-packages/tensorflow/python/data/ops/dataset_ops.py:506\u001b[0m, in \u001b[0;36mDatasetV2.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    504\u001b[0m \u001b[39mif\u001b[39;00m context\u001b[39m.\u001b[39mexecuting_eagerly() \u001b[39mor\u001b[39;00m ops\u001b[39m.\u001b[39minside_function():\n\u001b[1;32m    505\u001b[0m   \u001b[39mwith\u001b[39;00m ops\u001b[39m.\u001b[39mcolocate_with(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_variant_tensor):\n\u001b[0;32m--> 506\u001b[0m     \u001b[39mreturn\u001b[39;00m iterator_ops\u001b[39m.\u001b[39;49mOwnedIterator(\u001b[39mself\u001b[39;49m)\n\u001b[1;32m    507\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    508\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39m`tf.data.Dataset` only supports Python-style \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    509\u001b[0m                      \u001b[39m\"\u001b[39m\u001b[39miteration in eager mode or within tf.function.\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.conda/envs/relconvnet/lib/python3.10/site-packages/tensorflow/python/data/ops/iterator_ops.py:710\u001b[0m, in \u001b[0;36mOwnedIterator.__init__\u001b[0;34m(self, dataset, components, element_spec)\u001b[0m\n\u001b[1;32m    706\u001b[0m   \u001b[39mif\u001b[39;00m (components \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m element_spec \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m):\n\u001b[1;32m    707\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    708\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mWhen `dataset` is provided, `element_spec` and `components` must \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    709\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mnot be specified.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 710\u001b[0m   \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_create_iterator(dataset)\n\u001b[1;32m    712\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_next_call_count \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n",
      "File \u001b[0;32m~/.conda/envs/relconvnet/lib/python3.10/site-packages/tensorflow/python/data/ops/iterator_ops.py:749\u001b[0m, in \u001b[0;36mOwnedIterator._create_iterator\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    746\u001b[0m   \u001b[39massert\u001b[39;00m \u001b[39mlen\u001b[39m(fulltype\u001b[39m.\u001b[39margs[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39margs[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39margs) \u001b[39m==\u001b[39m \u001b[39mlen\u001b[39m(\n\u001b[1;32m    747\u001b[0m       \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_flat_output_types)\n\u001b[1;32m    748\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iterator_resource\u001b[39m.\u001b[39mop\u001b[39m.\u001b[39mexperimental_set_type(fulltype)\n\u001b[0;32m--> 749\u001b[0m gen_dataset_ops\u001b[39m.\u001b[39;49mmake_iterator(ds_variant, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_iterator_resource)\n",
      "File \u001b[0;32m~/.conda/envs/relconvnet/lib/python3.10/site-packages/tensorflow/python/ops/gen_dataset_ops.py:3420\u001b[0m, in \u001b[0;36mmake_iterator\u001b[0;34m(dataset, iterator, name)\u001b[0m\n\u001b[1;32m   3418\u001b[0m \u001b[39mif\u001b[39;00m tld\u001b[39m.\u001b[39mis_eager:\n\u001b[1;32m   3419\u001b[0m   \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 3420\u001b[0m     _result \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_FastPathExecute(\n\u001b[1;32m   3421\u001b[0m       _ctx, \u001b[39m\"\u001b[39;49m\u001b[39mMakeIterator\u001b[39;49m\u001b[39m\"\u001b[39;49m, name, dataset, iterator)\n\u001b[1;32m   3422\u001b[0m     \u001b[39mreturn\u001b[39;00m _result\n\u001b[1;32m   3423\u001b[0m   \u001b[39mexcept\u001b[39;00m _core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_trials = 5\n",
    "train_sizes = [500, 1000, 2000, 3000, 4000, 5000]\n",
    "\n",
    "model_creators = dict(transformer=create_transformer_model, corelnet=create_corelnet_model,\n",
    "    corelnet_nsoftmax=create_nosoftmax_corelnet_model, predinet=create_predinet_model,\n",
    "    relconvnet=create_relconvnet_model, relconvnet_agg=create_relconvnet_agg_model)\n",
    "\n",
    "model_eval_dicts = []\n",
    "for model_name, model_creator in model_creators.items():\n",
    "    print(f'evaluating learning curves for {model_name}')\n",
    "    model_eval_dict = evaluate_learning_curves(\n",
    "        model_creator, model_name,\n",
    "        X_train, y_train, X_val, y_val, X_test, y_test,\n",
    "        train_sizes, num_trials, use_wandb=False)\n",
    "    model_eval_dicts.append(model_eval_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss</th>\n",
       "      <th>acc</th>\n",
       "      <th>group</th>\n",
       "      <th>train_size</th>\n",
       "      <th>trial</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.727094</td>\n",
       "      <td>0.7670</td>\n",
       "      <td>transformer</td>\n",
       "      <td>500</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.693149</td>\n",
       "      <td>0.5005</td>\n",
       "      <td>corelnet</td>\n",
       "      <td>500</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.231487</td>\n",
       "      <td>0.9255</td>\n",
       "      <td>corelnet_nsoftmax</td>\n",
       "      <td>500</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.092715</td>\n",
       "      <td>0.6085</td>\n",
       "      <td>predinet</td>\n",
       "      <td>500</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.676435</td>\n",
       "      <td>0.8440</td>\n",
       "      <td>relconvnet</td>\n",
       "      <td>500</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       loss     acc              group  train_size  trial\n",
       "0  0.727094  0.7670        transformer         500      0\n",
       "0  0.693149  0.5005           corelnet         500      0\n",
       "0  0.231487  0.9255  corelnet_nsoftmax         500      0\n",
       "0  1.092715  0.6085           predinet         500      0\n",
       "0  0.676435  0.8440         relconvnet         500      0"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "dfs = [pd.DataFrame(eval_dict) for eval_dict in model_eval_dicts]\n",
    "learning_curves_df = pd.concat(dfs)\n",
    "learning_curves_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "datetimestr = datetime.datetime.now().strftime(\"%Y-%m-%d-%H%M\")\n",
    "learning_curves_df.to_csv(f'learning_curves_data_{datetimestr}.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/diskarray/home/awni/.conda/envs/relconvnet/lib/python3.10/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "  if pd.api.types.is_categorical_dtype(vector):\n",
      "/diskarray/home/awni/.conda/envs/relconvnet/lib/python3.10/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "  if pd.api.types.is_categorical_dtype(vector):\n",
      "/diskarray/home/awni/.conda/envs/relconvnet/lib/python3.10/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "  if pd.api.types.is_categorical_dtype(vector):\n",
      "/diskarray/home/awni/.conda/envs/relconvnet/lib/python3.10/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "  if pd.api.types.is_categorical_dtype(vector):\n",
      "/diskarray/home/awni/.conda/envs/relconvnet/lib/python3.10/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "  if pd.api.types.is_categorical_dtype(vector):\n",
      "/diskarray/home/awni/.conda/envs/relconvnet/lib/python3.10/site-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n",
      "  with pd.option_context('mode.use_inf_as_na', True):\n",
      "/diskarray/home/awni/.conda/envs/relconvnet/lib/python3.10/site-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n",
      "  with pd.option_context('mode.use_inf_as_na', True):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Axes: xlabel='epoch', ylabel='acc'>"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "sns.lineplot(learning_curves_df, y='acc', x='train_size', hue='group', estimator='mean', errorbar=('ci', 95), legend=True)\n",
    "# ax.set_title(task_label_map.get(task, task))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.12 64-bit ('relconvnet')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "cfd9851f5ad10fee1056595cab4382bd1be8335c56803e8ccc2b0eacb646d0c9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
