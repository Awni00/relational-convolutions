
\section{Discussion}\label{sec:discussion}

\textbf{Summary} In this paper, we proposed a framework for hierarchical relational representation learning via a novel relational convolution operation. The relational convolution operation we propose here is a `convolution' in the sense that it considers a patch of the relation tensor, given by a group, and compares the relations within it to a template graphlet filter via an appropriately-defined inner product. This is similar to how, in a convolutional neural network, we would take a patch of an image and compare it to an image filter. Since the same graphlet filters are used for all groupings, the relational convolution operation implements a form of \textit{parameter-sharing} which yields improved sample-efficiency and generalization.

An important feature of the relational convolution operation is its \textit{interpretability}. The filters $\bm{f} = (f_1, \ldots, f_{n_f})$ are each a particular pattern of relations between $s$ objects. Furthermore, each object in the relational convolution $R \ast \bm{f}$ represents the degree to which the relations in the group $g$ match the patterns in each filter. By iteratively applying multi-head relation layers and relational convolution layers, we obtain an architecture which naturally models \textit{hierarchical} relations.

\textbf{Limitations and future work.} One aspect of the architecture that our empirical evaluations miss is the use of grouping layers and relational convolutions with learnable soft groups. Explicitly modeling groups was unnecessary in the experiments we considered since the number of objects $m$ was small, so considering all possible combinations was feasible. Hence, a research question for future work is: when $m$ is large, how effective are grouping layers at identifying the relevant groups? How does this compare to considering a random sample of discrete groups?

In addition, the tasks considered here are solvable by modeling only second-order relations at most. We observe that the relational convolutional networks architecture saturates the relational games benchmark of~\citep{shanahanExplicitlyRelationalNeural}. A more thorough evaluation of the relational convolutional networks architecture, and future architectures for modeling higher-order relational tasks, requires the development of new benchmark tasks and datasets which involve a larger number of objects and higher-order relations. This is a non-trivial task that we leave for future work.

Finally, the experiments considered here are synthetic relational tasks designed for a controlled evaluation. In more realistic settings, we envision the relational convolutions network as a module embedded in a broader architecture. For example, the relational convolutions framework can be naturally integrated into a graph neural network by using the relational convolution outputs as the node inputs and the relation tensor as the graph inputs. Similarly, the framework can be integrated into a Transformer-based model for general sequence modeling tasks by having the decoder attend to the sequence of relational objects produced by relational convolutions.