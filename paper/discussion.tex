
\section{Discussion}\label{sec:discussion}

\textbf{Summary.} In this paper, we proposed a framework for hierarchical relational representation learning via a novel relational convolution operation. The relational convolution operation we propose here is a `convolution' in the sense that it considers a patch of the relation tensor, given by a group, and compares the relations within it to a template graphlet filter via an appropriately-defined inner product. This is analogous to convolutional neural networks, where we would compare an image filter against different patches of the input image. Since the same graphlet filters are used for all groupings, the relational convolution operation implements a form of \textit{parameter-sharing} which yields improved sample-efficiency and generalization.

An important feature of the relational convolution operation is its \textit{interpretability}. The filters $\bm{f} = (f_1, \ldots, f_{n_f})$ are each a particular pattern of relations between $s$ objects. Each object in the output of a relational convolution $R \ast \bm{f}$ represents the degree to which the relations in the group $g$ match the patterns in each filter. By iteratively applying inner product relation layers and relational convolution layers, we obtain an architecture which naturally models \textit{hierarchical} relations.

\textbf{Limitations and future work.} The tasks considered here are solvable by modeling only second-order relations at most. We observe that the relational convolutional networks architecture saturates the relational games benchmark of~\citep{shanahanExplicitlyRelationalNeural}. While the ``contains set'' task demonstrates a sharp separation between relational convolutional networks and existing baselines, this task too only involves second-order relations, and does not fully test the abilities of the framework. A more thorough evaluation of this architecture, and future architectures for modeling hierarchical relations, would require the development of new benchmark tasks and datasets which involve a larger number of objects and higher-order relations. This is a non-trivial task that we leave for future work.

The experiments considered here are synthetic relational tasks designed for a controlled evaluation. In more realistic settings, we envision relational convolutional networks as modules embedded in a broader architecture. For example, a relational convolutions network can be embedded into an RL agent to enable performing tasks involving relational reasoning. Similarly, the framework can be integrated into a Transformer-based model for general sequence modeling tasks by having the decoder attend to the sequence of relational objects produced by relational convolutions.