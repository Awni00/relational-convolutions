\section{Introduction}\label{sec:intro}

Modeling of relations between objects is important to a range of machine learning problems. For instance, an image analysis application might rely on comparing objects in terms of relations that extract color, size, or texture features; a natural language task may use relations that are based on syntactic or semantic features of pairs of words. To enable efficient learning of relational information, it is important to explore learning architectures that support processing of relations in a natural, expressive, and efficient manner. %In this paper we propose a framework we call ``relational convolutional networks.'' %Our framework provides for relational representation learning what standard convolutional neural networks provide for typical image processing applications---a natural, expressive architecture for extracting hierarchical features of the raw data that can be used for building flexible prediction algorithms.

Compositionality, the ability to compose modules together to build iteratively more complex feature maps, is key to the success of deep representation learning. For example, in a feed forward network, each layer builds on the one before, and in a CNN, each convolution builds an iteratively more complex feature map~\citep{zeiler2014visualizing}. So far, work on relational representation learning has been limited to ``flat'' first-order architectures. In this work, we propose a compositional framework for learning hierarchical relational representations, which we call ``relational convolutional networks.''

A schematic of the proposed architecture to support hierarchical relational learning is shown in~\Cref{fig:relconv_architecture}. The key idea involves formalizing a notion of ``convolving'' a relation tensor, describing the pairwise relations in a sequence of objects, with a ``graphlet filter'' which represents a template of relations between subsets of objects. Each composition of those operations computes relational features of a higher order.


\begin{wrapfigure}{R}{0.5\textwidth}
    \vskip -20pt
    \centering
    \includegraphics[width=.5\textwidth]{figs/relconv_architecture.pdf}
    \vskip-5pt
    \caption{Proposed architecture for relational convolutional networks. Hierarchical relations are modeled by iteratively computing pairwise relations between objects and convolving the resultant relation tensor with graphlet filters representing templates of relations between subsets of objects.
    }\label{fig:relconv_architecture}
\end{wrapfigure}

In a series of experiments, we show how relational convolutional neural networks provide an effective framework (and inductive bias) for relational learning. We first carry out experiments on ``relational games'' proposed as a benchmark for relational reasoning by~\citep{shanahanExplicitlyRelationalNeural}. This consists of a suite of binary classification tasks for identifying abstract relational rules between a set of objects represented as images. We next carry out experiments on a version of the SET game, which requires processing of  higher-order relations between multiple attributes on a set of cards. For both tasks, relational convolutional networks are able to achieve more sample efficient learning compared to Transformers, as well as other architectures that have been specifically developed for relational learning.

\input{related_work}