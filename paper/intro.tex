\section{Introduction}\label{sec:intro}

Objects in the real world rarely exist in isolation; understanding and modeling the relationships between them is essential to accurately capturing complex systems. As increasingly powerful machine learning models progress towards building internal ``world models,'' a crucial frontier of research is exploring natural inductive biases which enable efficient learning of relational representations. The computational challenge lies in developing the components necessary for constructing robust, flexible, and progressively complex relational representations.

% Modeling of relations between objects is important to a range of machine learning problems. For instance, an image analysis application might rely on comparing objects in terms of relations that extract color, size, or texture features; a natural language task may use relations that are based on syntactic or semantic features of pairs of words. To enable efficient learning of relational information, it is important to explore learning architectures that support processing of relations in a natural, expressive, and efficient manner.

Compositionality---the ability to compose modules together to build iteratively more complex feature representations---is key to the success of deep representation learning. 
% For example, in a feedforward network, each layer builds on the one before, and in a CNN, each convolution builds an iteratively more complex feature map~\citep{zeiler2014visualizing}. 
For example, a CNN extracts higher-level features (e.g., textures and object-specific features) by composing simpler feature maps.
So far, work on relational representation learning has been limited to ``flat'' first-order architectures. In this work, we propose a compositional framework for learning hierarchical relational representations, which we call ``relational convolutional networks.''

A schematic of the proposed architecture is shown in~\Cref{fig:relconv_architecture}. The key idea involves formalizing a notion of ``convolving'' a relation tensor, describing the pairwise relations in a set of objects, with a ``graphlet filter'' which represents a template of a relational pattern within a smaller groups of objects. This produces a sequence of vectors which represent the relational pattern within each group of objects. Crucially, each composition of those operations computes relational features of a higher order---i.e., relations between relations.

\begin{wrapfigure}{R}{0.5\textwidth}
    \vskip -20pt
    \centering
    \includegraphics[width=.5\textwidth]{figs/relconv_architecture.pdf}
    \vskip-5pt
    \caption{Proposed architecture for relational convolutional networks. Hierarchical relations are modeled by iteratively computing pairwise relations between objects and convolving the resultant relation tensor with graphlet filters representing templates of relations between groups of objects.
    }\label{fig:relconv_architecture}
\end{wrapfigure}
% AWNI: [TODO] UPDATE NOTATION IN THIS FIGURE. E.G., $n$, $n_g^{(l)}$, $d$, $d^{(l)}$ etc.

In a series of experiments, we show how relational convolutional neural networks provide a powerful framework and inductive bias for relational learning. We first carry out experiments on ``relational games'', a benchmark for relational reasoning proposed by~\citep{shanahanExplicitlyRelationalNeural} consisting of a suite of binary classification tasks for identifying abstract relational rules between a set of objects represented as images. We next carry out experiments on a version of the SET game, which requires processing of higher-order relations across multiple attributes. For both tasks, relational convolutional networks outperform transformers, graph neural networks, recurrent neural networks, as well as existing relational architectures. We argue that these results demonstrate that both compositionality and relational inductive biases are needed to efficiently learn representations of complex higher-order relations.

\input{related_work}