\section{Introduction}\label{sec:intro}

Objects in the real world rarely exist in isolation; understanding and modeling the relationships between them is essential to accurately capturing complex systems. As increasingly powerful machine learning models progress towards building internal ``world models,'' it is important to explore natural inductive biases to enable efficient learning of relational representations. The computational challenge lies in developing the components necessary for constructing robust, flexible, and progressively complex relational representations.

% Modeling of relations between objects is important to a range of machine learning problems. For instance, an image analysis application might rely on comparing objects in terms of relations that extract color, size, or texture features; a natural language task may use relations that are based on syntactic or semantic features of pairs of words. To enable efficient learning of relational information, it is important to explore learning architectures that support processing of relations in a natural, expressive, and efficient manner.

Compositionality---used here to mean an ability to compose modules together to build iteratively more complex feature representations---is essential to the success of deep representation learning. 
 For example, in a feedforward network, each layer builds on the one before, and in a CNN, each convolution builds an iteratively more complex feature map~\citep{zeiler2014visualizing}. 
In particular, a CNN extracts higher-level features (e.g., textures and object-specific features) by composing simpler feature maps, resulting in a flexible architecture for computing ``features of features.''
So far, work on relational representation learning has been limited to ``flat'' or first-order architectures. In this work, we propose \textit{\bfseries relational convolutional networks} as a compositional framework for learning hierarchical relational representations.

The key to the framework proposed in this paper involves formalizing the concept of convolving 
learnable templates of a relational pattern against a larger relation tensor. This operation produces a sequence of vectors representing the relational pattern within each group of objects. Importantly, each composition of these operations computes relational features of a higher order---i.e., relations between relations.
Specifically, our proposed architecture introduces the 
following novel concepts and computational mechanisms.
\begin{itemize}
    \item \textit{\bfseries Graphlet filters.} A ``graphlet filter'' is a template for the pattern of relations between a small set of objects. Since relations are computed between pairs of objects, corresponding to weighted edges in a graph, the term ``graphlet'' is used to refer to a subgraph, and the term ``filter'' is used to refer to a learnable template or pattern. Graphlet filters are analogous to filters (also called kernels) in traditional CNNs for images, which represent templates for local regions of an image.
    \item \textit{\bfseries Relational convolutions.} In CNNs for image processing, each filter is ``swept,'' or convolved across the image. For relational learning, we formalize an analogous notion of convolution where a graphlet filter can be matched against the relations between each subset of nodes.
    \item \textit{\bfseries Grouping mechanisms.} For large problem instances, it would be infeasible to consider relational convolutions across all combinations of nodes. To achieve scalability, we introduce a learnable grouping mechanism based on attention which identifies the relevant groups that should be considered. %Different pooling operations are also introduced.
    \item \textit{\bfseries Compositional relational modules.} The proposed architecture supports composable modules, where each module has learnable graphlet filters and groups. This enables learning of higher-order relationships between objects---relations between relations---that are analogous to the composed feature maps that are of the essence in CNNs.
\end{itemize}

The architecture is presented in detail in Sections~\ref{sec:mhr} and \ref{sec:relconv}, and a schematic of the proposed architecture is shown in~\Cref{fig:relconv_architecture}. 
In a series of experiments, we show how relational convolutional neural networks provide a powerful framework and inductive bias for relational learning. We first carry out experiments on the relational games benchmark for relational reasoning proposed by~\citep{shanahanExplicitlyRelationalNeural} consisting of a suite of binary classification tasks for identifying abstract relational rules between a set of objects represented as images. We next carry out experiments on a version of the SET game, which requires processing of higher-order relations across multiple attributes. For both tasks, relational convolutional networks outperform transformers, graph neural networks, recurrent neural networks, as well as existing relational architectures. We argue that these results demonstrate that both compositionality and relational inductive biases are needed to efficiently learn representations of complex higher-order relations.

\input{related}