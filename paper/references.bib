@online{altabaaAbstractorsTransformer2023,
  title = {Abstractors: {{Transformer Modules}} for {{Symbolic Message Passing}} and {{Relational Reasoning}}},
  shorttitle = {Abstractors},
  author = {Altabaa, Awni and Webb, Taylor and Cohen, Jonathan and Lafferty, John},
  date = {2023-04-01},
  year = {2023},
  url = {https://arxiv.org/abs/2304.00195v2},
  urldate = {2023-06-28},
  langid = {english},
  pubstate = {preprint}
}

@article{barronUniversalApproximation1993,
  title = {Universal Approximation Bounds for Superpositions of a Sigmoidal Function},
  author = {Barron, A.R.},
  date = {1993-05},
  year = {1993},
  journaltitle = {IEEE Transactions on Information Theory},
  shortjournal = {IEEE Trans. Inform. Theory},
  volume = {39},
  number = {3},
  pages = {930--945},
  issn = {0018-9448, 1557-9654},
  doi = {10.1109/18.256500},
  url = {https://ieeexplore.ieee.org/document/256500/},
  urldate = {2023-03-31}
}

@article{cybenkoApproximationSuperpositions1989,
  title = {Approximation by Superpositions of a Sigmoidal Function},
  author = {Cybenko, G.},
  date = {1989-12},
  year = {1989},
  journaltitle = {Mathematics of Control, Signals, and Systems},
  shortjournal = {Math. Control Signal Systems},
  volume = {2},
  number = {4},
  pages = {303--314},
  issn = {0932-4194, 1435-568X},
  doi = {10.1007/BF02551274},
  url = {http://link.springer.com/10.1007/BF02551274},
  urldate = {2023-03-31},
  langid = {english}
}

@online{devitoExtensionMercer2011,
  title = {An Extension of {{Mercer}} Theorem to Vector-Valued Measurable Kernels},
  author = {De Vito, Ernesto and Umanita`, Veronica and Villa, Silvia},
  date = {2011-10-18},
  year = {2011},
  eprint = {1110.4017},
  eprinttype = {arxiv},
  eprintclass = {math},
  url = {http://arxiv.org/abs/1110.4017},
  urldate = {2023-05-06},
  pubstate = {preprint}
}

@article{hornikMultilayerFeedforward1989,
  title = {Multilayer Feedforward Networks Are Universal Approximators},
  author = {Hornik, Kurt and Stinchcombe, Maxwell and White, Halbert},
  date = {1989-01-01},
  year = {1989},
  journaltitle = {Neural Networks},
  shortjournal = {Neural Networks},
  volume = {2},
  number = {5},
  pages = {359--366},
  issn = {0893-6080},
  doi = {10.1016/0893-6080(89)90020-8},
  url = {https://www.sciencedirect.com/science/article/pii/0893608089900208},
  urldate = {2022-10-07},
  langid = {english}
}

@online{kergNeuralArchitecture2022,
  title = {On {{Neural Architecture Inductive Biases}} for {{Relational Tasks}}},
  author = {Kerg, Giancarlo and Mittal, Sarthak and Rolnick, David and Bengio, Yoshua and Richards, Blake and Lajoie, Guillaume},
  date = {2022-06-09},
  year = {2022},
  eprint = {2206.05056},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2206.05056},
  url = {http://arxiv.org/abs/2206.05056},
  urldate = {2022-09-13},
  pubstate = {preprint}
}

@article{mercerFunctionsPositive1909,
  title = {Functions of {{Positive}} and {{Negative Type}}, and Their {{Connection}} with the {{Theory}} of {{Integral Equations}}},
  author = {Mercer, J.},
  date = {1909},
  journaltitle = {Philosophical Transactions of the Royal Society of London. Series A, Containing Papers of a Mathematical or Physical Character},
  volume = {209},
  eprint = {91043},
  eprinttype = {jstor},
  pages = {415--446},
  publisher = {{The Royal Society}},
  issn = {0264-3952},
  url = {https://www.jstor.org/stable/91043},
  urldate = {2023-03-31}
}

@article{micchelliUniversalKernels2006,
  title = {Universal {{Kernels}}},
  author = {Micchelli, Charles A. and Xu, Yuesheng and Zhang, Haizhang},
  date = {2006},
  journaltitle = {Journal of Machine Learning Research},
  volume = {7},
  number = {95},
  pages = {2651--2667},
  issn = {1533-7928},
  url = {http://jmlr.org/papers/v7/micchelli06a.html},
  urldate = {2023-03-31}
}

@inproceedings{santoroRelationalRecurrent2018,
  title = {Relational Recurrent Neural Networks},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Santoro, Adam and Faulkner, Ryan and Raposo, David and Rae, Jack and Chrzanowski, Mike and Weber, Theophane and Wierstra, Daan and Vinyals, Oriol and Pascanu, Razvan and Lillicrap, Timothy},
  date = {2018},
  year = {2018},
  volume = {31},
  publisher = {{Curran Associates, Inc.}},
  url = {https://proceedings.neurips.cc/paper/2018/hash/e2eabaf96372e20a9e3d4b5f83723a61-Abstract.html},
  urldate = {2022-11-11}
}

@online{santoroSimpleNeural2017,
  title = {A Simple Neural Network Module for Relational Reasoning},
  author = {Santoro, Adam and Raposo, David and Barrett, David G. T. and Malinowski, Mateusz and Pascanu, Razvan and Battaglia, Peter and Lillicrap, Timothy},
  date = {2017-06-05},
  year = {2017},
  eprint = {1706.01427},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1706.01427},
  urldate = {2022-11-11},
  pubstate = {preprint}
}

@article{seelyNonSymmetricKernels1919,
  title = {Non-{{Symmetric Kernels}} of {{Positive Type}}},
  author = {Seely, Dr. Caroline E.},
  date = {1919-03},
  year = {1919},
  journaltitle = {The Annals of Mathematics},
  shortjournal = {The Annals of Mathematics},
  volume = {20},
  number = {3},
  eprint = {1967866},
  eprinttype = {jstor},
  pages = {172},
  issn = {0003486X},
  doi = {10.2307/1967866},
  url = {https://www.jstor.org/stable/1967866?origin=crossref},
  urldate = {2023-04-13}
}

@article{sunMercerTheorem2005,
  title = {Mercer Theorem for {{RKHS}} on Noncompact Sets},
  author = {Sun, Hongwei},
  date = {2005-06},
  year = {2005},
  journaltitle = {Journal of Complexity},
  shortjournal = {Journal of Complexity},
  volume = {21},
  number = {3},
  pages = {337--349},
  issn = {0885064X},
  doi = {10.1016/j.jco.2004.09.002},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0885064X04000822},
  urldate = {2023-03-31},
  langid = {english}
}

@online{webbEmergentSymbols2021,
  title = {Emergent {{Symbols}} through {{Binding}} in {{External Memory}}},
  author = {Webb, Taylor W. and Sinha, Ishan and Cohen, Jonathan D.},
  date = {2021-03-09},
  year = {2021},
  eprint = {2012.14601},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2012.14601},
  url = {http://arxiv.org/abs/2012.14601},
  urldate = {2022-09-13},
  pubstate = {preprint}
}

@inproceedings{zhangReproducingKernel2009,
  title = {Reproducing Kernel {{Banach}} Spaces for Machine Learning},
  booktitle = {2009 {{International Joint Conference}} on {{Neural Networks}}},
  author = {Zhang, Haizhang and Xu, Yuesheng and Zhang, Jun},
  date = {2009-06},
  year = {2009},
  pages = {3520--3527},
  publisher = {{IEEE}},
  location = {{Atlanta, Ga, USA}},
  doi = {10.1109/IJCNN.2009.5179093},
  url = {http://ieeexplore.ieee.org/document/5179093/},
  urldate = {2023-05-06},
  eventtitle = {2009 {{International Joint Conference}} on {{Neural Networks}} ({{IJCNN}} 2009 - {{Atlanta}})},
  isbn = {978-1-4244-3548-7},
  langid = {english}
}


@InProceedings{shanahanExplicitlyRelationalNeural,
  title = 	 {An Explicitly Relational Neural Network Architecture},
  author =       {Shanahan, Murray and Nikiforou, Kyriacos and Creswell, Antonia and Kaplanis, Christos and Barrett, David and Garnelo, Marta},
  booktitle = 	 {Proceedings of the 37th International Conference on Machine Learning},
  pages = 	 {8593--8603},
  year = 	 {2020},
  editor = 	 {III, Hal Daum√© and Singh, Aarti},
  volume = 	 {119},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {13--18 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v119/shanahan20a/shanahan20a.pdf},
  url = 	 {https://proceedings.mlr.press/v119/shanahan20a.html},
  abstract = 	 {With a view to bridging the gap between deep learning and symbolic AI, we present a novel end-to-end neural network architecture that learns to form propositional representations with an explicitly relational structure from raw pixel data. In order to evaluate and analyse the architecture, we introduce a family of simple visual relational reasoning tasks of varying complexity. We show that the proposed architecture, when pre-trained on a curriculum of such tasks, learns to generate reusable representations that better facilitate subsequent learning on previously unseen tasks when compared to a number of baseline architectures. The workings of a successfully trained model are visualised to shed some light on how the architecture functions.}
}

@article{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@inproceedings{lahaControllableSparseAlternatives2018a,
  title = {On {{Controllable Sparse Alternatives}} to {{Softmax}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Laha, Anirban and Chemmengath, Saneem Ahmed and Agrawal, Priyanka and Khapra, Mitesh and Sankaranarayanan, Karthik and Ramaswamy, Harish G},
  year = {2018},
  volume = {31},
  publisher = {{Curran Associates, Inc.}},
  urldate = {2023-08-31},
  abstract = {Converting an n-dimensional vector to a probability distribution over n objects is a commonly used component in many machine learning tasks like multiclass classification, multilabel classification, attention mechanisms etc. For this, several probability mapping functions have been proposed and employed in literature such as softmax, sum-normalization, spherical softmax, and sparsemax, but there is very little understanding in terms how they relate with each other. Further, none of the above formulations offer an explicit control over the degree of sparsity. To address this, we develop a unified framework that encompasses all these formulations as special cases. This framework ensures simple closed-form solutions and existence of sub-gradients suitable for learning via backpropagation. Within this framework, we propose two novel sparse formulations, sparsegen-lin and sparsehourglass, that seek to provide a control over the degree of desired sparsity. We further develop novel convex loss functions that help induce the behavior of aforementioned formulations in the multilabel classification setting, showing improved performance. We also demonstrate empirically that the proposed formulations, when used to compute attention weights, achieve better or comparable performance on standard seq2seq tasks like neural machine translation and abstractive summarization.}
}

@inproceedings{zambaldiDeepReinforcementLearning2018,
  title={Deep reinforcement learning with relational inductive biases},
  author={Zambaldi, Vinicius and Raposo, David and Santoro, Adam and Bapst, Victor and Li, Yujia and Babuschkin, Igor and Tuyls, Karl and Reichert, David and Lillicrap, Timothy and Lockhart, Edward and others},
  booktitle={International conference on learning representations},
  year={2018}
}

@misc{battagliaRelationalInductiveBiases2018,
  title = {Relational Inductive Biases, Deep Learning, and Graph Networks},
  author = {Battaglia, Peter W. and Hamrick, Jessica B. and Bapst, Victor and {Sanchez-Gonzalez}, Alvaro and Zambaldi, Vinicius and Malinowski, Mateusz and Tacchetti, Andrea and Raposo, David and Santoro, Adam and Faulkner, Ryan and Gulcehre, Caglar and Song, Francis and Ballard, Andrew and Gilmer, Justin and Dahl, George and Vaswani, Ashish and Allen, Kelsey and Nash, Charles and Langston, Victoria and Dyer, Chris and Heess, Nicolas and Wierstra, Daan and Kohli, Pushmeet and Botvinick, Matt and Vinyals, Oriol and Li, Yujia and Pascanu, Razvan},
  year = {2018},
  month = oct,
  number = {arXiv:1806.01261},
  eprint = {1806.01261},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1806.01261},
  urldate = {2023-09-22},
  abstract = {Artificial intelligence (AI) has undergone a renaissance recently, making major progress in key domains such as vision, language, control, and decision-making. This has been due, in part, to cheap data and cheap compute resources, which have fit the natural strengths of deep learning. However, many defining characteristics of human intelligence, which developed under much different pressures, remain out of reach for current approaches. In particular, generalizing beyond one's experiences--a hallmark of human intelligence from infancy--remains a formidable challenge for modern AI. The following is part position paper, part review, and part unification. We argue that combinatorial generalization must be a top priority for AI to achieve human-like abilities, and that structured representations and computations are key to realizing this objective. Just as biology uses nature and nurture cooperatively, we reject the false choice between "hand-engineering" and "end-to-end" learning, and instead advocate for an approach which benefits from their complementary strengths. We explore how using relational inductive biases within deep learning architectures can facilitate learning about entities, relations, and rules for composing them. We present a new building block for the AI toolkit with a strong relational inductive bias--the graph network--which generalizes and extends various approaches for neural networks that operate on graphs, and provides a straightforward interface for manipulating structured knowledge and producing structured behaviors. We discuss how graph networks can support relational reasoning and combinatorial generalization, laying the foundation for more sophisticated, interpretable, and flexible patterns of reasoning. As a companion to this paper, we have released an open-source software library for building graph networks, with demonstrations of how to use them in practice.},
  archiveprefix = {arxiv}
}

@misc{kipfNeuralRelationalInference2018,
  title = {Neural {{Relational Inference}} for {{Interacting Systems}}},
  author = {Kipf, Thomas and Fetaya, Ethan and Wang, Kuan-Chieh and Welling, Max and Zemel, Richard},
  year = {2018},
  month = jun,
  number = {arXiv:1802.04687},
  eprint = {1802.04687},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  urldate = {2023-09-22},
  abstract = {Interacting systems are prevalent in nature, from dynamical systems in physics to complex societal dynamics. The interplay of components can give rise to complex behavior, which can often be explained using a simple model of the system's constituent parts. In this work, we introduce the neural relational inference (NRI) model: an unsupervised model that learns to infer interactions while simultaneously learning the dynamics purely from observational data. Our model takes the form of a variational auto-encoder, in which the latent code represents the underlying interaction graph and the reconstruction is based on graph neural networks. In experiments on simulated physical systems, we show that our NRI model can accurately recover ground-truth interactions in an unsupervised manner. We further demonstrate that we can find an interpretable structure and predict complex dynamics in real motion capture and sports tracking data.},
  archiveprefix = {arxiv}
}

@misc{locatelloObjectCentricLearningSlot2020,
  title = {Object-{{Centric Learning}} with {{Slot Attention}}},
  author = {Locatello, Francesco and Weissenborn, Dirk and Unterthiner, Thomas and Mahendran, Aravindh and Heigold, Georg and Uszkoreit, Jakob and Dosovitskiy, Alexey and Kipf, Thomas},
  year = {2020},
  month = oct,
  number = {arXiv:2006.15055},
  eprint = {2006.15055},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2006.15055},
  urldate = {2023-09-22},
  abstract = {Learning object-centric representations of complex scenes is a promising step towards enabling efficient abstract reasoning from low-level perceptual features. Yet, most deep learning approaches learn distributed representations that do not capture the compositional properties of natural scenes. In this paper, we present the Slot Attention module, an architectural component that interfaces with perceptual representations such as the output of a convolutional neural network and produces a set of task-dependent abstract representations which we call slots. These slots are exchangeable and can bind to any object in the input by specializing through a competitive procedure over multiple rounds of attention. We empirically demonstrate that Slot Attention can extract object-centric representations that enable generalization to unseen compositions when trained on unsupervised object discovery and supervised property prediction tasks.},
  archiveprefix = {arxiv}
}

@misc{zhangRAVENDatasetRelational2019,
  title = {{{RAVEN}}: {{A Dataset}} for {{Relational}} and {{Analogical Visual rEasoNing}}},
  shorttitle = {{{RAVEN}}},
  author = {Zhang, Chi and Gao, Feng and Jia, Baoxiong and Zhu, Yixin and Zhu, Song-Chun},
  year = {2019},
  month = mar,
  number = {arXiv:1903.02741},
  eprint = {1903.02741},
  primaryclass = {cs},
  publisher = {{arXiv}},
  urldate = {2023-09-22},
  abstract = {Dramatic progress has been witnessed in basic vision tasks involving low-level perception, such as object recognition, detection, and tracking. Unfortunately, there is still an enormous performance gap between artificial vision systems and human intelligence in terms of higher-level vision problems, especially ones involving reasoning. Earlier attempts in equipping machines with high-level reasoning have hovered around Visual Question Answering (VQA), one typical task associating vision and language understanding. In this work, we propose a new dataset, built in the context of Raven's Progressive Matrices (RPM) and aimed at lifting machine intelligence by associating vision with structural, relational, and analogical reasoning in a hierarchical representation. Unlike previous works in measuring abstract reasoning using RPM, we establish a semantic link between vision and reasoning by providing structure representation. This addition enables a new type of abstract reasoning by jointly operating on the structure representation. Machine reasoning ability using modern computer vision is evaluated in this newly proposed dataset. Additionally, we also provide human performance as a reference. Finally, we show consistent improvement across all models by incorporating a simple neural module that combines visual understanding and structure reasoning.},
  archiveprefix = {arxiv}
}

@misc{palmRecurrentRelationalNetworks2018,
  title = {Recurrent {{Relational Networks}}},
  author = {Palm, Rasmus Berg and Paquet, Ulrich and Winther, Ole},
  year = {2018},
  month = nov,
  number = {arXiv:1711.08028},
  eprint = {1711.08028},
  primaryclass = {cs},
  publisher = {{arXiv}},
  urldate = {2023-09-22},
  abstract = {This paper is concerned with learning to solve tasks that require a chain of interdependent steps of relational inference, like answering complex questions about the relationships between objects, or solving puzzles where the smaller elements of a solution mutually constrain each other. We introduce the recurrent relational network, a general purpose module that operates on a graph representation of objects. As a generalization of Santoro et al. [2017]'s relational network, it can augment any neural network model with the capacity to do many-step relational reasoning. We achieve state of the art results on the bAbI textual question-answering dataset with the recurrent relational network, consistently solving 20/20 tasks. As bAbI is not particularly challenging from a relational reasoning point of view, we introduce Pretty-CLEVR, a new diagnostic dataset for relational reasoning. In the Pretty-CLEVR set-up, we can vary the question to control for the number of relational reasoning steps that are required to obtain the answer. Using Pretty-CLEVR, we probe the limitations of multi-layer perceptrons, relational and recurrent relational networks. Finally, we show how recurrent relational networks can learn to solve Sudoku puzzles from supervised training data, a challenging task requiring upwards of 64 steps of relational reasoning. We achieve state-of-the-art results amongst comparable methods by solving 96.6\% of the hardest Sudoku puzzles.},
  archiveprefix = {arxiv}
}

@inproceedings{gilmerNeuralMessagePassing2017,
  title = {Neural Message Passing for Quantum Chemistry},
  booktitle = {International Conference on Machine Learning},
  author = {Gilmer, Justin and Schoenholz, Samuel S. and Riley, Patrick F. and Vinyals, Oriol and Dahl, George E.},
  year = {2017},
  pages = {1263--1272},
  publisher = {{PMLR}},
  isbn = {2640-3498}
}

@misc{kipfSemiSupervisedClassificationGraph2017,
  title = {Semi-{{Supervised Classification}} with {{Graph Convolutional Networks}}},
  author = {Kipf, Thomas N. and Welling, Max},
  year = {2017},
  month = feb,
  number = {arXiv:1609.02907},
  eprint = {1609.02907},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1609.02907},
  urldate = {2023-09-23},
  abstract = {We present a scalable approach for semi-supervised learning on graph-structured data that is based on an efficient variant of convolutional neural networks which operate directly on graphs. We motivate the choice of our convolutional architecture via a localized first-order approximation of spectral graph convolutions. Our model scales linearly in the number of graph edges and learns hidden layer representations that encode both local graph structure and features of nodes. In a number of experiments on citation networks and on a knowledge graph dataset we demonstrate that our approach outperforms related methods by a significant margin.},
  archiveprefix = {arxiv}
}

@misc{niepertLearningConvolutionalNeural2016,
  title = {Learning {{Convolutional Neural Networks}} for {{Graphs}}},
  author = {Niepert, Mathias and Ahmed, Mohamed and Kutzkov, Konstantin},
  year = {2016},
  month = jun,
  number = {arXiv:1605.05273},
  eprint = {1605.05273},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  urldate = {2023-09-22},
  abstract = {Numerous important problems can be framed as learning from graph data. We propose a framework for learning convolutional neural networks for arbitrary graphs. These graphs may be undirected, directed, and with both discrete and continuous node and edge attributes. Analogous to image-based convolutional networks that operate on locally connected regions of the input, we present a general approach to extracting locally connected regions from graphs. Using established benchmark data sets, we demonstrate that the learned feature representations are competitive with state of the art graph kernels and that their computation is highly efficient.},
  archiveprefix = {arxiv}
}

@misc{schlichtkrullModelingRelationalData2017,
  title = {Modeling {{Relational Data}} with {{Graph Convolutional Networks}}},
  author = {Schlichtkrull, Michael and Kipf, Thomas N. and Bloem, Peter and van den Berg, Rianne and Titov, Ivan and Welling, Max},
  year = {2017},
  month = oct,
  number = {arXiv:1703.06103},
  eprint = {1703.06103},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  urldate = {2023-09-23},
  abstract = {Knowledge graphs enable a wide variety of applications, including question answering and information retrieval. Despite the great effort invested in their creation and maintenance, even the largest (e.g., Yago, DBPedia or Wikidata) remain incomplete. We introduce Relational Graph Convolutional Networks (R-GCNs) and apply them to two standard knowledge base completion tasks: Link prediction (recovery of missing facts, i.e. subject-predicate-object triples) and entity classification (recovery of missing entity attributes). R-GCNs are related to a recent class of neural networks operating on graphs, and are developed specifically to deal with the highly multi-relational data characteristic of realistic knowledge bases. We demonstrate the effectiveness of R-GCNs as a stand-alone model for entity classification. We further show that factorization models for link prediction such as DistMult can be significantly improved by enriching them with an encoder model to accumulate evidence over multiple inference steps in the relational graph, demonstrating a large improvement of 29.8\% on FB15k-237 over a decoder-only baseline.},
  archiveprefix = {arxiv}
}

@book{hamiltonGraphRepresentationLearning2020,
  title = {Graph {{Representation Learning}}},
  author = {Hamilton, William L},
  year = {2020},
  month = sep,
  series = {Synthesis {{Lectures}} on {{Artificial Intelligence}} and {{Machine Learning}}},
  publisher = {{Morgan \& Claypool}},
  address = {{San Rafael, CA}},
  isbn = {978-1-68173-965-6}
}

@article{velickovicGraphAttentionNetworks2017,
  title = {Graph Attention Networks},
  author = {Veli{\v c}kovi{\'c}, Petar and Cucurull, Guillem and Casanova, Arantxa and Romero, Adriana and Lio, Pietro and Bengio, Yoshua},
  year = {2017},
  journal = {arXiv preprint arXiv:1710.10903},
  eprint = {1710.10903},
  archiveprefix = {arxiv}
}
