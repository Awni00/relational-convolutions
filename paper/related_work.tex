\subsection{Related Work}

\awni{I've added a possible outline here. What else should we mention? What else should we cite?}

\paragraph{Graph Neural Networks.} Overview of what a graph neural network is. Maybe give the message-passing formulation: $z_i' \gets \mathrm{Update}(z_i, \mathrm{Aggregate}(\set{z_j, j \in \calN(i)}))$. What are GNNs typically used for? CNNs are a special case where the graph is a grid. Transformers and self-attention are also a special case where the graph is a complete graph and the aggregate function is weighted based on the similarity between the nodes computed via $\mathrm{Softmax}(\bra{\iprod{W_q z_i}{W_k z_j}}_j)$. Hence, in our experiments, we compare to GNNs through Transformers as a representative.

Graph neural networks are a typical example of relational architectures thought of to operate on `relational tasks'. In this context, the `relations' are given to the model via edges in a graph. In contrast, our architecture, as well as the explicitly relational architectures described below, operate on collections of objects without any relations given as input. Instead, such relational architectures must infer the relevant relations from the objects themselves. Still, graph neural networks can be applied to relational tasks by passing in the collection of objects along with a fully-connected graph. A Transformer Encoder can be thought of as a special case of this architecture, and hence is the representative baseline we compare against in our experiments.

\paragraph{Emerging literature on ``explicitly relational'' architectures.} PrediNet, ESBN, CoRelNet. Mention in what sense these architectures are explicitly relational whereas Transformers (and GNNs more generally) process relations only implicitly through self-attention---they produce entangled representations of relational features as well as features of individual objects.