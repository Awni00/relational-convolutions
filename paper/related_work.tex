\subsection{Related Work}

Several previous works have considered the design of machine learning architectures which support the representation of relational information, in various forms~\citep{battagliaRelationalInductiveBiases2018,palmRecurrentRelationalNetworks2018,zhangRAVENDatasetRelational2019}.

\textbf{Graph Neural Networks.} Graph neural networks (GNNs) are a class of neural network architectures which operate on graphs and process ``relational'' data~\citep{niepertLearningConvolutionalNeural2016,kipfSemiSupervisedClassificationGraph2017,schlichtkrullModelingRelationalData2017,velickovicGraphAttentionNetworks2017,kipfNeuralRelationalInference2018}. A defining feature of the GNN model is its use of a form of neural message-passing, wherein the hidden representation of a node is updated as a function of the hidden representations of its neighbors~\citep{gilmerNeuralMessagePassing2017}. Typical examples of tasks which GNNs are applied to include node classification, graph classification, and link prediction~\citep{hamiltonGraphRepresentationLearning2020}. %This is a very general model which includes as a special case convolutional neural networks, where the graph is a grid, and Transformers, where the graph is a complete graph and the message-passing function is a convex sum of the neighbors' representations. In our experiments, we compare to Transformers as a representative of the GNN model.

In GNNs, the `relations' are given to the model via edges in a graph. In contrast, our architecture, as well as the explicitly relational architectures described below, operate on collections of objects without any relations given as input. Instead, such relational architectures must infer the relevant relations from the objects themselves. Still, graph neural networks can be applied to relational tasks by passing in the collection of objects along with a complete graph. A Transformer Encoder can be thought of as a special case of this architecture.%, and hence is the representative baseline we compare against in our experiments.

\textbf{Attention as modeling relations} Several works have proposed architectures with the ability to model relations by incorporating an attention mechanism~\citep{vaswani2017attention,locatelloObjectCentricLearningSlot2020,santoroRelationalRecurrent2018,zambaldiDeepReinforcementLearning2018,velickovicGraphAttentionNetworks2017}. Attention mechanisms model relations between objects implicitly as an intermediate step in a form of neural message-passing in order to update the representation of each object as a function of its context.

\textbf{Emerging literature on ``explicitly relational'' architectures.} There exists a growing literature on neural architectures which aim to explicitly model relational information between objects. An early example is~\citep{santoroSimpleNeural2017}.~\citet{shanahanExplicitlyRelationalNeural} proposes the PrediNet architecture, which aims to learn relational representations which are compatible with predicate logic.~\citet{webbEmergentSymbols2021} proposes ESBN, a recurrent neural network augmented with external memory with a memory-write operation which factors representations into `sensory' and `relational'.~\citet{kergNeuralArchitecture2022} proposes CoRelNet, a simple architecture based on `similarity scores' which aims to distill the relational inductive biases discovered in previous work into a minimal architecture.~\citet{altabaaAbstractorsTransformer2023} explored relational inductive biases in the context of Transformers, and proposed a view of relational inductive biases as a type of selective ``information bottleneck'' which disentangles relational information from object-level features.~\citet{webbRelationalBottleneckInductive2023} provides a cognitive science perspective on this idea, arguing that a relational information bottleneck may be a mechanism for abstraction in the human mind.% and brain.

We aim to contribute to this literature by proposing a framework for learning hierarchical relational representations through a natural compositional architecture. %in a natural, interpretable, sample-efficient, and parameter-efficient manner.