\section{Experiments}\label{sec:experiments}

%\aanote{this part is new.}
In this section, we empirically evaluate the proposed \textit{relational convolutional network} architecture (abbreviated RelConvNet) to assess its effectiveness at learning relational tasks. We compare this architecture to several existing relational architectures as well as general-purpose sequence models. The common input to all models is a sequence of objects $X = (x_1, \ldots, x_n) \in \reals^{n \times d}$. We evaluate against the following baselines.
\begin{itemize}
    \item \textbf{\textit{Transformer}} \citep{vaswani2017attention}. The Transformer is a powerful general-purpose sequence model. It consists of alternating self-attention and multi-layer perceptron blocks. Self-attention performs an information retrieval operation, which updates the internal representation of each object as a function of its context.
    Dot product attention is computed via $X' \gets \mathrm{Softmax}((X W_q) (X W_k)^{\intercal}) W_v X$, and the MLP is applied independently on each object's internal representation.
    The attention scores computed as an intermediate step in dot-product attention can perhaps be thought of as relations that determine what information to retrieve.
    \item \textbf{\textit{PrediNet}} \citep{shanahanExplicitlyRelationalNeural}. The PrediNet architecture is an explicitly relational architecture inspired by predicate logic. At a high-level, the PrediNet architecture computes $j$ relations between $k$ pairs of objects. The $k$ pairs of objects are selected via a learned attention operation. The ``$j$ relations'' refer to a difference between $j$-dimensional embeddings of the selected objects. More precisely, for each head $h \in [k]$, a pair of objects $E_1^h, E_2^h \in \reals^d$ is retrieved via an attention operation, and the final output of PrediNet is a set of difference relations given by $D^h = E_1^h W_s - E_2^h W_s$.
    \item \textbf{\textit{CoRelNet}} \citep{kergNeuralArchitecture2022}. The CoRelNet architecture is proposed as a minimal relational architecture distilling the core inductive biases that the authors argue are important for relational tasks. The CoRelNet module simply computes inner products between object representations and applies Softmax normalization, returning an $n \times n$ ``similarity matrix''. That is, the objects $X = (x_1, \ldots, x_n)$ are processed independently to produce embeddings $Z = (z_1, \ldots, z_n)$, and the similarity matrix is computed as $R = \mathrm{Softmax}(Z Z^\intercal)$. The similarity matrix $R$ is then flattened and passed through an MLP to produce the final output.
    \item \textbf{\textit{Graph Neural Networks}}. Graph neural networks are a class of neural network architectures which operate on graphs-structured data. A graph neural network typically receives two inputs: a graph described by a set of edges, and feature vectors for each node in the graph. GNNs can be described through the unifying framework of neural message-passing. Under this framework, graph-structured data is processed through an iterative message-passing operation given by $h_i^{(l+1)} \gets \mathrm{Update}(h_i^{(l)}, \{h_j^{(l)}\}_{j \in \calN(i)})$, where $h_i^{(0)} \gets x_i$. That is, each node's internal representation is iteratively updated as a function of its neighborhood. Here, $\mathrm{Update}$ is parameterized by a neural network, and the variation between different GNN architectures lies in the architectural design of this update process. We use Graph Convolution Networks~\citep{kipfSemiSupervisedClassificationGraph2017}, Graph Attention Networks~\citep{velickovicGraphAttentionNetworks2017}, and Graph Isomorphism Networks~\citep{xuHowPowerfulAre2018} as representative GNN baselines.
\end{itemize}

%\aawarning{TODO---add RelationNet baseline?}

\input{experiments_relational_games}

\input{experiments_set}