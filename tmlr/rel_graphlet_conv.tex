\section{Relational Convolutions with Graphlet Filters}\label{sec:relconv}

\subsection{Relational Convolutions with Discrete Groups}\label{ssec:relconv_discrete_groups}
In this section, we formalize a \textit{relational convolution} operation which processes pairwise relations between objects to produce representations of the relational patterns within groups of objects. Suppose that we have a sequence of objects $(x_1, \ldots, x_n)$ and a relation tensor $R$ describing the pairwise relations between them, obtained by an MD-IPR layer via $R[i,j] = r(x_i, x_j)$. The key idea is to learn a \textit{template} of relations between a small set of objects, and to ``convolve'' the template with the relation tensor, matching it against the relational patterns in different groups of objects. This transforms the relation tensor into a sequence of vectors, each summarizing the relational pattern in some group of objects. Crucially, this can now be composed with another relational layer to compute \textit{higher-order} relations---i.e., relations on relations.

Fix some filter size $s < n$, where $s$ is a hyperparameter of the relational convolution layer. One `filter' of size $s$ is given by the \textit{graphlet filter} $f_1 \in \mathbb{R}^{s \times s \times d_r}$. This is a template for the pairwise relations between a group of $s$ objects. 
Since pairwise relations can be viewed as edges on a graph, we use the term ``graphlet filter'' to refer to a template of pairwise relations between a small set of objects.
% Note that the dimension of the relations in this filter matches that of the input relation tensor. 
Let $g \subset [n]$ be a group of $s$ objects among $(x_1, \ldots, x_n)$. Then, denote the relation sub-tensor associated with this group by $R[g] := [R[i,j]]_{i,j \in g}$. We define the `relational inner product' between this relation subtensor and the filter $f_1$ by
\begin{equation}\label{eq:relational_inner_prod_one_filt}
    \reliprod{R[g]}{f_1} \coloneqq \sum_{i,j \in g} \sum_{k \in [d_r]} R[i,j,k] f_1[i,j,k].
\end{equation}
This is simply the standard inner product in the corresponding Euclidean space $\mathbb{R}^{s^2 d_r}$. This quantity represents how much the relational pattern in $g$ matches the template $f_1$.

% Since pairwise relations can be viewed as edges on a graph, we use the term ``graphlet filter'' to refer to a template of pairwise relations between a small set of objects.

% Another relevant configuration is when the relation tensor $R$ is assumed to be symmetric (i.e.: pairwise relations are symmetric). In this case, filters can be restricted to be symmetric, and can now be identified with the smaller space $\{f \in \mathbb{R}^{s \times s \times r}: f[i,j] = f[j,i] \ \forall i,j \}$. The definition of the relational inner product can be simplified in this case to $\langle R[g], f_1 \rangle_R \coloneqq \sum_{i \leq j \in g} \langle R[i,j], f_1[i,j] \rangle$.

In a relational convolution layer, we learn $n_f$ different filters. Denote the collection of filters by $\boldsymbol{f} = \paren{f_1, \ldots, f_{n_f}} \in \reals^{s \times s \times d_r \times n_f}$, which we call a \textit{graphlet filter}. We define the relational inner product of a relation subtensor $R[g]$ with the graphlet filters $\bm{f}$ as the $n_f$-dimensional vector consisting of the relational inner products with each individual filter,
\begin{equation}
    \label{eq:relational_inner_prod}
    \reliprod{R[g]}{\bm{f}} \coloneq \begin{pmatrix} \reliprod{R[g]}{f_1} \\ \vdots 
 \\ \reliprod{R[g]}{f_{n_f}} \end{pmatrix} \in \mathbb{R}^{n_f}.
\end{equation}
This vector summarizes various aspects of the relational pattern within a group, captured by several different filters\footnote{We have overloaded the notation $\reliprod{\cdot}{\cdot}$, but will use the convention that a collection of filters is denoted by a bold symbol (e.g., $\bm{f}$ vs $f_i$) to distinguish between the two forms of the relational inner product.}. Each filter corresponds to one dimension. This is reminiscent of convolutional neural networks, where each filter gives us one channel in the output tensor.

For a given group $g \subset [n]$, the relational inner product with a graphlet filter, $\iprod{R[g]}{\bm{f}}_\mathrm{rel}$, gives us a vector summarizing the relational patterns inside that group. Let $\calG$ be a set of groupings of the $n$ objects, each of size $s$. The relational convolution between a relation tensor $R$ and a relational graphlet filter $\bm{f}$ is defined as the sequence of relational inner products with each group in $\calG$
\begin{equation}\label{eq:relational_convolution}
    R \ast \bm{f} \coloneq \left( \reliprod{R[g]}{\boldsymbol{f}} \right)_{g \in \calG} \equiv \left(z_1, \ldots, z_{\abs{\calG}}\right) \in \reals^{\abs{\calG} \times n_f}
\end{equation}
In this section, we assumed that $\calG$ was given.
% (i.e., a pre-specified hyperparameter of the relational convolution module). 
If some prior information is known about what groupings are relevant, this can be encoded in $\calG$. Otherwise, if $n$ is small, $\calG$ can be all possible combinations of size $s$. However, when $n$ is large, considering all combinations will be intractable. In the next subsection, we consider the problem of \textit{differentiably learning} the relevant groups.

% In the above, we either consider all possible groups or we somehow assume that the relevant groups $\calG$ are known and given. We may also wish to \textit{learn} the relevant groups.

\begin{figure*}[t]
    \centering
    \includegraphics[width=0.9\textwidth]{figs/relconv_figs_updated.pdf}
    % \vskip-12pt
    \caption{A depiction of the relational convolution operation. A graphlet filter $\bm{f}$ is compared to the relation subtensor in each group of objects, producing a sequence of vectors summarizing the relational pattern within each group. The groups can be differentiably learned through an attention mechanism.
    }\label{fig:relconvdiagram}
    \vskip-7.5pt
\end{figure*}

\subsection{Relational Convolutions with Group Attention}\label{ssec:relconv_groupattn}

In the above formulation, the groups are `discrete'. Having discrete groups can be desirable for interpretability if the relevant groupings are known a priori or if considering every possible grouping is computationally and statistically feasible. However, if the relevant groupings are not known, then considering all possible combinations results in a rapid growth of the number of objects at each layer.
% Besides being computationally intractable, considering every possible grouping may be unnecessary and may make learning more difficult.

In order to address these issues, we can explicitly model and \textit{learn} the relevant groups. This allows us to control the number of objects in the output sequence of a relational convolution such that only relevant groups are considered. We propose modeling groups via an \textit{attention} operation.

Consider the input $(x_1, \ldots, x_n),\, x_i \in \reals^d$. Let $n_g$ be the number of groups to be learned and $s$ be the size of the graphlet filter (and hence the size of each group). These are hyperparameters of the model that we control.
% We learn $n_g \times s$ group \textit{query} vectors $Q = \{q_{i}^{g}\}_{g \in [n_g], i \in [s]}$. That is, 
For each group $g \in [n_g]$, we learn $s$ different \textit{queries}, $\{q_{i}^{g}\}_{g \in [n_g], i \in [s]}$, that will be used to retrieve a group of size $s$ via attention. The $i$-th object in the $k$-th group is retrieved as follows,
\begin{equation}\label{eq:group_attn}
    \begin{aligned}
        \bar{x}_{i}^{g} &= \sum_{j = 1}^{n} \alpha_{ij}^{g} x_j, && g \in [n_g], i \in [s],\\
        \alpha_{ij}^{g} &= \frac{\exp\paren{\beta \iprod{q_i^g}{\mathtt{key}(x_j)}}}{\sum_{k = 1}^{n}{\exp\paren{\beta \iprod{q_i^g}{\mathtt{key}(x_k)}}}}, \quad && g \in [n_g], i \in [s], j \in [n]
    \end{aligned}
\end{equation}
where $\bar{x}_{i}^{g}$ is the $i$-th object retrieved in the $g$-th group, $q_{i}^{g}$ is the query for retrieving the $i$-th object in the $g$-th group, $\mathtt{key}(x_j)$ is the key associated with the object $x_j$, and $\beta$ is a temperature scaling parameter.

The $\mathtt{key}$ for each object is computed as a function of its position, features, and/or context. For example, to group objects based on their position, the key can be a positional embedding, $\mathtt{key}(x_i) = PE_i$. To group based on features, the $\mathtt{key}$ can be a linear projection of the object's feature vector, $\mathtt{key}(x_i) = W_k x_i$. To group based on both position and features, the $\mathtt{key}$ can be a sum or concatenation of the above. Finally, computing keys after a self-attention operation allows objects to be grouped based on the context in which they occur.

The relation subtensor $\bar{R}[g] \in \reals^{s \times s \times d_r}$ for each group $g \in [n_g]$ is computed using a shared MD-IPR layer $r(\cdot, \cdot)$,
\begin{equation}\label{eq:group_attn_relsubtensor}
    \bar{R}[g] = \bra{r(\bar{x}_{i}^{g}, \bar{x}_{j}^{g})}_{i,j \in [s]}.
\end{equation}

The relational convolution is computed as before via,
\begin{equation}\label{eq:relconv_groupattn}
    \bar{R} \ast \bm{f} \equiv \paren{\reliprod{\bar{R}[g]}{\bm{f}}}_{g \in [n_g]}.
\end{equation}

Overall, relational convolution with group attention can be summarized as follows: 1) learn $n_g$ groupings of objects, retrieving $s$ objects per group; 2) compute the relation tensor of each group using an MD-IPR module; 3) compute a relational convolution with a learned set of graphlet filters $\bm{f}$, producing a sequence of $n_g$ vectors each describing the relational pattern within a (learned) group of objects.

\textbf{Computing input-dependent queries.} In the simplest case, the query vectors are simply learned parameters of the model, representing a fixed criterion for selecting the $n_g$ groups. The queries can also be produced in an input-dependent manner. There are many ways to do this. For example, the input $(x_1, \ldots, x_n)$ can be processed with some sequence or set embedder (e.g., through a self-attention operation) producing a vector embedding that can be mapped to different queries $\{q_{i}^g\}_{i,g}$ using learned linear maps.
% For example, the query vectors can be produced sequentially by a recurrent neural network with each retrieved object fed back into the LSTM so that it can produce the next query according to the groups selected so far. 
% Alternatively, the input $(x_1, \ldots, x_n)$ can be aggregated by a self-attention operation, and the resulting representation can be used to produce queries by linear maps.
% I.e., $q_i^g \gets \mathrm{LSTM}(h_{i-1}, \bar{x}_{i-1}^g)$.

\textbf{Entropy regularization.} Intuitively, we would ideally like the group attention scores in~\cref{eq:group_attn} to be close to discrete assignments. 
% That is, the vector $\alpha_{i,\cdot}^{g} \in \Delta^{n}$ is ideally a one-hot vector for each $g \in [n_g],\, i \in [s]$.
To encourage the model to learn more structured group assignments, we add an entropy regularization to the loss function, $\calL_{\mathtt{entr}} = (n_g \cdot s)^{-1} \sum_{g, i} H(\alpha_{i,\cdot}^{g})$, where $H(\alpha_{i,\cdot}^{g}) = - \sum_{j} \alpha_{ij}^{g} \log(\alpha_{ij}^g)$ is the Shannon entropy. As a heuristic, this regularization can be scaled by a factor proportional to $\log(\mathtt{n\_classes}) / \log(n)$ so that it doesn't dominate the underlying task's loss.

\textbf{Symmetric relational inner products.} So far, we considered \textit{ordered} groups. That is, the relational pattern computed by the relational inner product $\reliprod{R[g]}{\bm{f}}$ for the group $(1, 2, 3)$ is different from the group $(2, 3, 1)$. In some scenarios, symmetry in the representation of the relational pattern is a useful inductive bias. To capture this, we define a symmetric variant of the relational inner product that is invariant to the ordering of the elements in the group. This can be done by pooling over all permutations in the group. In particular, we suggest max-pooling or average-pooling, although any set-aggregator would be valid. We define the permutation-invariant relational inner product as
\begin{equation}\label{eq:symmetric_relational_inner_prod}
    \iprod{R[g]}{\bm{f}}_{\mathrm{rel}, \mathrm{sym}} \coloneq \mathrm{Pool}\paren{\set{\reliprod{R[g']}{\bm{f}} \colon g' \in g!}},
\end{equation}
where $g!$ denotes the set of permutations of the group $g$, and pooling is done independently across dimensions.
% Recall that each $\iprod{R[g']}{\bm{f}}_{\mathrm{rel}}$ is $n_f$-dimensional, and the pooling is done independently for each dimension.


\textbf{Computational efficiency.} \cref{eq:group_attn} can be computed in parallel with $\calO(n \cdot n_g \cdot s \cdot d)$ operations. When the hyperparameters of the model are fixed, this is linear in the sequence length $n$.~\cref{eq:group_attn_relsubtensor} can be computed in parallel via efficient matrix multiplication with $\calO(n_g \cdot s^2 \cdot d_r \cdot d_{\mathrm{proj}})$ operations. Finally,~\cref{eq:relconv_groupattn} can be computed in parallel with $\calO(n_g \cdot s^2 \cdot d_r \cdot n_f)$ operations. The latter two computations do not scale with the number of objects in the input, and are only a function of the hyperparameters of the model.
% Note that, typically, the filter size $s$ is chosen to be of fixed order with respect to the number of objects $n$ (e.g., fixed to $3$ or $5$, as in CNNs).
